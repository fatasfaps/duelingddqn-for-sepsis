{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNSeNs4YDQRGm+hIm67xe8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatasfaps/duelingddqn-for-sepsis/blob/main/defining_sepsis_cohort.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:32px; font-weight:600; margin-bottom:12px;\">\n",
        "  <b>SEPSIS-3 COHORT DEFINITION</b>\n",
        "</h1>\n",
        "\n",
        "<p style=\"font-size:14px; line-height:1.6;\">\n",
        "  This notebook defines the patient cohort used for downstream modeling,\n",
        "  following the <b>Sepsis-3</b> clinical criteria.\n",
        "  <br><br>\n",
        "  The cohort construction is based on the methodology described in:\n",
        "  <br>\n",
        "  <a href=\"https://www.nature.com/articles/s41591-018-0213-5\"\n",
        "     target=\"_blank\"\n",
        "     style=\"color:#444; text-decoration:none;\">\n",
        "    Komorowski et al., Nature Medicine (2018)\n",
        "  </a>\n",
        "  and is adapted from the\n",
        "  <a href=\"https://github.com/matthieukomorowski/AI_Clinician\"\n",
        "     target=\"_blank\"\n",
        "     style=\"color:#444; text-decoration:none;\">\n",
        "    AI-Clinician GitHub repository.</a>\n",
        "</p>\n",
        "\n",
        "<p style=\"font-size:14px; line-height:1.6;\">\n",
        "  Cohort inclusion is determined using the following logic:\n",
        "</p>\n",
        "\n",
        "<ul style=\"font-size:14px; line-height:1.6;\">\n",
        "  <li>\n",
        "    <b>Suspected infection</b> is identified by the temporal co-occurrence\n",
        "    of antibiotic administration and microbiological culture sampling.\n",
        "  </li>\n",
        "  <li>\n",
        "    <b>Sepsis onset</b> is defined as the first time point at which suspected\n",
        "    infection is accompanied by an increase in <b>SOFA score ≥ 2</b> from baseline.\n",
        "  </li>\n",
        "  <li>\n",
        "    Only adult ICU stays are considered, and each patient contributes a single ICU admission.\n",
        "  </li>\n",
        "</ul>\n",
        "\n",
        "<p style=\"font-size:14px; line-height:1.6;\">\n",
        "  The resulting cohort represents ICU patients meeting Sepsis-3 criteria\n",
        "  and forms the basis for subsequent state space construction and MDP modeling.\n",
        "</p>\n",
        "\n",
        "<p style=\"font-size:13px; color:#555;\">\n",
        "  This notebook focuses on cohort definition only; feature extraction and model training\n",
        "  are performed in subsequent stages of the pipeline.\n",
        "</p>"
      ],
      "metadata": {
        "id": "p8tef5A_5TU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGu4uZbIlCY4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm\n",
        "from datetime import timedelta\n",
        "import itertools\n",
        "from itertools import tee\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from sklearn.impute import KNNImputer\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gomByQcalU9K",
        "outputId": "e6fb6811-ae79-4aab-e44a-3022a4a71d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/mimic3'"
      ],
      "metadata": {
        "id": "1zXL_aRklYWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "M5hkjxAwlvLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all data tables\n",
        "abx = pd.read_csv('/content/drive/MyDrive/mimic3/abx.csv')\n",
        "culture = pd.read_csv('/content/drive/MyDrive/mimic3/culture.csv')\n",
        "microbio = pd.read_csv('/content/drive/MyDrive/mimic3/microbio.csv')\n",
        "demog = pd.read_csv('/content/drive/MyDrive/mimic3/demog.csv')\n",
        "all_vitals = pd.concat([\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce010.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce1020.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce2030.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce3040.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce4050.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce5060.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce6070.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce7080.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce8090.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce90100.csv'),\n",
        "], ignore_index=True)\n",
        "labU = pd.concat([\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/labs_ce.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/labs_le.csv')\n",
        "], ignore_index=True)\n",
        "MechVent = pd.read_csv('/content/drive/MyDrive/mimic3/mechvent.csv')\n",
        "inputpreadm = pd.read_csv('/content/drive/MyDrive/mimic3/preadm_fluid.csv')\n",
        "inputMV = pd.read_csv('/content/drive/MyDrive/mimic3/fluid_mv.csv')\n",
        "inputCV = pd.read_csv('/content/drive/MyDrive/mimic3/fluid_cv.csv')\n",
        "vasoMV = pd.read_csv('/content/drive/MyDrive/mimic3/vaso_mv.csv')\n",
        "vasoCV = pd.read_csv('/content/drive/MyDrive/mimic3/vaso_cv.csv')\n",
        "UOpreadm = pd.read_csv('/content/drive/MyDrive/mimic3/preadm_uo.csv')\n",
        "UO = pd.read_csv('/content/drive/MyDrive/mimic3/uo.csv')\n",
        "\n",
        "print(f\"Data imported in {time.time() - start_time:.1f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44calpgilfDP",
        "outputId": "4f6e1b78-fcc2-4947-b9b4-a4ea09e437f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data imported in 37.9 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all time columns to datetime type (with unit='s')\n",
        "demog['intime'] = pd.to_datetime(demog['intime'], unit='s', errors='coerce')\n",
        "demog['outtime'] = pd.to_datetime(demog['outtime'], unit='s', errors='coerce')\n",
        "demog['dod'] = pd.to_datetime(demog['dod'], unit='s', errors='coerce')\n",
        "abx['startdate'] = pd.to_datetime(abx['startdate'], unit='s', errors='coerce')\n",
        "abx['enddate'] = pd.to_datetime(abx['enddate'], unit='s', errors='coerce')\n",
        "microbio['charttime'] = pd.to_datetime(microbio['charttime'], unit='s', errors='coerce')\n",
        "culture['charttime'] = pd.to_datetime(culture['charttime'], unit='s', errors='coerce')\n",
        "labU['charttime'] = pd.to_datetime(labU['charttime'], unit='s', errors='coerce')\n",
        "MechVent['charttime'] = pd.to_datetime(MechVent['charttime'], unit='s', errors='coerce')\n",
        "all_vitals['charttime'] = pd.to_datetime(all_vitals['charttime'], unit='s', errors='coerce')\n",
        "inputMV['starttime'] = pd.to_datetime(inputMV['starttime'], unit='s', errors='coerce')\n",
        "inputCV['charttime'] = pd.to_datetime(inputCV['charttime'], unit='s', errors='coerce')\n",
        "vasoMV['starttime'] = pd.to_datetime(vasoMV['starttime'], unit='s', errors='coerce')\n",
        "vasoCV['charttime'] = pd.to_datetime(vasoCV['charttime'], unit='s', errors='coerce')\n",
        "UO['charttime'] = pd.to_datetime(UO['charttime'], unit='s', errors='coerce')\n",
        "\n",
        "print(f\"Conversion to datetime has finished in {time.time() - start_time:.1f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkAvXKS1lw_z",
        "outputId": "c79fc833-e83c-4ca0-8751-e87abe14f7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion to datetime has finished in 62.0 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge microbio and culture into bacterio\n",
        "bacterio = pd.concat([microbio, culture], ignore_index=True)\n",
        "\n",
        "# Handle missing icustay_id in bacterio\n",
        "bacterio_missing_icustay = bacterio[bacterio['icustay_id'].isnull()].copy()\n",
        "bacterio_has_icustay = bacterio[bacterio['icustay_id'].notnull()].copy()\n",
        "\n",
        "# Merge missing data with demog for finding the matches\n",
        "bacterio_missing_icustay = pd.merge(bacterio_missing_icustay.drop(columns='icustay_id'),\n",
        "                                     demog[['subject_id', 'hadm_id', 'icustay_id', 'intime', 'outtime']],\n",
        "                                     on=['subject_id', 'hadm_id'], how='left')\n",
        "\n",
        "# Implementing Komorowski's (2018)\n",
        "# 1. If charttime falls within the ICU intime and outtime range (+/- 48 hours)\n",
        "valid_match = (bacterio_missing_icustay['charttime'] >= bacterio_missing_icustay['intime'] - pd.Timedelta(hours=48)) & \\\n",
        "              (bacterio_missing_icustay['charttime'] <= bacterio_missing_icustay['outtime'] + pd.Timedelta(hours=48))\n",
        "\n",
        "bacterio_missing_icustay.loc[valid_match, 'filled_icustay_id'] = bacterio_missing_icustay.loc[valid_match, 'icustay_id']\n",
        "\n",
        "# 2. If there is only one hadm_id for the matching subject_id, assign that icustay_id\n",
        "grouped_by_hadm = bacterio_missing_icustay.groupby('hadm_id')['icustay_id'].nunique()\n",
        "single_match_hadm = grouped_by_hadm[grouped_by_hadm == 1].index\n",
        "bacterio_missing_icustay.loc[bacterio_missing_icustay['hadm_id'].isin(single_match_hadm), 'filled_icustay_id'] = bacterio_missing_icustay.loc[bacterio_missing_icustay['hadm_id'].isin(single_match_hadm), 'icustay_id']\n",
        "\n",
        "# Merge back the filled and complete data\n",
        "bacterio_missing_icustay['icustay_id'] = bacterio_missing_icustay['filled_icustay_id']\n",
        "bacterio = pd.concat([bacterio_has_icustay, bacterio_missing_icustay.drop(columns=['intime', 'outtime', 'filled_icustay_id'])], ignore_index=True)\n",
        "\n",
        "\n",
        "# Handle missing icustay_id in abx\n",
        "abx_missing_icustay = abx[abx['icustay_id'].isnull()].copy()\n",
        "abx_has_icustay = abx[abx['icustay_id'].notnull()].copy()\n",
        "\n",
        "abx_missing_icustay = pd.merge(abx_missing_icustay.drop(columns='icustay_id'),\n",
        "                                demog[['hadm_id', 'icustay_id', 'intime', 'outtime']],\n",
        "                                on='hadm_id', how='left')\n",
        "\n",
        "# Use 'startdate'\n",
        "valid_match_abx = (abx_missing_icustay['startdate'] >= abx_missing_icustay['intime'] - pd.Timedelta(hours=48)) & \\\n",
        "                  (abx_missing_icustay['startdate'] <= abx_missing_icustay['outtime'] + pd.Timedelta(hours=48))\n",
        "\n",
        "abx_missing_icustay.loc[valid_match_abx, 'filled_icustay_id'] = abx_missing_icustay.loc[valid_match_abx, 'icustay_id']\n",
        "\n",
        "grouped_by_hadm_abx = abx_missing_icustay.groupby('hadm_id')['icustay_id'].nunique()\n",
        "single_match_hadm_abx = grouped_by_hadm_abx[grouped_by_hadm_abx == 1].index\n",
        "abx_missing_icustay.loc[abx_missing_icustay['hadm_id'].isin(single_match_hadm_abx), 'filled_icustay_id'] = abx_missing_icustay.loc[abx_missing_icustay['hadm_id'].isin(single_match_hadm_abx), 'icustay_id']\n",
        "\n",
        "abx_missing_icustay['icustay_id'] = abx_missing_icustay['filled_icustay_id']\n",
        "abx = pd.concat([abx_has_icustay, abx_missing_icustay.drop(columns=['intime', 'outtime', 'filled_icustay_id'])], ignore_index=True)\n",
        "\n",
        "print(f\"Missing icustay_ids have been founded in {time.time() - start_time:.1f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wbkTC_vmWNg",
        "outputId": "7ca6a4b7-3139-43d3-899c-906bfa815dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing icustay_ids have been founded in 68.2 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the onset time of Sepsis\n",
        "def find_onset_for_patient(icustayid, abx_data, bacterio_data):\n",
        "    \"\"\"\n",
        "    Processes a single icustay_id to find the onset_time\n",
        "    Args:\n",
        "          icustayid_chunk (list): List of icustay_ids to be processed\n",
        "          abx_data (pd.DataFrame): Global ABx DataFrame\n",
        "          bacterio_data (pd.DataFrame): Global Bacterio DataFrame\n",
        "      Returns:\n",
        "          list: List of onset_time results\n",
        "    \"\"\"\n",
        "    # Filter ABx and Bacterio data for the current patient\n",
        "    ab_patient = abx_data[abx_data['icustay_id'] == icustayid].copy()\n",
        "    bact_patient = bacterio_data[bacterio_data['icustay_id'] == icustayid].copy()\n",
        "\n",
        "    # If either table has no data, return None\n",
        "    if ab_patient.empty or bact_patient.empty:\n",
        "        return None\n",
        "\n",
        "    # Perform merge only for this patient’s data\n",
        "    patient_events = pd.merge(ab_patient, bact_patient, on='icustay_id', suffixes=('_abx', '_bact'))\n",
        "\n",
        "    # Compute time differences\n",
        "    patient_events['diff_time'] = (patient_events['startdate'] - patient_events['charttime']).dt.total_seconds() / 3600\n",
        "\n",
        "    # Sort by absolute time differences\n",
        "    patient_events['abs_diff'] = patient_events['diff_time'].abs()\n",
        "    patient_events = patient_events.sort_values(by='abs_diff')\n",
        "\n",
        "    for _, row in patient_events.iterrows():\n",
        "        # Rule 1: Antibiotics (ABx) given first\n",
        "        if row['diff_time'] <= 0 and row['abs_diff'] <= 24:\n",
        "            return {'icustay_id': icustayid, 'onset_time': row['startdate']}\n",
        "        # Rule 2: Culture taken first\n",
        "        elif row['diff_time'] > 0 and row['diff_time'] <= 72:\n",
        "            return {'icustay_id': icustayid, 'onset_time': row['charttime']}\n",
        "\n",
        "    return None\n",
        "\n",
        "# Wrapper function to process one patient and update progress\n",
        "def process_and_update(icustayid, abx_data, bacterio_data, progress_counter):\n",
        "    result = find_onset_for_patient(icustayid, abx_data, bacterio_data)\n",
        "    progress_counter.value += 1\n",
        "    return result\n",
        "\n",
        "# --- Main Program ---\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Remove rows with NaN values in primary keys before processing\n",
        "    abx = abx.dropna(subset=['icustay_id', 'startdate']).copy()\n",
        "    bacterio = bacterio.dropna(subset=['icustay_id', 'charttime']).copy()\n",
        "\n",
        "    # Get unique list of icustay_id from both tables\n",
        "    all_icustayids = pd.concat([abx['icustay_id'], bacterio['icustay_id']]).dropna().unique()\n",
        "\n",
        "    print(f\"Total unique icustay_ids to process: {len(all_icustayids)}\")\n",
        "\n",
        "    num_cores = mp.cpu_count()\n",
        "    print(f\"Using {num_cores} CPU cores for parallel processing.\")\n",
        "\n",
        "   # Use multiprocessing.Manager to create shareable objects\n",
        "    with mp.Manager() as manager:\n",
        "        progress_counter = manager.Value('i', 0) # 'i' for integer\n",
        "\n",
        "        # Prepare arguments for each process; each task processes ONE icustay_id\n",
        "        task_args = [(icustayid, abx, bacterio, progress_counter) for icustayid in all_icustayids]\n",
        "\n",
        "        with mp.Pool(processes=num_cores) as pool:\n",
        "            # Use starmap_async to run processes asynchronously\n",
        "            result_async = pool.starmap_async(process_and_update, task_args)\n",
        "\n",
        "            # Print progress periodically in the main process\n",
        "            while not result_async.ready():\n",
        "                time.sleep(5)  # Wait 5 seconds before checking again\n",
        "                progress = progress_counter.value\n",
        "                total = len(all_icustayids)\n",
        "                percentage = (progress / total) * 100 if total > 0 else 0\n",
        "                print(f\"Processing... {progress}/{total} ({percentage:.1f}%)\")\n",
        "\n",
        "            # Retrieve results after all processes have completed\n",
        "            results = result_async.get()\n",
        "\n",
        "    final_results = [res for res in results if res is not None]\n",
        "\n",
        "    onset = pd.DataFrame(final_results)\n",
        "\n",
        "    if not onset.empty:\n",
        "        onset['icustay_id'] = onset['icustay_id'].astype(int)\n",
        "        onset['onset_time'] = pd.to_datetime(onset['onset_time'])\n",
        "        onset = onset.drop_duplicates(subset=['icustay_id'], keep='first')\n",
        "\n",
        "    # Print first 50 rows for verification\n",
        "    print(f\"\\nNumber of records with onset_time found: {len(onset)}\")\n",
        "    print(f\"Total time taken: {time.time() - start_time:.1f} seconds\")\n",
        "    print(\"\\n--- Onset Time Findings (first 50) ---\")\n",
        "    print(onset[['icustay_id', 'onset_time']].head(50).to_markdown(index=False))\n",
        "\n",
        "    # Saving results\n",
        "    onset.to_csv('onset.csv', index=False)\n",
        "    onset.to_csv(os.path.join(data_dir, 'onset.csv'), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIItJsOYmcaG",
        "outputId": "c8b44c03-9649-43c8-c87d-c7e6e2e9db24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique icustay_ids to process: 52953\n",
            "Using 8 CPU cores for parallel processing.\n",
            "Processing... 2053/52953 (3.9%)\n",
            "Processing... 6529/52953 (12.3%)\n",
            "Processing... 11739/52953 (22.2%)\n",
            "Processing... 17385/52953 (32.8%)\n",
            "Processing... 22363/52953 (42.2%)\n",
            "Processing... 29292/52953 (55.3%)\n",
            "Processing... 36719/52953 (69.3%)\n",
            "Processing... 37446/52953 (70.7%)\n",
            "\n",
            "Number of records with onset_time found: 26299\n",
            "Total time taken: 40.4 seconds\n",
            "\n",
            "--- Onset Time Findings (first 50) ---\n",
            "|   icustay_id | onset_time          |\n",
            "|-------------:|:--------------------|\n",
            "|       291788 | 2108-04-12 00:00:00 |\n",
            "|       253656 | 2162-05-16 21:10:00 |\n",
            "|       214619 | 2177-09-04 16:15:00 |\n",
            "|       239289 | 2177-03-15 00:00:00 |\n",
            "|       217590 | 2188-05-27 22:59:00 |\n",
            "|       252772 | 2109-08-22 00:00:00 |\n",
            "|       201668 | 2170-09-19 17:47:00 |\n",
            "|       222038 | 2185-04-17 12:15:00 |\n",
            "|       210325 | 2140-11-16 00:00:00 |\n",
            "|       200853 | 2194-07-06 00:00:00 |\n",
            "|       214267 | 2164-04-23 00:00:00 |\n",
            "|       245719 | 2115-02-27 15:50:00 |\n",
            "|       221136 | 2183-04-20 21:53:00 |\n",
            "|       270105 | 2183-03-23 15:40:00 |\n",
            "|       200580 | 2174-05-07 10:37:00 |\n",
            "|       289655 | 2195-08-12 00:00:00 |\n",
            "|       260971 | 2176-02-06 00:00:00 |\n",
            "|       279769 | 2179-09-22 00:00:00 |\n",
            "|       217069 | 2124-07-14 00:00:00 |\n",
            "|       201906 | 2178-12-26 00:00:00 |\n",
            "|       270174 | 2176-04-09 23:30:00 |\n",
            "|       296887 | 2106-10-01 15:40:00 |\n",
            "|       297065 | 2156-11-17 16:00:00 |\n",
            "|       220320 | 2108-09-29 17:13:00 |\n",
            "|       292080 | 2156-07-21 22:30:00 |\n",
            "|       216749 | 2108-04-23 00:00:00 |\n",
            "|       240176 | 2201-06-27 00:00:00 |\n",
            "|       254601 | 2201-06-21 23:56:00 |\n",
            "|       295421 | 2153-01-05 00:00:00 |\n",
            "|       291389 | 2157-09-30 11:30:00 |\n",
            "|       256633 | 2166-05-01 00:00:00 |\n",
            "|       202030 | 2198-09-08 10:11:00 |\n",
            "|       229201 | 2179-03-16 09:36:00 |\n",
            "|       282685 | 2196-09-28 15:29:00 |\n",
            "|       232928 | 2151-09-15 00:00:00 |\n",
            "|       276931 | 2179-09-22 00:00:00 |\n",
            "|       243747 | 2173-10-01 00:00:00 |\n",
            "|       241092 | 2171-08-17 22:54:00 |\n",
            "|       249287 | 2153-02-04 00:00:00 |\n",
            "|       289313 | 2103-05-20 18:30:00 |\n",
            "|       291381 | 2121-02-01 08:19:00 |\n",
            "|       260488 | 2135-12-27 00:00:00 |\n",
            "|       268122 | 2162-10-10 12:59:00 |\n",
            "|       286165 | 2107-01-22 00:00:00 |\n",
            "|       286268 | 2107-02-03 00:00:00 |\n",
            "|       233457 | 2146-11-17 19:30:00 |\n",
            "|       268883 | 2185-11-15 00:00:00 |\n",
            "|       295727 | 2195-10-10 00:00:00 |\n",
            "|       267595 | 2174-10-23 00:00:00 |\n",
            "|       274281 | 2115-10-08 20:03:00 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## 1. COMBINE BASE DATA\n",
        "# ----------------------------------------------------------------------\n",
        "# Use 'demog' and 'onset' from previous cells\n",
        "combined_df = pd.merge(onset, demog, on=['icustay_id'], how='left')\n",
        "combined_df['onset_time'] = pd.to_datetime(combined_df['onset_time'])\n",
        "combined_df = combined_df[['icustay_id', 'onset_time',\n",
        "                           'gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los']]\n",
        "print(f\"[DEBUG] Patients in combined_df (based on onset valid): {combined_df['icustay_id'].nunique()}\")\n",
        "# combined_filter_cols (pasien 20.115)\n",
        "combined_filter_cols = combined_df[['icustay_id', 'onset_time']].copy()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 2. FILTER AND MERGE DATA\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"Filtering and merging data...\")\n",
        "\n",
        "# Take icustay_id and onset_time for time-series filtering\n",
        "combined_filter_cols = combined_df[['icustay_id', 'onset_time']].copy()\n",
        "\n",
        "# Filter large tables first using inner merge\n",
        "vitals_filtered = pd.merge(all_vitals, combined_filter_cols, on='icustay_id', how='inner')\n",
        "lab_filtered = pd.merge(labU, combined_filter_cols, on='icustay_id', how='inner')\n",
        "\n",
        "# Drop all_vitals and labU, which are large, since they’ve already been filtered\n",
        "del all_vitals, labU\n",
        "gc.collect()\n",
        "\n",
        "# Standardize column names of value columns before concatenation\n",
        "if 'value' in lab_filtered.columns:\n",
        "    lab_filtered = lab_filtered.rename(columns={'value': 'valuenum'})\n",
        "elif 'valuenum' not in lab_filtered.columns:\n",
        "    print(\"WARNING: Laboratory value column not found (neither 'value' nor 'valuenum').\")\n",
        "\n",
        "# Now, merge the filtered data\n",
        "all_data_filtered = pd.concat([vitals_filtered, lab_filtered], ignore_index=True)\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del vitals_filtered, lab_filtered\n",
        "gc.collect()\n",
        "\n",
        "# Perform time-based filtering and deduplication on the smaller dataset\n",
        "all_data_filtered['charttime'] = pd.to_datetime(all_data_filtered['charttime'])\n",
        "all_data_filtered = all_data_filtered.drop_duplicates(subset=['icustay_id', 'charttime', 'itemid'], keep='first')\n",
        "all_data_filtered['time_diff_hours'] = (all_data_filtered['charttime'] - all_data_filtered['onset_time']).dt.total_seconds() / 3600\n",
        "\n",
        "# Create the final fully filtered DataFrame for 80 hrs\n",
        "filtered_df = all_data_filtered[(all_data_filtered['time_diff_hours'] >= -48) & (all_data_filtered['time_diff_hours'] <= 60)].copy()\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del all_data_filtered\n",
        "gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 3. MAPPING AND RESHAPING\n",
        "# ----------------------------------------------------------------------\n",
        "# Utilize BigQuery and MIMIC-3 Demo Ver. to check itemid labels\n",
        "column_name_map = {\n",
        "    # 1. Demographics and Scales\n",
        "    580: 'Weight_kg', 581: 'Weight_kg', 224639: 'Weight_kg', 226512: 'Weight_kg',\n",
        "    226707: 'Height_cm', 226730: 'Height_cm',\n",
        "    228096: 'RASS',\n",
        "\n",
        "    # 2. Vital Signs\n",
        "    211: 'HR', 220045: 'HR',\n",
        "    6: 'SysBP', 51: 'SysBP', 455: 'SysBP', 220179: 'SysBP', 225309: 'SysBP', 6701: 'SysBP', 224167: 'SysBP', 227243: 'SysBP',\n",
        "    52: 'MeanBP', 443: 'MeanBP', 456: 'MeanBP', 6702: 'MeanBP', 220052: 'MeanBP', 220181: 'MeanBP', 224322: 'MeanBP', 225312: 'MeanBP',\n",
        "    8368: 'DiaBP', 8440: 'DiaBP', 8441: 'DiaBP', 8555: 'DiaBP', 225310: 'DiaBP',\n",
        "    615: 'RR', 618: 'RR', 3337: 'RR', 3603: 'RR', 220210: 'RR', 224422: 'RR',\n",
        "    678: 'Temp_C', 3655: 'Temp_C', 223761: 'Temp_C', 223762: 'Temp_C',\n",
        "    646: 'SpO2', 220228: 'SpO2', 220277: 'SpO2', 834: 'SpO2',\n",
        "    190: 'FiO2', 3420: 'FiO2', 223835: 'FiO2', 727: 'FiO2',\n",
        "    535: 'Peak_Ins_Pressure', 224695: 'Peak_Ins_Pressure',\n",
        "    543: 'Plateau_Pressure', 224696: 'Plateau_Pressure',\n",
        "    444: 'Mean_Airway_Pressure', 224697: 'Mean_Airway_Pressure',\n",
        "    445: 'Minute_Volume', 448: 'Minute_Volume', 450: 'Minute_Volume', 224687: 'Minute_Volume',\n",
        "    654: 'Tidal_Volume', 681: 'Tidal_Volume', 684: 'Tidal_Volume', 96: 'Tidal_Volume', 97: 'Tidal_Volume', 98: 'Tidal_Volume', 2566: 'Tidal_Volume', 3050: 'Tidal_Volume', 3083: 'Tidal_Volume', 224421: 'Tidal_Volume', 224684: 'Tidal_Volume', 224686: 'Tidal_Volume',\n",
        "    113: 'CVP', 220074: 'CVP',\n",
        "    116: 'Cardiac_Index', 228177: 'Cardiac_Index', 228368: 'Cardiac_Index', 1366: 'Cardiac_Index', 1372: 'Cardiac_Index',\n",
        "    491: 'PAP', 492: 'PAP', 8448: 'PAP',\n",
        "\n",
        "    # 3. Lab Values\n",
        "    829: 'Potassium', 1535: 'Potassium', 3725: 'Potassium', 3792: 'Potassium', 4194: 'Potassium', 227442: 'Potassium', 227464: 'Potassium', 50971: 'Potassium', 50822: 'Potassium',\n",
        "    837: 'Sodium', 1536: 'Sodium', 3726: 'Sodium', 3803: 'Sodium', 4195: 'Sodium', 220645: 'Sodium', 226534: 'Sodium', 50983: 'Sodium', 50824: 'Sodium',\n",
        "    788: 'Chloride', 1523: 'Chloride', 3724: 'Chloride', 3747: 'Chloride', 4193: 'Chloride', 220602: 'Chloride', 226536: 'Chloride', 50902: 'Chloride', 50806: 'Chloride',\n",
        "    807: 'Glucose', 811: 'Glucose', 1529: 'Glucose', 3744: 'Glucose', 220621: 'Glucose', 225664: 'Glucose', 226537: 'Glucose', 50931: 'Glucose', 50809: 'Glucose',\n",
        "    821: 'Magnesium', 1532: 'Magnesium', 220635: 'Magnesium', 50960: 'Magnesium',\n",
        "    786: 'Calcium', 1522: 'Calcium', 112: 'Calcium', 3746: 'Calcium', 3766: 'Calcium', 225625: 'Calcium', 50808: 'Calcium', 50893: 'Calcium',\n",
        "    816: 'Ionized Calcium', 225667: 'Ionized Calcium',\n",
        "    814: 'Hb', 220228: 'Hb', 51222: 'Hb', 50811: 'Hb',\n",
        "    1127: 'WBC_Count', 1542: 'WBC_Count', 3834: 'WBC_Count', 4200: 'WBC_Count', 220546: 'WBC_Count', 51301: 'WBC_Count', 51300: 'WBC_Count',\n",
        "    828: 'Platelets_count', 3789: 'Platelets_count', 227457: 'Platelets_count', 51265: 'Platelets_count',\n",
        "    825: 'PTT', 1533: 'PTT', 3796: 'PTT', 227466: 'PTT', 51275: 'PTT',\n",
        "    824: 'PT', 1286: 'PT', 51274: 'PT',\n",
        "    780: 'Arterial_pH', 1126: 'Arterial_pH', 132: 'Arterial_pH', 139: 'Arterial_pH', 3839: 'Arterial_pH', 4753: 'Arterial_pH', 50820: 'Arterial_pH',\n",
        "    490: 'paO2', 3785: 'paO2', 3837: 'paO2', 3838: 'paO2', 50821: 'paO2', 779: 'paO2',\n",
        "    3784: 'paCO2', 3835: 'paCO2', 3836: 'paCO2', 778: 'paCO2',\n",
        "    74: 'Arterial_BE', 776: 'Arterial_BE', 108: 'Arterial_BE', 110: 'Arterial_BE', 136: 'Arterial_BE', 3736: 'Arterial_BE', 3740: 'Arterial_BE', 4196: 'Arterial_BE', 224828: 'Arterial_BE', 50802: 'Arterial_BE',\n",
        "    51: 'HCO3', 777: 'HCO3', 787: 'HCO3', 227443: 'HCO3', 50803: 'HCO3', 50804: 'HCO3',\n",
        "    86: 'Lactate', 1531: 'Lactate', 225668: 'Lactate', 50813: 'Lactate', 50882: 'Lactate',\n",
        "    781: 'BUN', 1162: 'BUN', 3737: 'BUN', 225624: 'BUN',\n",
        "    791: 'Creatinine', 1525: 'Creatinine', 114: 'Creatinine', 3750: 'Creatinine', 220615: 'Creatinine', 50912: 'Creatinine', 51081: 'Creatinine',\n",
        "    769: 'SGPT(ALT)', 3802: 'SGPT(ALT)', 50861: 'SGPT(ALT)',\n",
        "    770: 'SGOT(AST)', 3801: 'SGOT(AST)', 50878: 'SGOT(AST)',\n",
        "    848: 'Total_bili', 1538: 'Total_bili', 225690: 'Total_bili', 51464: 'Total_bili',\n",
        "    815: 'INR', 1530: 'INR', 227467: 'INR', 51237: 'INR',\n",
        "    813: 'HCT', 115: 'HCT', 3761: 'HCT', 220545: 'HCT', 226540: 'HCT', 51221: 'HCT', 50810: 'HCT',\n",
        "    803: 'Direct_Bili', 1527: 'Direct_Bili', 225651: 'Direct_Bili', 50883: 'Direct_Bili',\n",
        "    772: 'Albumin', 1521: 'Albumin', 107: 'Albumin', 3727: 'Albumin', 227456: 'Albumin', 50862: 'Albumin',\n",
        "    851: 'Troponin', 227429: 'Troponin', 51003: 'Troponin', 51002: 'Troponin',\n",
        "\n",
        "    # 4. Oxygenation\n",
        "    823: 'Mixed_Venous_O2', 227686: 'Mixed_Venous_O2',\n",
        "\n",
        "    # 5. Coagulation\n",
        "    768: 'ACT', 1520: 'ACT', 1671: 'ACT', 220507: 'ACT',\n",
        "\n",
        "    # 6. Others\n",
        "    626: 'SVR',\n",
        "    198: 'GCS',\n",
        "    1817: 'ETCO2',\n",
        "    470: 'O2_Flow_Main', 471: 'O2_Flow_Main', 223834: 'O2_Flow_Main',\n",
        "    467: 'O2_Delivery_Device',\n",
        "    227287: 'O2_Flow_Add',\n",
        "    160: 'Ectopy_Frequency',\n",
        "    849: 'Total_Protein', 1539: 'Total_Protein', 126: 'Total_Protein', 3807: 'Total_Protein', 50976: 'Total_Protein',\n",
        "    227444: 'CRP', 50889: 'CRP',\n",
        "    3799: 'RBC', 4197: 'RBC', 51279: 'RBC',\n",
        "    224691: 'Flow_Rate'\n",
        "}\n",
        "\n",
        "# Create mapping dictionary with default itemid\n",
        "itemid_map_with_default = {k: v for k, v in column_name_map.items()}\n",
        "unique_itemids = filtered_df['itemid'].unique()\n",
        "for itemid in unique_itemids:\n",
        "    if itemid not in itemid_map_with_default:\n",
        "        itemid_map_with_default[itemid] = f'ITEMID_{itemid}'\n",
        "\n",
        "# Apply mapping to filtered_df\n",
        "unique_itemids = filtered_df['itemid'].unique()\n",
        "for itemid in unique_itemids:\n",
        "    if itemid not in itemid_map_with_default:\n",
        "        itemid_map_with_default[itemid] = f'ITEMID_{itemid}'\n",
        "filtered_df['itemid_name'] = filtered_df['itemid'].map(itemid_map_with_default)\n",
        "\n",
        "# Isolate the required time-series data columns\n",
        "pivot_cols_base = ['icustay_id', 'charttime', 'onset_time', 'itemid_name', 'valuenum']\n",
        "\n",
        "# Extract necessary columns from filtered_df for pivot operation\n",
        "df_time_series_base = filtered_df[pivot_cols_base].copy()\n",
        "\n",
        "# Delete unused filtered_df\n",
        "del filtered_df\n",
        "gc.collect()\n",
        "\n",
        "# Merge static data into df_time_series_base\n",
        "static_cols_needed = combined_df.copy()\n",
        "\n",
        "# Merge all static columns + onset_time (which may be duplicated; handled below)\n",
        "df_time_series_only = pd.merge(df_time_series_base, static_cols_needed, on='icustay_id', how='left', suffixes=('_base', '_static'))\n",
        "\n",
        "# Handle duplicate 'onset_time' columns that may be created by Pandas\n",
        "if 'onset_time_base' in df_time_series_only.columns and 'onset_time_static' in df_time_series_only.columns:\n",
        "    # Remove onset_time from the static data, keep the one from time-series (already filtered in Part 2)\n",
        "    df_time_series_only = df_time_series_only.drop(columns=['onset_time_static'])\n",
        "    df_time_series_only = df_time_series_only.rename(columns={'onset_time_base': 'onset_time'})\n",
        "elif 'onset_time_static' in df_time_series_only.columns:\n",
        "    # If there are only suffixes, remove the suffixes\n",
        "    df_time_series_only = df_time_series_only.rename(columns={'onset_time_static': 'onset_time'})\n",
        "\n",
        "# Hapus kolom statis ganda lainnya yang mungkin tidak sengaja terbawa dengan suffix\n",
        "cols_to_drop = [col for col in df_time_series_only.columns if col.endswith('_static')]\n",
        "df_time_series_only = df_time_series_only.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del df_time_series_base\n",
        "gc.collect()\n",
        "\n",
        "# Perform pivot table operation (ONLY on data containing time-series)\n",
        "reformat_valid = pd.pivot_table(\n",
        "    df_time_series_only,\n",
        "    index=['icustay_id', 'onset_time', 'charttime',\n",
        "           'gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los'],\n",
        "    columns='itemid_name',\n",
        "    values='valuenum',\n",
        "    aggfunc='mean'\n",
        ").reset_index()\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del df_time_series_only\n",
        "gc.collect()\n",
        "\n",
        "# Identify missing patients\n",
        "all_patient_ids = combined_df['icustay_id'].unique()\n",
        "existing_ids = reformat_valid['icustay_id'].unique()\n",
        "missing_ids = np.setdiff1d(all_patient_ids, existing_ids)\n",
        "print(f\"[DEBUG] Missing patients (static data only): {len(missing_ids)}\")\n",
        "\n",
        "# Create a DataFrame for missing patients (Static Data Only)\n",
        "missing_patients_df = combined_df[combined_df['icustay_id'].isin(missing_ids)].copy()\n",
        "\n",
        "# Generate dummy rows with the same columns as reformat_valid, but all time-series features as NaN\n",
        "# Retrieve the list of time-series columns from reformat_valid\n",
        "time_series_cols = [col for col in reformat_valid.columns if col not in combined_df.columns]\n",
        "\n",
        "# Add time-series columns with NaN values\n",
        "for col in time_series_cols:\n",
        "    missing_patients_df[col] = np.nan\n",
        "\n",
        "# Ensure missing_patients_df has charttime = NaT (for consistency)\n",
        "missing_patients_df['charttime'] = pd.NaT\n",
        "missing_patients_df = missing_patients_df[reformat_valid.columns]\n",
        "# Combine all data back together\n",
        "reformat = pd.concat([reformat_valid, missing_patients_df], ignore_index=True)\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del reformat_valid, missing_patients_df\n",
        "gc.collect()\n",
        "\n",
        "# Saving results\n",
        "reformat.to_csv(os.path.join(data_dir, 'reformat_pivot.csv'), index=False)\n",
        "print(f\"DataFrame 'reformat_pivot.csv' has been made in {time.time() - start_time:.1f} seconds.\")\n",
        "print(f\"Total unique icustay_ids: {reformat['icustay_id'].nunique()}\")\n",
        "\n",
        "# Delete all large variables\n",
        "del itemid_map_with_default, unique_itemids, column_name_map\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "dPIesmMensoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48ba1c9-f0a1-4dcb-a45e-006fd47fcf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Patients in combined_df (based on onset valid): 26299\n",
            "Filtering and merging data...\n",
            "[DEBUG] Missing patients (static data only): 16425\n",
            "DataFrame 'reformat_pivot.csv' has been made in 108.8 seconds.\n",
            "Total unique icustay_ids: 26299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## P95 CAPPING FUNCTION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def cap_p95_upper(df, col_names):\n",
        "    \"\"\"\n",
        "    Applies P95 upper limit capping to specified columns.\n",
        "    Values > P95 are replaced by the P95 value.\n",
        "    \"\"\"\n",
        "    print(f\"Applying P95 upper capping to: {col_names}\")\n",
        "\n",
        "    for col in col_names:\n",
        "        if col in df.columns:\n",
        "            # Calculate value P95\n",
        "            p95_value = df[col].quantile(0.95)\n",
        "\n",
        "            # Apply capping\n",
        "            df.loc[df[col] > p95_value, col] = p95_value\n",
        "\n",
        "            print(f\"  - {col}: Max value capped at {p95_value:.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "features_to_cap = ['SGOT(AST)', 'SGPT(ALT)', 'BUN']\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## OUTLIER CLEANING\n",
        "# ----------------------------------------------------------------------\n",
        "# --- Main Program ---\n",
        "def delout(df, col_name, threshold, direction):\n",
        "    \"\"\"\n",
        "    Replace values outside the threshold with NaN.\n",
        "    The 'direction' parameter can be either 'above' or 'below'.\n",
        "    \"\"\"\n",
        "    if col_name not in df.columns:\n",
        "        return df\n",
        "\n",
        "    if direction == 'above':\n",
        "        df.loc[df[col_name] > threshold, col_name] = np.nan\n",
        "    elif direction == 'below':\n",
        "        df.loc[df[col_name] < threshold, col_name] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "# Clinical Outlier Rules (Komorowski (2018) & MIMIC-Extract)\n",
        "outlier_rules_revised = {\n",
        "    # 1. Demographics and Scales\n",
        "    'Weight_kg': [(0, 'below'), (250, 'above')],\n",
        "    'Height_cm': [(0, 'below'), (240, 'above')],\n",
        "    'GCS': [(3, 'below'), (15, 'above')],\n",
        "\n",
        "    # 2. Vital Signs\n",
        "    'HR': [(20, 'below'), (350, 'above')],\n",
        "    'SysBP': [(10, 'below'), (375, 'above')],\n",
        "    'DiaBP': [(10, 'below'), (375, 'above')],\n",
        "    'MeanBP': [(14, 'below'), (330, 'above')],\n",
        "    'RR': [(5, 'below'), (100, 'above')],\n",
        "    'SpO2': [(70, 'below'), (100, 'above')],\n",
        "    'Temp_C': [(26, 'below'), (45, 'above')],\n",
        "    'CVP': [(-10, 'below'), (50, 'above')],\n",
        "\n",
        "    # 3. Lab Values\n",
        "    'PEEP_Set': [(0, 'below'), (25, 'above')],\n",
        "    'paO2': [(32, 'below'), (700, 'above')],\n",
        "    'paCO2': [(10, 'below'), (200, 'above')],\n",
        "    'Arterial_pH': [(6.3, 'below'), (8.4, 'above')],\n",
        "    'Lactate': [(0.4, 'below'), (100, 'above')],\n",
        "    'Arterial_BE': [(-50, 'below'), (50, 'above')],\n",
        "    'Tidal_Volume': [(50, 'below'), (1800, 'above')],\n",
        "    'Minute_Volume': [(0.1, 'below'), (50, 'above')],\n",
        "    'Flow_Rate': [(0, 'below'), (70, 'above')],\n",
        "\n",
        "    # b. Metabolic\n",
        "    'Potassium': [(2, 'below'), (12, 'above')],\n",
        "    'Sodium': [(50, 'below'), (225, 'above')],\n",
        "    'Chloride': [(50, 'below'), (175, 'above')],\n",
        "    'Glucose': [(33, 'below'), (2000, 'above')],\n",
        "    'Creatinine': [(0.1, 'below'), (60, 'above')],\n",
        "    'Magnesium': [(0, 'below'), (20, 'above')],\n",
        "    'Calcium': [(0, 'below'), (20, 'above')],\n",
        "    'Ionized Calcium': [(0, 'below'), (5, 'above')],\n",
        "    'BUN': [(0, 'below'), (150, 'above')],\n",
        "    'HCO3': [(0, 'below'), (50, 'above')],\n",
        "\n",
        "    # c. Liver function\n",
        "    'SGOT(AST)': [(6, 'below'), (200, 'above')],\n",
        "    'SGPT(ALT)': [(2, 'below'), (150, 'above')],\n",
        "    'PT': [(9.9, 'below'), (97.1, 'above')],\n",
        "    'INR': [(0.8, 'below'), (15, 'above')],\n",
        "    'PTT': [(10, 'below'), (150, 'above')],\n",
        "\n",
        "    # d. Hematology\n",
        "    'Hb': [(2, 'below'), (25, 'above')],\n",
        "    'HCT': [(5, 'below'), (75, 'above')],\n",
        "    'WBC_Count': [(0, 'below'), (1000, 'above')],\n",
        "    'Platelets_count': [(10, 'below'), (2000, 'above')],\n",
        "\n",
        "    # 6. Fluid Balance\n",
        "    'input_total': [(0, 'below'), (300000, 'above')],\n",
        "    'uo_total': [(0, 'below'), (300000, 'above')],\n",
        "    'cumulated_balance': [(-100000, 'below'), (300000, 'above')]\n",
        "}\n",
        "\n",
        "\n",
        "# IQR Function\n",
        "def iqr_outlier_removal_robust(df, factor=3.0):\n",
        "    \"\"\"Replace statistical outliers (IQR 3.0) with NaN and ensure non-negative values.\"\"\"\n",
        "\n",
        "    cols_to_exclude = ['icustay_id', 'charttime', 'onset_time', 'timestep_id', 'Total_bili', 'Direct_Bili',\n",
        "                       'gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los', 'Temp_F']\n",
        "\n",
        "    NON_NEGATIVE_COLS = list(outlier_rules_revised.keys())\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Ensure icustay_id is never included in cols_to_clean\n",
        "    cols_to_clean = [col for col in numeric_cols if col not in cols_to_exclude]\n",
        "\n",
        "    for col in cols_to_clean:\n",
        "        if df[col].isnull().all() or df[col].nunique() < 2: continue\n",
        "\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - factor * IQR\n",
        "        upper_bound = Q3 + factor * IQR\n",
        "\n",
        "        if IQR > 0 and df[col].std() > 0.001:\n",
        "            # 1. Apply Upper IQR Limit\n",
        "            df.loc[df[col] > upper_bound, col] = np.nan\n",
        "            # 2. Apply Lower IQR Limit (with non-negative protection/floor capping)\n",
        "            current_lower_bound = lower_bound\n",
        "\n",
        "            if col in NON_NEGATIVE_COLS and lower_bound < 0:\n",
        "                current_lower_bound = 0\n",
        "            df.loc[df[col] < current_lower_bound, col] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "# Outlier Cleaning Execution\n",
        "available_features = reformat.columns.tolist()\n",
        "\n",
        "print(\"Outlier cleaning starts...\")\n",
        "\n",
        "# 1. Create a Copy of the Data\n",
        "reformat_cleaned = reformat.copy()\n",
        "\n",
        "# 2. Standardize FiO2\n",
        "if 'FiO2' in available_features:\n",
        "    print(\"Standardizing FiO2 (converting % to fraction and capping)...\")\n",
        "\n",
        "    # a. Convert percentage values (>1.0 up to 100) into fractions\n",
        "    mask_persen = (reformat_cleaned['FiO2'] > 1.0) & (reformat_cleaned['FiO2'] <= 100.0)\n",
        "    reformat_cleaned.loc[mask_persen, 'FiO2'] /= 100\n",
        "\n",
        "    # b. Cap maximum FiO2 at 1.0\n",
        "    reformat_cleaned.loc[reformat_cleaned['FiO2'] > 1.0, 'FiO2'] = 1.0\n",
        "\n",
        "    # c. Cap minimum FiO2 at 0.21\n",
        "    reformat_cleaned.loc[reformat_cleaned['FiO2'] < 0.21, 'FiO2'] = 0.21\n",
        "\n",
        "\n",
        "# 3. Iterate and Clean Outliers based on Clinical Rules (Hard Capping)\n",
        "print(\"Applying improved clinical outlier rules (hard capping)...\")\n",
        "for col, rules in outlier_rules_revised.items():\n",
        "    if col in available_features:\n",
        "        for threshold, direction in rules:\n",
        "            reformat_cleaned = delout(reformat_cleaned, col, threshold, direction)\n",
        "\n",
        "# Apply P95 Capping\n",
        "reformat_cleaned = cap_p95_upper(reformat_cleaned, features_to_cap)\n",
        "\n",
        "# 4. Remove Statistical Outliers (IQR)\n",
        "print(\"Applying IQR (factor 3.0) outlier removal and floor capping...\")\n",
        "reformat_cleaned = iqr_outlier_removal_robust(reformat_cleaned, factor=1.5)\n",
        "\n",
        "# Saving results\n",
        "try:\n",
        "    final_unique_icustayids = reformat_cleaned['icustay_id'].nunique()\n",
        "    reformat_cleaned.to_csv(os.path.join(data_dir, 'reformat_cleaned.csv'), index=False)\n",
        "\n",
        "    try:\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f\"\\nOutlier clearance complete. 'reformat_cleaned.csv' saved in {time_elapsed:.1f} seconds.\")\n",
        "    except NameError:\n",
        "        print(\"\\nOutlier clearance complete. 'reformat_cleaned.csv' saved.\")\n",
        "\n",
        "    print(f\"Total unique icustay_ids: {final_unique_icustayids}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nWarning: 'data_dir' or 'start_time' is not defined. Skipping time logging and saving.\")\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del outlier_rules_revised, available_features\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3lMNbbvBEk",
        "outputId": "8b922e12-ab19-46bb-d8d0-4f5e5e9da00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlier cleaning starts...\n",
            "Standardizing FiO2 (converting % to fraction and capping)...\n",
            "Applying improved clinical outlier rules (hard capping)...\n",
            "Applying P95 upper capping to: ['SGOT(AST)', 'SGPT(ALT)', 'BUN']\n",
            "  - SGOT(AST): Max value capped at 158.00\n",
            "  - SGPT(ALT): Max value capped at 116.00\n",
            "  - BUN: Max value capped at 88.00\n",
            "Applying IQR (factor 3.0) outlier removal and floor capping...\n",
            "\n",
            "Outlier clearance complete. 'reformat_cleaned.csv' saved in 145.7 seconds.\n",
            "Total unique icustay_ids: 26299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global SAH Hold Limits\n",
        "# Create the SAH function according to:\n",
        "# \"Detecting Hazardous Intensive Care PatientEpisodes Using Real-time Mortality Models\" (Hug, 2006)\n",
        "\n",
        "HOLD_LIMITS_SEC = {\n",
        "    # 1. Demographics and Scales\n",
        "    'GCS': 28 * 3600,             # From Table 3.1\n",
        "    'Weight_kg': 28 * 3600,       # Taken from Weight (581) in Table 3.1\n",
        "    'Height_cm': 28 * 3600,       # Considered constant, use 28 hours\n",
        "    'RASS': 5 * 3600,             # RASS/Sedation changes frequently, use the second shortest limit (5 hours)\n",
        "\n",
        "    # 2. Vital Signs (Usually 4 hours)\n",
        "    'HR': 4 * 3600,               # From Table 3.1\n",
        "    'RR': 4 * 3600,               # Resp Rate (RESP) in Table 3.1 is 4 hours\n",
        "    'SysBP': 4 * 3600,            # SBP in Table 3.1 is 4 hours\n",
        "    'DiaBP': 4 * 3600,            # DBP in Table 3.1 is 4 hours\n",
        "    'MeanBP': 4 * 3600,           # MAP in Table 3.1 is 4 hours\n",
        "    'SpO2': 4 * 3600,             # From Table 3.1\n",
        "    'CVP': 4 * 3600,              # From Table 3.1\n",
        "    'Temp_C': 28 * 3600,          # Temp (678) in Table 3.1 is 28 hours\n",
        "    'FiO2': 28 * 3600,            # Taken from FiO2Set in Table 3.1\n",
        "    'Peak_Ins_Pressure': 28 * 3600, # Taken from PIP in Table 3.1\n",
        "    'Plateau_Pressure': 28 * 3600, # Taken from PlateauPres in Table 3.1\n",
        "    'Mean_Airway_Pressure': 28 * 3600, # Assumed equivalent to PlateauPres (28 hours)\n",
        "    'Minute_Volume': 28 * 3600,   # Not listed in table, use 28 hours (default)\n",
        "    'Tidal_Volume': 28 * 3600,    # Taken from TidVolObs/Set/Spon in Table 3.1\n",
        "    'PAP': 4 * 3600,              # PAPMean in Table 3.1 is 4 hours\n",
        "    'Cardiac_Index': 10 * 3600,   # From Table 3.1 (10 hours)\n",
        "    'SVR': 10 * 3600,             # From Table 3.1 (10 hours)\n",
        "\n",
        "    # 3. a. Lab Values & Hematology (Generally 28 hours)\n",
        "    'Potassium': 28 * 3600,       # From Table 3.1\n",
        "    'Sodium': 28 * 3600,          # From Table 3.1\n",
        "    'Chloride': 28 * 3600,        # From Table 3.1\n",
        "    'Glucose': 28 * 3600,         # From Table 3.1\n",
        "    'Magnesium': 28 * 3600,       # From Table 3.1\n",
        "    'Calcium': 28 * 3600,         # From Table 3.1\n",
        "    'Ionized Calcium': 28 * 3600, # From Table 3.1 (IonCa)\n",
        "    'BUN': 28 * 3600,             # From Table 3.1\n",
        "    'Creatinine': 28 * 3600,      # From Table 3.1\n",
        "    'SGOT(AST)': 28 * 3600,       # AST in Table 3.1\n",
        "    'SGPT(ALT)': 28 * 3600,       # ALT in Table 3.1\n",
        "    'Total_bili': 28 * 3600,      # Total Bilirubin in Table 3.1\n",
        "    'Direct_Bili': 28 * 3600,     # Direct Bilirubin in Table 3.1\n",
        "    'Total_Protein': 28 * 3600,   # From Table 3.1\n",
        "    'Albumin': 28 * 3600,         # From Table 3.1\n",
        "    'Lactate': 28 * 3600,         # From Table 3.1\n",
        "    'HCT': 28 * 3600,             # Hematocrit in Table 3.1\n",
        "    'Hb': 28 * 3600,              # Hemoglobin in Table 3.1\n",
        "    'WBC_Count': 28 * 3600,       # From Table 3.1\n",
        "    'Platelets_count': 28 * 3600, # Platelets in Table 3.1\n",
        "    'INR': 28 * 3600,             # From Table 3.1\n",
        "    'PTT': 28 * 3600,             # From Table 3.1\n",
        "    'PT': 28 * 3600,              # From Table 3.1\n",
        "    'RBC': 28 * 3600,             # RBC Count in Table 3.1\n",
        "\n",
        "    # 3. b. ABG (28 hours)\n",
        "    'Arterial_pH': 28 * 3600,     # Art pH in Table 3.1\n",
        "    'paCO2': 28 * 3600,           # Art PaCO2 in Table 3.1\n",
        "    'paO2': 28 * 3600,            # Art PaO2 in Table 3.1\n",
        "    'Arterial_BE': 28 * 3600,     # Art Base Excess in Table 3.1\n",
        "    'HCO3': 28 * 3600,            # Art CO2 in Table 3.1\n",
        "\n",
        "    # 6. Variables Not Found in Table 3.1 (Use 28 hours, except RASS/Flow)\n",
        "    'Ionized Calcium': 28 * 3600,\n",
        "    'Troponin': 28 * 3600,\n",
        "    'Mixed_Venous_O2': 28 * 3600,\n",
        "    'Ectopy_Frequency': 3 * 3600, # From Table 3.2 (3 hours)\n",
        "    'ACT': 28 * 3600,\n",
        "    'ETCO2': 28 * 3600,\n",
        "    'O2_Flow_Main': 4 * 3600,     # Assumed same as HR/BP (4 hours)\n",
        "    'O2_Flow_Add': 4 * 3600,      # Assumed same as HR/BP (4 hours)\n",
        "    'Flow_Rate': 4 * 3600,        # Assumed same as HR/BP (4 hours)\n",
        "    'CRP': 28 * 3600,\n",
        "}\n",
        "\n",
        "# Default limit for variables not listed\n",
        "DEFAULT_HOLD_LIMIT = 28 * 3600"
      ],
      "metadata": {
        "id": "7RgaqL5o3yUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## IMPUTATION FROM EXISTING VALUE\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"Data imputation from existing values starts...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create DataFrame copy to store imputation results\n",
        "# Use 'reformat_cleaned' DataFrame in memory from previous cell run\n",
        "temp_reformat_impute = reformat_cleaned.copy()\n",
        "available_columns = temp_reformat_impute.columns\n",
        "\n",
        "# Isolate Active and Static Patients\n",
        "original_rows = len(temp_reformat_impute)\n",
        "print(f\"Original rows count: {original_rows}\")\n",
        "\n",
        "# 1. Identify Dummy Patients to be EXCLUDED from SAH/Relational Imputation\n",
        "missing_patient_ids = temp_reformat_impute[temp_reformat_impute['charttime'].isnull()]['icustay_id'].unique()\n",
        "\n",
        "# 2. Active Patients (Only those with non-NaN 'charttime') - These will be imputed\n",
        "reformat_active = temp_reformat_impute.dropna(subset=['charttime']).copy()\n",
        "print(f\"Total rows for Imputation/SAH (Active Patients): {len(reformat_active)}\")\n",
        "\n",
        "# 3. Static Dummy Patients (No need for SAH/Relational Imputation)\n",
        "# Take ONLY one row per icustay_id\n",
        "reformat_static_dummy = temp_reformat_impute[temp_reformat_impute['icustay_id'].isin(missing_patient_ids)].drop_duplicates(subset=['icustay_id']).copy()\n",
        "print(f\"Total unique icustay_ids to SKIP SAH (Static Patients): {len(reformat_static_dummy['icustay_id'].unique())}\")\n",
        "\n",
        "# Delete unused temporary DataFrames\n",
        "del temp_reformat_impute\n",
        "gc.collect()\n",
        "\n",
        "# Imputation for reformat_active\n",
        "# 1. GCS imputation from RASS (map RASS value to GCS)\n",
        "if 'GCS' in available_columns and 'RASS' in available_columns:\n",
        "    print(\"Applying GCS Imputation from RASS...\")\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] >= 0), 'GCS'] = 15\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -1), 'GCS'] = 14\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -2), 'GCS'] = 12\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -3), 'GCS'] = 11\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -4), 'GCS'] = 6\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -5), 'GCS'] = 3\n",
        "\n",
        "# 2. FiO2 imputation and conversion (percentage vs fraction)\n",
        "def harmonize_fio2(df):\n",
        "    \"\"\"\n",
        "    Converts FiO2 values from percentage (21–100) to fraction (0.21–1.0)\n",
        "    and applies minimum/maximum capping\n",
        "    \"\"\"\n",
        "    if 'FiO2' in df.columns:\n",
        "        # 1. CONVERSION: Divide values > 1.0 and <= 100 by 100\n",
        "        mask_persen = (df['FiO2'] > 1.0) & (df['FiO2'] <= 100.0)\n",
        "        df.loc[mask_persen, 'FiO2'] /= 100\n",
        "\n",
        "        # 2. Cap maximum FiO2 at 1.0 (for cases > 1.0 that remain uncorrected)\n",
        "        df.loc[df['FiO2'] > 1.0, 'FiO2'] = 1.0\n",
        "\n",
        "        # 3. Cap minimum FiO2 at 0.21\n",
        "        df.loc[df['FiO2'] < 0.21, 'FiO2'] = 0.21\n",
        "    return df\n",
        "\n",
        "# 3. Imputation of FiO2 from O2 Flow and Interface Type\n",
        "if 'FiO2' in available_columns:\n",
        "    print(\"Applying FiO2 Imputation from O2 Flow...\")\n",
        "    # Combine O2 Flow columns\n",
        "    reformat_active['O2_Flow_Combined'] = reformat_active.get('Flow_Rate').fillna(\n",
        "        reformat_active.get('O2_Flow_Main')).fillna(\n",
        "        reformat_active.get('O2_Flow_Add'))\n",
        "\n",
        "    # NO FiO2, YES O2 flow, no interface (0) OR cannula (2)\n",
        "    if 'O2_Delivery_Device' in available_columns:\n",
        "        mask = reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].notnull() & (reformat_active['O2_Delivery_Device'].isin([0, 2]))\n",
        "        # Commands must be ordered from highest to lowest threshold\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 15), 'FiO2'] = 70\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 12), 'FiO2'] = 62\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 10), 'FiO2'] = 55\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 8), 'FiO2'] = 50\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 6), 'FiO2'] = 44\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 5), 'FiO2'] = 40\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 4), 'FiO2'] = 36\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 3), 'FiO2'] = 32\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 2), 'FiO2'] = 28\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 1), 'FiO2'] = 24\n",
        "\n",
        "        # NO FiO2, YES O2 flow, face mask OR...\n",
        "        mask = reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].notnull() & (reformat_active['O2_Delivery_Device'].isin([1, 3, 4, 5, 6, 9, 10]))\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 15), 'FiO2'] = 75\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 12), 'FiO2'] = 69\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 10), 'FiO2'] = 66\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 8), 'FiO2'] = 58\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 6), 'FiO2'] = 40\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 4), 'FiO2'] = 36\n",
        "\n",
        "        # NO FiO2, YES O2 flow, Non rebreather mask\n",
        "        mask = reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].notnull() & (reformat_active['O2_Delivery_Device'] == 7)\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] >= 15), 'FiO2'] = 100\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] >= 10), 'FiO2'] = 90\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] < 10), 'FiO2'] = 80\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 8), 'FiO2'] = 70\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 6), 'FiO2'] = 60\n",
        "\n",
        "        # NO FiO2, NO O2 flow\n",
        "        # reformat_active.loc[reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].isnull(), 'FiO2'] = 21\n",
        "\n",
        "    # Re-run FiO2 harmonization/capping\n",
        "    reformat_active = harmonize_fio2(reformat_active)\n",
        "\n",
        "    # Hapus kolom pembantu\n",
        "    reformat_active.drop(columns=['O2_Flow_Combined'], inplace=True)\n",
        "\n",
        "# 4. Imputation of BP (using 2 of 3 available values)\n",
        "if all(col in available_columns for col in ['SysBP', 'MeanBP', 'DiaBP']):\n",
        "    print(\"Applying BP Imputation (3-point method)...\")\n",
        "    reformat_active.loc[reformat_active['DiaBP'].isnull() & reformat_active['SysBP'].notnull() & reformat_active['MeanBP'].notnull(), 'DiaBP'] = (3 * reformat_active['MeanBP'] - reformat_active['SysBP']) / 2\n",
        "    reformat_active.loc[reformat_active['MeanBP'].isnull() & reformat_active['SysBP'].notnull() & reformat_active['DiaBP'].notnull(), 'MeanBP'] = (reformat_active['SysBP'] + 2 * reformat_active['DiaBP']) / 3\n",
        "    reformat_active.loc[reformat_active['SysBP'].isnull() & reformat_active['MeanBP'].notnull() & reformat_active['DiaBP'].notnull(), 'SysBP'] = 3 * reformat_active['MeanBP'] - 2 * reformat_active['DiaBP']\n",
        "\n",
        "    for col in ['SysBP', 'MeanBP', 'DiaBP']:\n",
        "        # Floor capping setelah kalkulasi\n",
        "        reformat_active.loc[reformat_active[col] < 0, col] = 0\n",
        "\n",
        "# 5. Imputation of Temp (Convert °C to °F and vice versa)\n",
        "if 'Temp_C' in available_columns and 'Temp_F' in available_columns:\n",
        "    print(\"Applying Temperature Conversion/Correction...\")\n",
        "    # a. Move Temp_C values that were recorded in Fahrenheit\n",
        "    mask_c_is_f = (reformat_active['Temp_C'] > 45) & (reformat_active['Temp_C'] < 100) & reformat_active['Temp_F'].isnull()\n",
        "    reformat_active.loc[mask_c_is_f, 'Temp_F'] = reformat_active['Temp_C']\n",
        "    reformat_active.loc[mask_c_is_f, 'Temp_C'] = np.nan\n",
        "\n",
        "    # b. Move Temp_F values that were recorded in Celsius\n",
        "    mask_f_is_c = (reformat_active['Temp_F'] > 15) & (reformat_active['Temp_F'] < 45) & reformat_active['Temp_C'].isnull()\n",
        "    reformat_active.loc[mask_f_is_c, 'Temp_C'] = reformat_active['Temp_F']\n",
        "    reformat_active.loc[mask_f_is_c, 'Temp_F'] = np.nan\n",
        "\n",
        "    # c. Temperature Conversion\n",
        "    reformat_active.loc[reformat_active['Temp_F'].isnull() & reformat_active['Temp_C'].notnull(), 'Temp_F'] = reformat_active['Temp_C'] * 1.8 + 32\n",
        "    reformat_active.loc[reformat_active['Temp_C'].isnull() & reformat_active['Temp_F'].notnull(), 'Temp_C'] = (reformat_active['Temp_F'] - 32) / 1.8\n",
        "\n",
        "# 6. Imputation of Hb/HCT\n",
        "if 'Hb' in available_columns and 'HCT' in available_columns:\n",
        "    print(\"Applying Hb/HCT Imputation...\")\n",
        "    # Gunakan regresi empiris/klinis untuk mengimputasi\n",
        "    reformat_active.loc[reformat_active['HCT'].isnull() & reformat_active['Hb'].notnull(), 'HCT'] = reformat_active['Hb'] * 2.862 + 1.216\n",
        "    reformat_active.loc[reformat_active['Hb'].isnull() & reformat_active['HCT'].notnull(), 'Hb'] = (reformat_active['HCT'] - 1.216) / 2.862\n",
        "\n",
        "    for col in ['Hb', 'HCT']:\n",
        "        reformat_active.loc[reformat_active[col] < 0, col] = 0\n",
        "\n",
        "# 7. Imputation of Bilirubin\n",
        "# Lower threshold (mg/dL)\n",
        "TOTAL_BILI_MIN_CAP = 0.2\n",
        "DIRECT_BILI_MIN_CAP = 0.1\n",
        "\n",
        "if 'Total_bili' in available_columns and 'Direct_Bili' in available_columns:\n",
        "    print(\"Applying Bilirubin Imputation...\")\n",
        "    reformat_active.loc[reformat_active['Direct_Bili'].isnull() & reformat_active['Total_bili'].notnull(), 'Direct_Bili'] = reformat_active['Total_bili'] * 0.6934 - 0.1752\n",
        "    reformat_active.loc[reformat_active['Total_bili'].isnull() & reformat_active['Direct_Bili'].notnull(), 'Total_bili'] = (reformat_active['Direct_Bili'] + 0.1752) / 0.6934\n",
        "\n",
        "    for col in ['Total_bili', 'Direct_Bili']:\n",
        "        # Convert negative values into lower threshold (0.2 / 0.1 mg/dL)\n",
        "        if col == 'Total_bili':\n",
        "            min_cap = TOTAL_BILI_MIN_CAP\n",
        "        else:\n",
        "            min_cap = DIRECT_BILI_MIN_CAP\n",
        "\n",
        "        reformat_active.loc[reformat_active[col] < min_cap, col] = min_cap\n",
        "\n",
        "        print(f\"Minimum Capping for {col} applied at {min_cap} mg/dL (eliminates 0s and negatives).\")\n",
        "\n",
        "# Save the imputed 'reformat_active' to a CSV\n",
        "reformat_active.to_csv(os.path.join(data_dir, 'reformat_imputed.csv'), index=False)\n",
        "print(f\"\\nImputation from the existing value (Pasien Aktif only) has been done in {time.time() - start_time:.1f} seconds.\")\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del reformat_cleaned\n",
        "gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## TIME-LIMITED SAMPLE AND HOLD (SAH) IMPUTATION\n",
        "# ----------------------------------------------------------------------\n",
        "# Apply Sample and Hold (SAH) imputation to all variables\n",
        "print(\"\\nRunning Time-Limited Sample and Hold (SAH) imputation on active patients...\")\n",
        "sah_start_time = time.time()\n",
        "\n",
        "# Ensure the data is sorted by patient and time (use reformat_active)\n",
        "reformat_active = reformat_active.sort_values(by=['icustay_id', 'charttime'])\n",
        "\n",
        "# Set icustay_id and charttime as indices\n",
        "reformat_imputed_temp = reformat_active.set_index(['icustay_id', 'charttime'])\n",
        "\n",
        "# Initialize the output DataFrame\n",
        "reformat_imputed_sah = reformat_imputed_temp.copy()\n",
        "\n",
        "# Create charttime series for delta calculation\n",
        "charttime_series = reformat_imputed_temp.index.get_level_values('charttime').to_series()\n",
        "charttime_series.index = reformat_imputed_temp.index\n",
        "\n",
        "# Apply Time-Limited SAH (ffill dan bfill) for each column\n",
        "for col in reformat_imputed_temp.columns:\n",
        "    # Process only columns that contain missing values\n",
        "    if reformat_imputed_temp[col].isnull().any():\n",
        "\n",
        "        # Use the given function: HOLD_LIMITS_SEC and DEFAULT_HOLD_LIMIT\n",
        "        hold_limit = HOLD_LIMITS_SEC.get(col, DEFAULT_HOLD_LIMIT)\n",
        "\n",
        "        # 1. Forward Fill (ffill) Time-Limited\n",
        "        ffill_result = reformat_imputed_temp[col].groupby(level='icustay_id').ffill()\n",
        "        last_valid_time = charttime_series.where(reformat_imputed_temp[col].notnull()).groupby(level='icustay_id').ffill()\n",
        "        time_delta = (charttime_series - last_valid_time).dt.total_seconds()\n",
        "        mask_time_limit_exceeded = time_delta > hold_limit\n",
        "\n",
        "        reformat_imputed_sah[col] = ffill_result\n",
        "        reformat_imputed_sah.loc[mask_time_limit_exceeded, col] = np.nan\n",
        "\n",
        "\n",
        "        # 2. Backward Fill (bfill) Time-Limited\n",
        "        bfill_result = reformat_imputed_temp[col].groupby(level='icustay_id').bfill()\n",
        "        next_valid_time = charttime_series.where(reformat_imputed_temp[col].notnull()).groupby(level='icustay_id').bfill()\n",
        "        time_delta_bfill = (next_valid_time - charttime_series).dt.total_seconds()\n",
        "        mask_bfill_limit_exceeded = time_delta_bfill > hold_limit\n",
        "        mask_is_nan_after_ffill = reformat_imputed_sah[col].isnull()\n",
        "\n",
        "        bfill_valid = bfill_result.copy()\n",
        "        bfill_valid.loc[mask_bfill_limit_exceeded] = np.nan\n",
        "\n",
        "        # Merge bfill_valid results into columns that are still NaN after ffill\n",
        "        reformat_imputed_sah[col].fillna(bfill_valid, inplace=True)\n",
        "\n",
        "\n",
        "# Reset index for Active Patients after SAH\n",
        "reformat_imputed_sah = reformat_imputed_sah.reset_index()\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del reformat_imputed_temp, reformat_active\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# Final merge (Cohort reconciliation)\n",
        "print(\"\\nRecombining Active and Static Patients...\")\n",
        "\n",
        "time_series_cols_imputed = [col for col in reformat_imputed_sah.columns if col not in reformat_static_dummy.columns]\n",
        "\n",
        "# Only add columns that are MISSING in dummy\n",
        "for col in time_series_cols_imputed:\n",
        "    if col not in reformat_static_dummy.columns:\n",
        "        reformat_static_dummy[col] = np.nan\n",
        "\n",
        "# Ensure the column order is consistent\n",
        "reformat_static_dummy = reformat_static_dummy[reformat_imputed_sah.columns]\n",
        "\n",
        "# Combine SAH results with Static Patients\n",
        "reformat_imputed_final = pd.concat([reformat_imputed_sah, reformat_static_dummy], ignore_index=True)\n",
        "\n",
        "# Save the final imputed DataFrame\n",
        "reformat_imputed_final.to_csv(os.path.join(data_dir, 'reformat_imputed_final.csv'), index=False)\n",
        "print(f\"Final imputed data saved to 'reformat_imputed_final.csv' in {time.time() - sah_start_time:.1f} seconds.\")\n",
        "print(f\"Total unique icustay_ids in final file: {reformat_imputed_final['icustay_id'].nunique()}\")\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del reformat_imputed_sah, reformat_static_dummy\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYPBTez532pi",
        "outputId": "7363606e-76e8-47ce-95f6-c151666c23e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data imputation from existing values starts...\n",
            "Original rows count: 911266\n",
            "Total rows for Imputation/SAH (Active Patients): 894841\n",
            "Total unique icustay_ids to SKIP SAH (Static Patients): 16425\n",
            "Applying GCS Imputation from RASS...\n",
            "Applying FiO2 Imputation from O2 Flow...\n",
            "Applying BP Imputation (3-point method)...\n",
            "Applying Hb/HCT Imputation...\n",
            "Applying Bilirubin Imputation...\n",
            "Minimum Capping for Total_bili applied at 0.2 mg/dL (eliminates 0s and negatives).\n",
            "Minimum Capping for Direct_Bili applied at 0.1 mg/dL (eliminates 0s and negatives).\n",
            "\n",
            "Imputation from the existing value (Pasien Aktif only) has been done in 21.8 seconds.\n",
            "\n",
            "Running Time-Limited Sample and Hold (SAH) imputation on active patients...\n",
            "\n",
            "Recombining Active and Static Patients...\n",
            "Final imputed data saved to 'reformat_imputed_final.csv' in 37.2 seconds.\n",
            "Total unique icustay_ids in final file: 26299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missingness in reformat_imputed_final\n",
        "df = reformat_imputed_final.copy()\n",
        "\n",
        "# Set display options so Pandas prints all rows and columns\n",
        "# Setting to 'None' or a large number disables the display limit\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# --- Missingness Calculation Process ---\n",
        "\n",
        "# 1. Count Missing Values per Column\n",
        "missing_count = df.isnull().sum()\n",
        "\n",
        "# 2. Count Total Rows\n",
        "total_rows = len(df)\n",
        "\n",
        "# 3. Calculate Missing Value Percentage per Column\n",
        "missing_percent = (missing_count / total_rows) * 100\n",
        "\n",
        "# 4. Create a Missingness Report DataFrame\n",
        "missing_report = pd.DataFrame({\n",
        "    'Missing Count': missing_count,\n",
        "    'Missing Percent': missing_percent.round(2)\n",
        "})\n",
        "\n",
        "# Filter only columns that contain missing values\n",
        "missing_report = missing_report[missing_report['Missing Count'] > 0].sort_values(\n",
        "    by='Missing Percent', ascending=False\n",
        ")\n",
        "\n",
        "# Print the Report\n",
        "print(\"=== Missingness Report (After Relational Imputation & SAH) ===\")\n",
        "# Since row limit has been disabled, the entire report will be printed\n",
        "print(missing_report)\n",
        "\n",
        "# After finishing, reset the display limits to default (usually 60)\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc6wyHV0qL5j",
        "outputId": "ed5549ed-bbb4-4a00-c996-6f72c487e606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Missingness Report (After Relational Imputation & SAH) ===\n",
            "                      Missing Count  Missing Percent\n",
            "Flow_Rate                    910735            99.94\n",
            "Ectopy_Frequency             910080            99.87\n",
            "ITEMID_505                   909427            99.80\n",
            "RBC                          909338            99.79\n",
            "CRP                          905359            99.35\n",
            "ITEMID_194                   901542            98.93\n",
            "ACT                          901483            98.93\n",
            "Total_Protein                900140            98.78\n",
            "O2_Flow_Add                  888052            97.45\n",
            "Temp_C                       875219            96.04\n",
            "SVR                          860815            94.46\n",
            "PAP                          860223            94.40\n",
            "Cardiac_Index                853094            93.62\n",
            "Mixed_Venous_O2              833438            91.46\n",
            "Height_cm                    799264            87.71\n",
            "ITEMID_224700                768466            84.33\n",
            "Troponin                     759749            83.37\n",
            "SGPT(ALT)                    738514            81.04\n",
            "SGOT(AST)                    733485            80.49\n",
            "O2_Delivery_Device           674157            73.98\n",
            "RASS                         670404            73.57\n",
            "ITEMID_220339                662206            72.67\n",
            "ITEMID_506                   638740            70.09\n",
            "CVP                          610592            67.00\n",
            "paCO2                        587550            64.48\n",
            "paO2                         585508            64.25\n",
            "Arterial_pH                  580567            63.71\n",
            "Albumin                      572969            62.88\n",
            "O2_Flow_Main                 550565            60.42\n",
            "PT                           535424            58.76\n",
            "Plateau_Pressure             530478            58.21\n",
            "Ionized Calcium              499346            54.80\n",
            "Tidal_Volume                 479555            52.63\n",
            "Arterial_BE                  477921            52.45\n",
            "Total_bili                   473987            52.01\n",
            "Direct_Bili                  473987            52.01\n",
            "Lactate                      441514            48.45\n",
            "Peak_Ins_Pressure            435108            47.75\n",
            "Mean_Airway_Pressure         389595            42.75\n",
            "Minute_Volume                388977            42.69\n",
            "Weight_kg                    315286            34.60\n",
            "FiO2                         196687            21.58\n",
            "INR                          186356            20.45\n",
            "GCS                          183604            20.15\n",
            "PTT                          170215            18.68\n",
            "DiaBP                        129191            14.18\n",
            "SysBP                        128233            14.07\n",
            "Creatinine                    74450             8.17\n",
            "Calcium                       55699             6.11\n",
            "WBC_Count                     46668             5.12\n",
            "Platelets_count               44559             4.89\n",
            "Magnesium                     34059             3.74\n",
            "HCO3                          33240             3.65\n",
            "Sodium                        28881             3.17\n",
            "Hb                            28530             3.13\n",
            "HCT                           28530             3.13\n",
            "BUN                           24444             2.68\n",
            "Chloride                      24307             2.67\n",
            "SpO2                          24067             2.64\n",
            "Glucose                       23320             2.56\n",
            "Potassium                     23056             2.53\n",
            "MeanBP                        22079             2.42\n",
            "RR                            21744             2.39\n",
            "HR                            19569             2.15\n",
            "charttime                     16425             1.80\n",
            "morta_hosp                    14920             1.64\n",
            "morta_90                      14920             1.64\n",
            "elixhauser                       23             0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DEFINE TIME WINDOW PARAMETERS ---\n",
        "window_size_hours = 4\n",
        "TOTAL_STEPS = 20\n",
        "MIN_PRE_ONSET_HOURS = -24 # -24 jam (Inclusive)\n",
        "MAX_POST_ONSET_HOURS = 56 # +56 jam (Exclusive)\n",
        "\n",
        "# Offset calculation for Timestep IDs 1–20\n",
        "TIMESTEP_OFFSET = abs(MIN_PRE_ONSET_HOURS) // window_size_hours\n",
        "T0_BASELINE_STEP_ID = 1\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## DATA COMBINATION & ONSET PREPARATION\n",
        "# --------------------------------------------------------------------------------------\n",
        "print(\"Starting data combination and onset preparation...\")\n",
        "\n",
        "# 1. Combine Input Fluids (input_combined)\n",
        "inputMV_renamed = inputMV.rename(columns={'amount': 'input_volume', 'starttime': 'charttime'}).copy()\n",
        "inputCV_renamed = inputCV.rename(columns={'amount': 'input_volume'}).copy()\n",
        "input_combined = pd.concat([\n",
        "    inputMV_renamed[['icustay_id', 'charttime', 'input_volume']],\n",
        "    inputCV_renamed[['icustay_id', 'charttime', 'input_volume']]\n",
        "], ignore_index=True)\n",
        "input_combined['input_volume'] = input_combined['input_volume'].fillna(0)\n",
        "input_combined['charttime'] = pd.to_datetime(input_combined['charttime'], errors='coerce')\n",
        "\n",
        "# 2. Combine Vasopressors (vaso_combined)\n",
        "vasoMV_renamed = vasoMV.rename(columns={'starttime': 'charttime'}).copy()\n",
        "vaso_combined = pd.concat([\n",
        "    vasoMV_renamed[['icustay_id', 'charttime', 'rate_std']],\n",
        "    vasoCV[['icustay_id', 'charttime', 'rate_std']]\n",
        "], ignore_index=True)\n",
        "vaso_combined['charttime'] = pd.to_datetime(vaso_combined['charttime'], errors='coerce')\n",
        "\n",
        "# 3. Retrieve onset time (T=0)\n",
        "onset_df = reformat_imputed_final[['icustay_id', 'onset_time']].drop_duplicates().copy()\n",
        "onset_df['onset_time'] = pd.to_datetime(onset_df['onset_time'], errors='coerce')\n",
        "onset_df.dropna(subset=['onset_time'], inplace=True)\n",
        "onset_df['icustay_id'] = onset_df['icustay_id'].astype('int64')\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## BINNING & AGREGATION FUNCTION (Fluid, UO, Vaso)\n",
        "# --------------------------------------------------------------------------------------\n",
        "def perform_binning_aggregation(data_df, value_col, agg_funcs, prefix):\n",
        "    \"\"\"Merge data with onset_time, perform binning, and aggregation.\"\"\"\n",
        "    if data_df.empty:\n",
        "        return pd.DataFrame(columns=['icustay_id', 'window_label', f'{prefix}_{list(agg_funcs.keys())[0]}'], dtype=np.float64)\n",
        "\n",
        "    # Merge with onset_time\n",
        "    df_merged = pd.merge(data_df, onset_df, on='icustay_id', how='left')\n",
        "    df_merged.dropna(subset=['charttime', 'onset_time'], inplace=True)\n",
        "\n",
        "    # Calculate duration\n",
        "    df_merged['duration_from_onset_hours'] = (\n",
        "        df_merged['charttime'] - df_merged['onset_time']\n",
        "    ).dt.total_seconds() / 3600\n",
        "\n",
        "    # Filter duration\n",
        "    df_binned = df_merged[\n",
        "        (df_merged['duration_from_onset_hours'] >= MIN_PRE_ONSET_HOURS) &\n",
        "        (df_merged['duration_from_onset_hours'] < MAX_POST_ONSET_HOURS)\n",
        "    ].copy()\n",
        "\n",
        "    if df_binned.empty:\n",
        "         return pd.DataFrame(columns=['icustay_id', 'window_label', f'{prefix}_{list(agg_funcs.keys())[0]}'], dtype=np.float64)\n",
        "\n",
        "    # Label time windows (window_label, can be negative)\n",
        "    df_binned['window_label'] = np.floor(\n",
        "        df_binned['duration_from_onset_hours'] / window_size_hours\n",
        "    ).astype('int64') # Use standard int64\n",
        "\n",
        "    # Perform aggregation\n",
        "    agg_result = df_binned.groupby(\n",
        "        ['icustay_id', 'window_label']\n",
        "    ).agg(\n",
        "        **{f'{prefix}_{func}': (value_col, func) for func in agg_funcs}\n",
        "    ).reset_index()\n",
        "\n",
        "    return agg_result\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## INTERVAL AGREGATION & FLUID/VASO MERGE\n",
        "# --------------------------------------------------------------------------------------\n",
        "bin_start_time = time.time()\n",
        "\n",
        "input_4h = perform_binning_aggregation(input_combined, 'input_volume', {'sum': 'sum'}, 'input_step')\n",
        "uo_4h = perform_binning_aggregation(UO, 'value', {'sum': 'sum'}, 'uo_step')\n",
        "vaso_4h = perform_binning_aggregation(vaso_combined, 'rate_std', {'max': 'max', 'median': 'median'}, 'vaso_step')\n",
        "\n",
        "# Merge Fluid/UO/Vaso Step Data\n",
        "fluid_vaso_agg_final = pd.merge(input_4h, uo_4h, on=['icustay_id', 'window_label'], how='outer')\n",
        "fluid_vaso_agg_final = pd.merge(fluid_vaso_agg_final, vaso_4h, on=['icustay_id', 'window_label'], how='outer')\n",
        "\n",
        "print(f\"Binning aggregation (Fluid/Vaso) complete in {time.time() - bin_start_time:.1f} seconds.\")\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## MERGING T0 (PRE-ADMISSION) AND PRE-ONSET\n",
        "# --------------------------------------------------------------------------------------\n",
        "cum_start_time = time.time()\n",
        "\n",
        "# Merge Pre-adm\n",
        "preadm_input = inputpreadm.groupby('icustay_id').agg(preadm_input=('inputpreadm', 'sum')).reset_index()\n",
        "preadm_uo = UOpreadm.groupby('icustay_id').agg(preadm_uo=('value', 'sum')).reset_index()\n",
        "\n",
        "fluid_vaso_agg_final = pd.merge(fluid_vaso_agg_final, preadm_input, on='icustay_id', how='left').fillna({'preadm_input': 0})\n",
        "fluid_vaso_agg_final = pd.merge(fluid_vaso_agg_final, preadm_uo, on='icustay_id', how='left').fillna({'preadm_uo': 0})\n",
        "\n",
        "# Impute NaN steps with 0 (representing zero input/output in that time window)\n",
        "fluid_vaso_agg_final['input_step_sum'] = fluid_vaso_agg_final['input_step_sum'].fillna(0)\n",
        "fluid_vaso_agg_final['uo_step_sum'] = fluid_vaso_agg_final['uo_step_sum'].fillna(0)\n",
        "vaso_step_cols = ['vaso_step_max', 'vaso_step_median']\n",
        "fluid_vaso_agg_final[vaso_step_cols] = fluid_vaso_agg_final[vaso_step_cols].fillna(0)\n",
        "\n",
        "\n",
        "# Calculate TOTAL T0 (Pre-Adm + Pre-Onset)\n",
        "def calculate_t0_only(group):\n",
        "    # Sum only for window_label < 0 (pre-onset)\n",
        "    input_t0_total = group['preadm_input'].iloc[0] + group.loc[group['window_label'] < 0, 'input_step_sum'].sum()\n",
        "    uo_t0_total = group['preadm_uo'].iloc[0] + group.loc[group['window_label'] < 0, 'uo_step_sum'].sum()\n",
        "    return pd.Series({'input_t0_total': input_t0_total, 'uo_t0_total': uo_t0_total})\n",
        "\n",
        "t0_totals_df = fluid_vaso_agg_final.groupby('icustay_id').apply(calculate_t0_only).reset_index()\n",
        "\n",
        "# Extract Step Data and Merge T0\n",
        "fluid_vaso_step_data = fluid_vaso_agg_final.copy()\n",
        "fluid_vaso_step_data.drop(columns=['preadm_input', 'preadm_uo'], inplace=True, errors='ignore')\n",
        "\n",
        "# Convert window_label to timestep_id (1 to 20)\n",
        "fluid_vaso_step_data.rename(columns={'window_label': 'timestep_id_raw'}, inplace=True)\n",
        "fluid_vaso_step_data['timestep_id'] = fluid_vaso_step_data['timestep_id_raw'] + TIMESTEP_OFFSET + 1\n",
        "fluid_vaso_step_data = fluid_vaso_step_data[\n",
        "    (fluid_vaso_step_data['timestep_id'] >= 1) &\n",
        "    (fluid_vaso_step_data['timestep_id'] <= TOTAL_STEPS)\n",
        "].copy()\n",
        "fluid_vaso_step_data['timestep_id'] = fluid_vaso_step_data['timestep_id'].astype('int64')\n",
        "\n",
        "# Merge total T0 data into the first timestep (T=1)\n",
        "t0_to_merge = t0_totals_df.copy()\n",
        "t0_to_merge['timestep_id'] = T0_BASELINE_STEP_ID\n",
        "\n",
        "fluid_vaso_agg_pre_t0 = pd.merge(\n",
        "    fluid_vaso_step_data,\n",
        "    t0_to_merge,\n",
        "    on=['icustay_id', 'timestep_id'],\n",
        "    how='left'\n",
        ")\n",
        "fluid_vaso_agg_pre_t0.drop(columns=['timestep_id_raw'], inplace=True, errors='ignore')\n",
        "\n",
        "print(f\"Pre-T0 calculation and step-data extraction complete in {time.time() - cum_start_time:.1f} seconds.\")\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## CLINICAL FEATURE AGGREGATION & FINAL MERGE\n",
        "# --------------------------------------------------------------------------------------\n",
        "final_merge_start = time.time()\n",
        "\n",
        "# 1. Create Dense Timeline\n",
        "def create_dense_timeline_binned(df_onset):\n",
        "    all_time_steps = []\n",
        "    max_steps = TOTAL_STEPS\n",
        "    for _, row in df_onset.iterrows():\n",
        "        icustay_id = row['icustay_id']\n",
        "        start_time = row['onset_time']\n",
        "        temp_df = pd.DataFrame({\n",
        "            'icustay_id': icustay_id,\n",
        "            'timestep_id': np.arange(1, max_steps + 1)\n",
        "        })\n",
        "        temp_df['t0_window_start'] = start_time + pd.to_timedelta(\n",
        "            MIN_PRE_ONSET_HOURS + (temp_df['timestep_id'] - 1) * window_size_hours,\n",
        "            unit='h'\n",
        "        )\n",
        "        temp_df['onset_time'] = start_time\n",
        "        all_time_steps.append(temp_df)\n",
        "    dense_timeline_df = pd.concat(all_time_steps, ignore_index=True)\n",
        "    dense_timeline_df['icustay_id'] = dense_timeline_df['icustay_id'].astype('int64')\n",
        "    dense_timeline_df['timestep_id'] = dense_timeline_df['timestep_id'].astype('int64')\n",
        "    return dense_timeline_df.sort_values(by=['icustay_id', 't0_window_start'])\n",
        "\n",
        "dense_timeline_df = create_dense_timeline_binned(onset_df)\n",
        "\n",
        "# 2. Prepare Clinical Features for Aggregation (reformat_imputed_final)\n",
        "reformat_imputed_final_merge = pd.merge(reformat_imputed_final.drop(columns=['onset_time'], errors='ignore'),\n",
        "                                   onset_df, on='icustay_id', how='left')\n",
        "\n",
        "# Convert time and filter duration\n",
        "reformat_imputed_final_merge['charttime'] = pd.to_datetime(reformat_imputed_final_merge['charttime'], errors='coerce')\n",
        "reformat_imputed_final_merge['onset_time'] = pd.to_datetime(reformat_imputed_final_merge['onset_time'], errors='coerce')\n",
        "reformat_imputed_final_merge.dropna(subset=['charttime', 'onset_time'], inplace=True)\n",
        "\n",
        "reformat_imputed_final_merge['duration_from_onset_hours'] = (\n",
        "    reformat_imputed_final_merge['charttime'] - reformat_imputed_final_merge['onset_time']\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# Filter duration\n",
        "reformat_imputed_final_merge = reformat_imputed_final_merge[\n",
        "    (reformat_imputed_final_merge['duration_from_onset_hours'] < MAX_POST_ONSET_HOURS) &\n",
        "    (reformat_imputed_final_merge['duration_from_onset_hours'] >= MIN_PRE_ONSET_HOURS)\n",
        "].copy()\n",
        "\n",
        "# Correct Clinical Timestep ID (ensure consistency in type and calculation)\n",
        "reformat_imputed_final_merge['timestep_id'] = (\n",
        "    np.floor(reformat_imputed_final_merge['duration_from_onset_hours'] / window_size_hours).astype(int)\n",
        "    + TIMESTEP_OFFSET\n",
        "    + 1\n",
        ")\n",
        "reformat_imputed_final_merge['timestep_id'] = reformat_imputed_final_merge['timestep_id'].astype('int64')\n",
        "\n",
        "# 3. Aggregate Laboratory/Vital Sign Features (Mean) per Timestep\n",
        "clinical_skip_cols = ['icustay_id', 'timestep_id', 'onset_time', 'charttime', 'duration_from_onset_hours']\n",
        "demog_target_cols = ['gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los']\n",
        "clinical_skip_cols.extend(demog_target_cols)\n",
        "clinical_mean_cols = [col for col in reformat_imputed_final_merge.columns if col not in clinical_skip_cols]\n",
        "\n",
        "# Convert clinical columns to numeric before mean aggregation\n",
        "for col in clinical_mean_cols:\n",
        "    if col in reformat_imputed_final_merge.columns:\n",
        "        reformat_imputed_final_merge[col] = pd.to_numeric(reformat_imputed_final_merge[col], errors='coerce')\n",
        "\n",
        "if not clinical_mean_cols:\n",
        "    print(\"!!! ERROR: clinical_mean_cols are empty. No clinical features are known.\")\n",
        "    clinical_features_agg = pd.DataFrame(columns=['icustay_id', 'timestep_id'])\n",
        "else:\n",
        "    # Perform Aggregation\n",
        "    clinical_features_agg = reformat_imputed_final_merge.groupby(['icustay_id', 'timestep_id'])[clinical_mean_cols].mean().reset_index()\n",
        "\n",
        "    # Filter only valid timesteps (1 to 20)\n",
        "    clinical_features_agg = clinical_features_agg[\n",
        "        (clinical_features_agg['timestep_id'] >= 1) &\n",
        "        (clinical_features_agg['timestep_id'] <= TOTAL_STEPS)\n",
        "    ].copy()\n",
        "\n",
        "    # Ensure int64 key type\n",
        "    clinical_features_agg['icustay_id'] = clinical_features_agg['icustay_id'].astype('int64')\n",
        "    clinical_features_agg['timestep_id'] = clinical_features_agg['timestep_id'].astype('int64')\n",
        "\n",
        "    print(f\"[DEBUG] Number of clinical/lab features to be aggregated (mean): {len(clinical_mean_cols)}\")\n",
        "    print(f\"[DEBUG] Number of rows in clinical_features_agg: {len(clinical_features_agg)}\")\n",
        "\n",
        "# 4. Final Merge Data\n",
        "final_merged_df = pd.merge(dense_timeline_df, clinical_features_agg,\n",
        "                           on=['icustay_id', 'timestep_id'], how='left')\n",
        "\n",
        "# Merge Fluid/Vaso data\n",
        "fluid_vaso_cols = [col for col in fluid_vaso_agg_pre_t0.columns if col not in ['icustay_id', 'timestep_id']]\n",
        "final_merged_df = pd.merge(final_merged_df,\n",
        "                           fluid_vaso_agg_pre_t0[['icustay_id', 'timestep_id'] + fluid_vaso_cols],\n",
        "                           on=['icustay_id', 'timestep_id'], how='left')\n",
        "\n",
        "present_clinical_cols = [col for col in clinical_mean_cols if col in final_merged_df.columns]\n",
        "print(f\"[DEBUG] Number of clinical features successfully merged into final_merged_df: {len(present_clinical_cols)}\")\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## FFILL/BFILL & CUMULATIVE BALANCE IMPUTATION\n",
        "# --------------------------------------------------------------------------------------\n",
        "print(\"\\nApplying LOCF/FFILL and BFILL to clinical/lab features...\")\n",
        "\n",
        "# 1. Apply FFILL/BFILL only on clinical/lab features\n",
        "for col in present_clinical_cols:\n",
        "    final_merged_df[col] = pd.to_numeric(final_merged_df[col], errors='coerce')\n",
        "    final_merged_df[col] = final_merged_df.groupby('icustay_id')[col].ffill()\n",
        "    final_merged_df[col] = final_merged_df.groupby('icustay_id')[col].bfill()\n",
        "\n",
        "#  Zero Imputation for Fluid/Vaso (step)\n",
        "final_merged_df['input_step_sum'] = final_merged_df['input_step_sum'].fillna(0)\n",
        "final_merged_df['uo_step_sum'] = final_merged_df['uo_step_sum'].fillna(0)\n",
        "final_merged_df['vaso_step_max'] = final_merged_df['vaso_step_max'].fillna(0)\n",
        "final_merged_df['vaso_step_median'] = final_merged_df['vaso_step_median'].fillna(0)\n",
        "\n",
        "# 2. Impute and Calculate Cumulative Balance Values\n",
        "final_merged_df['input_t0_total'] = final_merged_df.groupby('icustay_id')['input_t0_total'].ffill().fillna(0)\n",
        "final_merged_df['uo_t0_total'] = final_merged_df.groupby('icustay_id')['uo_t0_total'].ffill().fillna(0)\n",
        "\n",
        "final_merged_df['input_total'] = final_merged_df['input_t0_total'] + final_merged_df.groupby('icustay_id')['input_step_sum'].cumsum()\n",
        "final_merged_df['uo_total'] = final_merged_df['uo_t0_total'] + final_merged_df.groupby('icustay_id')['uo_step_sum'].cumsum()\n",
        "final_merged_df['cumulated_balance'] = final_merged_df['input_total'] - final_merged_df['uo_total']\n",
        "\n",
        "# Drop temporary t0/step columns\n",
        "final_merged_df.drop(columns=['input_step_sum', 'uo_step_sum', 'input_t0_total', 'uo_t0_total'], inplace=True, errors='ignore')\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## FILTER LOS AND MERGE DEMOGRAPHICS\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Merge Demografi & LOS\n",
        "demog_los = demog[['icustay_id', 'outtime', 'dod', 'expire_flag']].copy()\n",
        "demog_los['icustay_id'] = demog_los['icustay_id'].astype(np.int64)\n",
        "\n",
        "# Create actual_out_time\n",
        "demog_los['actual_out_time'] = demog_los.apply(\n",
        "    lambda row: row['dod'] if row['expire_flag'] == 1 and pd.notna(row['dod']) else row['outtime'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "final_merged_df = pd.merge(final_merged_df, demog_los[['icustay_id', 'actual_out_time']], on='icustay_id', how='left')\n",
        "\n",
        "# ====================================================================================\n",
        "# DEBUG TIME: Use the correct column names\n",
        "# ====================================================================================\n",
        "# Setelah merge LOS dan sebelum filter\n",
        "print(\"\\n[TIME DEBUGGING]\")\n",
        "\n",
        "# Check if actual_out_time is earlier than onset_time (T=0)\n",
        "diff_to_onset = (final_merged_df['actual_out_time'] - final_merged_df['onset_time']).dt.total_seconds() / 3600\n",
        "print(f\"Minimal difference (LOS - Onset): {diff_to_onset.min():.2f} hours\")\n",
        "print(f\"Maksimal difference (LOS - Onset): {diff_to_onset.max():.2f} hours\")\n",
        "print(f\"Median difference (LOS - Onset): {diff_to_onset.median():.2f} hours\")\n",
        "\n",
        "# Check which column causes rows to be dropped\n",
        "rows_to_be_removed = (final_merged_df['t0_window_start'] >= final_merged_df['actual_out_time']) & \\\n",
        "                     (final_merged_df['actual_out_time'].notna())\n",
        "print(f\"Total rows to be REMOVED (based on filter logic): {rows_to_be_removed.sum()}\")\n",
        "\n",
        "if rows_to_be_removed.sum() == len(final_merged_df):\n",
        "    print(\"!!! CONFIRMATION: ALL ROWS MEET THE DELETION CRITERIA (LOS FILTER).\")\n",
        "    # Cek nilai t0_window_start pertama\n",
        "    first_timestep_rows = final_merged_df[final_merged_df['timestep_id'] == 1].copy()\n",
        "    print(\"\\nSample Data (Timestep 1):\")\n",
        "    print(first_timestep_rows[['icustay_id', 't0_window_start', 'actual_out_time']].head(5).to_markdown(index=False))\n",
        "\n",
        "# ====================================================================================\n",
        "\n",
        "# LOS Filter: Remove timesteps occurring AFTER discharge time\n",
        "initial_rows = len(final_merged_df)\n",
        "filter_out_of_los = (final_merged_df['t0_window_start'] >= final_merged_df['actual_out_time']) & \\\n",
        "                    (final_merged_df['actual_out_time'].notna())\n",
        "\n",
        "final_merged_df = final_merged_df[~filter_out_of_los].copy()\n",
        "rows_removed = initial_rows - len(final_merged_df)\n",
        "print(f\"LOS Filter Applied: Removed {rows_removed} timesteps.\")\n",
        "\n",
        "final_merged_df.drop(columns=['actual_out_time'], inplace=True, errors='ignore')\n",
        "\n",
        "# Merge non-temporal demographic features\n",
        "demog['icustay_id'] = demog['icustay_id'].astype(np.int64)\n",
        "\n",
        "# Calculate is_readmit\n",
        "demog['is_readmit'] = demog['adm_order'] > 1\n",
        "\n",
        "time_diff = (demog['dod'] - demog['outtime']).dt.total_seconds()\n",
        "demog['recent_death'] = (time_diff > 0) & (time_diff < (48*3600)) & (demog['expire_flag'] == 1)\n",
        "demog['recent_death'] = demog['recent_death'].fillna(False)\n",
        "\n",
        "demog_cols = ['icustay_id', 'gender', 'age', 'elixhauser', 'is_readmit', 'morta_hosp', 'recent_death', 'morta_90', 'los']\n",
        "demog_constant = demog[demog_cols].drop_duplicates(subset=['icustay_id'])\n",
        "demog_constant['gender'] = demog_constant['gender'].replace({1: 'M', 2: 'F'})\n",
        "\n",
        "# Drop demographic columns already present in final_merged_df before merging\n",
        "demog_cols_to_drop = [col for col in demog_cols if col != 'icustay_id']\n",
        "final_merged_df = final_merged_df.drop(columns=demog_cols_to_drop, errors='ignore')\n",
        "\n",
        "# Merge akhir\n",
        "final_merged_df = pd.merge(final_merged_df, demog_constant, on='icustay_id', how='left')\n",
        "\n",
        "# Final Cleanup and Verification\n",
        "final_merged_df.sort_values(by=['icustay_id', 't0_window_start'], inplace=True)\n",
        "final_merged_df.rename(columns={'onset_time': 'onset_time_T0'}, inplace=True)\n",
        "\n",
        "print(f\"\\nFinal merge and data processing complete in {time.time() - final_merge_start:.1f} seconds.\")\n",
        "print(f\"Final DataFrame has {len(final_merged_df.icustay_id.unique())} patients and {len(final_merged_df)} timesteps.\")\n",
        "print(\"\\nVerifikasi Kolom Penting (t0, onset, kumulatif, Klinis):\")\n",
        "print(final_merged_df[['icustay_id', 'timestep_id', 't0_window_start', 'onset_time_T0', 'input_total', 'cumulated_balance'] + ['HR', 'MeanBP', 'Creatinine']].head(10).to_markdown(index=False))\n",
        "\n",
        "del dense_timeline_df, clinical_features_agg, fluid_vaso_agg_pre_t0\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT_TQpOgq_qi",
        "outputId": "b90d8c71-8915-4897-dfee-4a939dbdc2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data combination and onset preparation...\n",
            "Binning aggregation (Fluid/Vaso) complete in 1.2 seconds.\n",
            "Pre-T0 calculation and step-data extraction complete in 15.0 seconds.\n",
            "[DEBUG] Number of clinical/lab features to be aggregated (mean): 64\n",
            "[DEBUG] Number of rows in clinical_features_agg: 119717\n",
            "[DEBUG] Number of clinical features successfully merged into final_merged_df: 64\n",
            "\n",
            "Applying LOCF/FFILL and BFILL to clinical/lab features...\n",
            "\n",
            "[TIME DEBUGGING]\n",
            "Minimal difference (LOS - Onset): -1507.36 hours\n",
            "Maksimal difference (LOS - Onset): 89524.75 hours\n",
            "Median difference (LOS - Onset): 145.87 hours\n",
            "Total rows to be REMOVED (based on filter logic): 77459\n",
            "LOS Filter Applied: Removed 77459 timesteps.\n",
            "\n",
            "Final merge and data processing complete in 35.4 seconds.\n",
            "Final DataFrame has 24948 patients and 448521 timesteps.\n",
            "\n",
            "Verifikasi Kolom Penting (t0, onset, kumulatif, Klinis):\n",
            "|   icustay_id |   timestep_id | t0_window_start     | onset_time_T0       |   input_total |   cumulated_balance |   HR |   MeanBP |   Creatinine |\n",
            "|-------------:|--------------:|:--------------------|:--------------------|--------------:|--------------------:|-----:|---------:|-------------:|\n",
            "|       200001 |             1 | 2181-11-17 00:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             2 | 2181-11-17 04:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             3 | 2181-11-17 08:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             4 | 2181-11-17 12:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             5 | 2181-11-17 16:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             6 | 2181-11-17 20:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             7 | 2181-11-18 00:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             8 | 2181-11-18 04:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             9 | 2181-11-18 08:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |            10 | 2181-11-18 12:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatur opsi tampilan agar bisa melihat semua kolom (fitur klinis/lab)\n",
        "pd.set_option('display.max_rows', 100) # Batas baris untuk laporan missingness\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "print(\"=== STATISTIK DESKRIPTIF DATA AKHIR (final_merged_df) ===\")\n",
        "print(f\"Total Baris (Timestep): {len(final_merged_df)}\")\n",
        "print(f\"Total Kolom: {len(final_merged_df.columns)}\")\n",
        "print(f\"Total Pasien Unik: {final_merged_df['icustay_id'].nunique()}\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 1. Statistik Fitur Numerik (Klinis, Lab, Kumulatif, Demografi Numerik)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Pengecualian: icustay_id dan timestep_id tidak perlu statistik deskriptif\n",
        "# Pilih kolom numerik, buang kolom waktu (datetime)\n",
        "numeric_cols = final_merged_df.select_dtypes(include=np.number).columns.tolist()\n",
        "cols_to_exclude = ['icustay_id', 'timestep_id']\n",
        "final_numeric_cols = [col for col in numeric_cols if col not in cols_to_exclude]\n",
        "\n",
        "print(\"\\n--- STATISTIK FITUR NUMERIK (Mean, Std, Min, Max, Quantile) ---\")\n",
        "# Gunakan transpose agar mudah dibaca di konsol\n",
        "numeric_stats = final_merged_df[final_numeric_cols].describe().T\n",
        "print(numeric_stats)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 2. Missingness Report (Verifikasi Akhir)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"--- VERIFIKASI MISSINGNESS AKHIR ---\")\n",
        "missing_count = final_merged_df.isnull().sum()\n",
        "missing_percent = (missing_count / len(final_merged_df)) * 100\n",
        "missing_report = pd.DataFrame({\n",
        "    'Missing Count': missing_count,\n",
        "    'Missing Percent': missing_percent.round(2)\n",
        "})\n",
        "\n",
        "# Filter hanya kolom yang masih memiliki missing values (harusnya hanya fitur non-temporal/target jika Median Imputation belum dilakukan)\n",
        "missing_report = missing_report[missing_report['Missing Count'] > 0].sort_values(\n",
        "    by='Missing Percent', ascending=False\n",
        ")\n",
        "print(missing_report)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 3. Statistik Fitur Kategorikal/Boolean (Gender, Target Outcome, dll.)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "print(\"--- STATISTIK FITUR KATEGORIKAL/BOOLEAN ---\")\n",
        "# Pilih kolom bertipe object atau boolean\n",
        "categorical_cols = final_merged_df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if col in final_merged_df.columns:\n",
        "        print(f\"\\nKolom: {col}\")\n",
        "        # Gunakan value_counts dengan normalize=True untuk melihat proporsi\n",
        "        value_counts = final_merged_df[col].value_counts(dropna=False)\n",
        "        value_proportions = final_merged_df[col].value_counts(normalize=True, dropna=False).mul(100).round(2)\n",
        "\n",
        "        # Gabungkan count dan proportion ke dalam satu DataFrame\n",
        "        stats_df = pd.DataFrame({\n",
        "            'Count': value_counts,\n",
        "            'Proportion (%)': value_proportions\n",
        "        })\n",
        "        print(stats_df)\n",
        "\n",
        "# Reset opsi tampilan\n",
        "pd.reset_option('display.max_rows')\n",
        "pd.reset_option('display.max_columns')\n",
        "pd.reset_option('display.width')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHZyRt4VvYOp",
        "outputId": "ced8099d-451c-49f6-cc61-dd5847a815ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STATISTIK DESKRIPTIF DATA AKHIR (final_merged_df) ===\n",
            "Total Baris (Timestep): 448521\n",
            "Total Kolom: 81\n",
            "Total Pasien Unik: 24948\n",
            "\n",
            "--- STATISTIK FITUR NUMERIK (Mean, Std, Min, Max, Quantile) ---\n",
            "                         count         mean           std            min         25%         50%          75%           max\n",
            "ACT                     2002.0   173.167465     40.227815      93.000000  145.000000  165.000000   196.000000  3.040000e+02\n",
            "Albumin                85965.0     2.768985      0.592321       1.100000    2.400000    2.800000     3.200000  4.300000e+00\n",
            "Arterial_BE            84059.0     2.655574      3.193618       0.000000    0.000000    1.181818     4.000000  1.300000e+01\n",
            "Arterial_pH            67370.0     7.392790      0.073245       7.170000    7.350000    7.400000     7.446000  7.600000e+00\n",
            "BUN                   182165.0    33.180343     21.350832       0.000000   17.000000   27.000000    44.000000  8.800000e+01\n",
            "CRP                     1639.0   111.447712     72.566202       4.300000   51.400000  111.800000   166.850000  2.990000e+02\n",
            "CVP                    63749.0    11.046152      5.295668       0.000000    7.000000   10.666667    14.428571  2.500000e+01\n",
            "Calcium               176900.0     8.247294      0.725875       6.300000    7.800000    8.200000     8.700000  1.020000e+01\n",
            "Cardiac_Index          11531.0     3.046298      0.934139       0.850000    2.352941    2.912660     3.592946  5.502650e+00\n",
            "Chloride              182208.0   104.707135      6.484049      85.000000  100.250000  105.000000   109.000000  1.250000e+02\n",
            "Creatinine            171997.0     1.307828      0.816772       0.100000    0.700000    1.020000     1.600000  3.800000e+00\n",
            "DiaBP                 180335.0    55.957221     13.417951       0.000000   47.000000   55.200000    64.500000  1.237500e+02\n",
            "Direct_Bili            99842.0     1.683371      3.818213       0.100000    0.102160    0.379520     1.211600  3.920992e+01\n",
            "Ectopy_Frequency         697.0     0.000000      0.000000       0.000000    0.000000    0.000000     0.000000  0.000000e+00\n",
            "FiO2                  142092.0     0.422632      0.105258       0.240000    0.350000    0.400000     0.500000  7.500000e-01\n",
            "Flow_Rate                350.0    29.262857     30.408009       0.000000    0.000000    1.000000    60.000000  7.000000e+01\n",
            "GCS                   148247.0    12.464959      3.558892       3.000000   10.714286   14.600000    15.000000  1.500000e+01\n",
            "Glucose               182295.0   136.936061     41.742475      34.000000  106.666667  129.333333   160.666667  2.570000e+02\n",
            "HCO3                  180730.0    24.622947      5.235471      10.750000   21.000000   24.200000    28.000000  3.800000e+01\n",
            "HCT                   181133.0    30.280175      4.466911      17.500000   27.133334   29.977779    33.200000  4.150000e+01\n",
            "HR                    183601.0    88.216241     17.545254      36.000000   75.333333   87.000000    99.800000  1.400000e+02\n",
            "Hb                    181133.0    10.060599      1.538169       5.689727    9.000000    9.942857    11.042857  1.407547e+01\n",
            "Height_cm              30171.0   117.364645      7.355123      94.000000  111.500000  117.000000   124.000000  1.415000e+02\n",
            "INR                   155653.0     1.461289      0.441212       0.800000    1.200000    1.300000     1.600000  2.900000e+00\n",
            "ITEMID_194              1954.0     4.727862      2.487762       0.000000    5.000000    6.000000     6.000000  8.000000e+00\n",
            "ITEMID_220339          38572.0     6.041743      2.710273       0.000000    5.000000    5.000000     7.000000  1.700000e+01\n",
            "ITEMID_224700          23353.0     7.433435      3.232219      -3.200000    5.000000    6.000000    10.000000  1.870000e+01\n",
            "ITEMID_505               403.0     5.345986      0.467588       5.000000    5.000000    5.000000     6.000000  6.000000e+00\n",
            "ITEMID_506             50696.0     6.036107      2.010956       1.000000    5.000000    5.000000     5.000000  1.250000e+01\n",
            "Ionized Calcium        76985.0     1.122664      0.084076       0.890000    1.070000    1.124444     1.180000  1.350000e+00\n",
            "Lactate                99937.0     1.883271      1.177370       0.400000    1.100000    1.500000     2.200000  7.000000e+00\n",
            "Magnesium             180346.0     2.024697      0.335003       1.100000    1.800000    2.000000     2.200000  3.000000e+00\n",
            "MeanBP                183120.0    76.817209     13.194387      34.000000   67.285714   75.428571    85.250000  1.200000e+02\n",
            "Mean_Airway_Pressure   88586.0     9.934822      3.874494       0.000000    7.000000    9.000000    12.000000  2.300000e+01\n",
            "Minute_Volume          88683.0     9.637174      2.868100       2.400000    7.500000    9.271429    11.600000  1.680000e+01\n",
            "Mixed_Venous_O2        15641.0    69.505722     10.240113      41.000000   63.000000   70.000000    77.000000  9.200000e+01\n",
            "O2_Delivery_Device     69244.0     2.375383      0.716299       1.000000    2.000000    2.000000     2.500000  4.000000e+00\n",
            "O2_Flow_Add            10240.0     4.084225      1.919708       0.000000    3.000000    4.000000     5.000000  1.000000e+01\n",
            "O2_Flow_Main          130477.0     5.737157      4.587067       0.000000    2.000000    4.000000    10.000000  2.200000e+01\n",
            "PAP                    10350.0    31.717418      8.864955       9.866667   25.333333   30.666667    37.000000  5.533333e+01\n",
            "PT                     88500.0    15.069767      2.618168      10.000000   13.300000   14.300000    16.100000  2.370000e+01\n",
            "PTT                   158757.0    36.352938     12.771311      13.700000   27.900000   32.366667    40.000000  8.570000e+01\n",
            "Peak_Ins_Pressure      82135.0    22.817478      8.334209       0.000000   17.000000   23.000000    28.000000  4.850000e+01\n",
            "Plateau_Pressure       69152.0    20.377497      5.305333       8.000000   16.500000   20.000000    24.000000  3.550000e+01\n",
            "Platelets_count       178231.0   211.446390    113.160726      10.000000  127.000000  198.000000   280.000000  5.190000e+02\n",
            "Potassium             182365.0     4.086687      0.554356       2.600000    3.700000    4.000000     4.400000  5.600000e+00\n",
            "RASS                   45672.0    -0.887368      1.698624      -5.000000   -1.500000    0.000000     0.000000  3.000000e+00\n",
            "RBC                      459.0     3.822690      0.498949       2.740000    3.440000    3.820000     4.060000  4.880000e+00\n",
            "RR                    183484.0    20.630907      5.191533       5.000000   17.000000   20.000000    24.000000  3.850000e+01\n",
            "SGOT(AST)              48830.0    47.361099     34.552760       6.000000   22.000000   35.000000    62.000000  1.570000e+02\n",
            "SGPT(ALT)              47254.0    33.687198     24.829840       2.000000   15.000000   26.000000    45.000000  1.120000e+02\n",
            "SVR                    10220.0   914.233510    336.726984     182.613137  661.277633  886.747009  1133.451969  1.850203e+03\n",
            "Sodium                181416.0   138.844167      4.750110     127.000000  136.000000  139.000000   142.000000  1.510000e+02\n",
            "SpO2                  183527.0    97.045812      2.331052      89.000000   95.545455   97.375000    99.000000  1.000000e+02\n",
            "SysBP                 180381.0   118.579630     20.829736      10.999905  103.200000  116.000000   132.000000  2.230000e+02\n",
            "Temp_C                  5630.0    36.646262      1.035285      34.600000   35.900002   36.700000    37.400000  3.890000e+01\n",
            "Tidal_Volume           76120.0   469.556723    111.533038     153.000000  400.000000  470.000000   549.000000  7.270000e+02\n",
            "Total_Protein           3783.0     5.356498      0.854844       3.200000    4.800000    5.400000     5.900000  7.400000e+00\n",
            "Total_bili             99842.0     2.671965      5.507424       0.200000    0.400000    0.800000     2.100000  5.680000e+01\n",
            "Troponin               36941.0     0.128733      0.192691       0.010000    0.010000    0.050000     0.150000  9.300000e-01\n",
            "WBC_Count             179194.0    11.600882      5.847976       0.000000    7.400000   10.700000    15.100000  2.810000e+01\n",
            "Weight_kg             141979.0    77.280168     19.395337      25.600000   63.300000   75.200000    89.200000  1.345000e+02\n",
            "paCO2                  65617.0    40.845603      9.027227      17.000000   34.500000   40.000000    46.000000  6.500000e+01\n",
            "paO2                   65875.0   110.420836     39.575243      32.000000   81.000000  102.000000   134.000000  2.280000e+02\n",
            "vaso_step_max         448521.0     0.031617      0.503106       0.000000    0.000000    0.000000     0.000000  1.753250e+02\n",
            "vaso_step_median      448521.0     0.021129      0.151533       0.000000    0.000000    0.000000     0.000000  4.648800e+01\n",
            "input_total           448521.0  2548.065318  14144.871404  -16479.055000    0.000000  220.000000  3199.200000  2.004300e+06\n",
            "uo_total              448521.0  1821.450447   3303.430944       0.000000    0.000000  300.000000  2545.000000  1.870000e+05\n",
            "cumulated_balance     448521.0   726.614872  13998.222252 -176193.333000 -138.500000    0.000000   770.000000  1.997250e+06\n",
            "age                   448521.0    57.785067     25.561246       0.000000   48.000000   64.000000    77.000000  9.000000e+01\n",
            "elixhauser            448186.0     3.585094      2.381752       0.000000    2.000000    4.000000     5.000000  1.400000e+01\n",
            "morta_hosp            221824.0     0.264642      0.441144       0.000000    0.000000    0.000000     1.000000  1.000000e+00\n",
            "morta_90              221824.0     0.470391      0.499124       0.000000    0.000000    0.000000     1.000000  1.000000e+00\n",
            "los                   448521.0     7.966648     13.309777       0.001500    1.805200    3.379500     8.314600  1.730725e+02\n",
            "\n",
            "======================================================================\n",
            "\n",
            "--- VERIFIKASI MISSINGNESS AKHIR ---\n",
            "                      Missing Count  Missing Percent\n",
            "Flow_Rate                    448171            99.92\n",
            "ITEMID_505                   448118            99.91\n",
            "RBC                          448062            99.90\n",
            "Ectopy_Frequency             447824            99.84\n",
            "CRP                          446882            99.63\n",
            "ITEMID_194                   446567            99.56\n",
            "ACT                          446519            99.55\n",
            "Total_Protein                444738            99.16\n",
            "Temp_C                       442891            98.74\n",
            "O2_Flow_Add                  438281            97.72\n",
            "SVR                          438301            97.72\n",
            "PAP                          438171            97.69\n",
            "Cardiac_Index                436990            97.43\n",
            "Mixed_Venous_O2              432880            96.51\n",
            "ITEMID_224700                425168            94.79\n",
            "Height_cm                    418350            93.27\n",
            "Troponin                     411580            91.76\n",
            "ITEMID_220339                409949            91.40\n",
            "RASS                         402849            89.82\n",
            "SGPT(ALT)                    401267            89.46\n",
            "SGOT(AST)                    399691            89.11\n",
            "ITEMID_506                   397825            88.70\n",
            "CVP                          384772            85.79\n",
            "paCO2                        382904            85.37\n",
            "paO2                         382646            85.31\n",
            "Arterial_pH                  381151            84.98\n",
            "Plateau_Pressure             379369            84.58\n",
            "O2_Delivery_Device           379277            84.56\n",
            "Tidal_Volume                 372401            83.03\n",
            "Ionized Calcium              371536            82.84\n",
            "Peak_Ins_Pressure            366386            81.69\n",
            "Arterial_BE                  364462            81.26\n",
            "Albumin                      362556            80.83\n",
            "PT                           360021            80.27\n",
            "Mean_Airway_Pressure         359935            80.25\n",
            "Minute_Volume                359838            80.23\n",
            "Direct_Bili                  348679            77.74\n",
            "Total_bili                   348679            77.74\n",
            "Lactate                      348584            77.72\n",
            "O2_Flow_Main                 318044            70.91\n",
            "Weight_kg                    306542            68.35\n",
            "FiO2                         306429            68.32\n",
            "GCS                          300274            66.95\n",
            "INR                          292868            65.30\n",
            "PTT                          289764            64.60\n",
            "Creatinine                   276524            61.65\n",
            "Calcium                      271621            60.56\n",
            "Platelets_count              270290            60.26\n",
            "WBC_Count                    269327            60.05\n",
            "Magnesium                    268175            59.79\n",
            "DiaBP                        268186            59.79\n",
            "SysBP                        268140            59.78\n",
            "HCO3                         267791            59.71\n",
            "HCT                          267388            59.62\n",
            "Hb                           267388            59.62\n",
            "Sodium                       267105            59.55\n",
            "BUN                          266356            59.39\n",
            "Chloride                     266313            59.38\n",
            "Glucose                      266226            59.36\n",
            "Potassium                    266156            59.34\n",
            "MeanBP                       265401            59.17\n",
            "RR                           265037            59.09\n",
            "SpO2                         264994            59.08\n",
            "HR                           264920            59.07\n",
            "morta_hosp                   226697            50.54\n",
            "morta_90                     226697            50.54\n",
            "elixhauser                      335             0.07\n",
            "\n",
            "======================================================================\n",
            "\n",
            "--- STATISTIK FITUR KATEGORIKAL/BOOLEAN ---\n",
            "\n",
            "Kolom: gender\n",
            "         Count  Proportion (%)\n",
            "gender                        \n",
            "M       249296           55.58\n",
            "F       199225           44.42\n",
            "\n",
            "Kolom: is_readmit\n",
            "             Count  Proportion (%)\n",
            "is_readmit                        \n",
            "False       314549           70.13\n",
            "True        133972           29.87\n",
            "\n",
            "Kolom: recent_death\n",
            "               Count  Proportion (%)\n",
            "recent_death                        \n",
            "False         443483           98.88\n",
            "True            5038            1.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURE DISPLAY SETTINGS FOR IMPORTANT COLUMNS/ROWS ---\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"=== STARTING KOMOROWSKI IMPUTATION & FEATURE ENGINEERING ===\")\n",
        "\n",
        "# --- 0. DUPLICATE DATAFRAME ---\n",
        "# Create a copy to store results as requested\n",
        "final_merged_df2 = final_merged_df.copy()\n",
        "print(\"DataFrame duplicated to 'final_merged_df2'.\")\n",
        "\n",
        "# --- Prepare Clinical/Lab Columns (Once Only) ---\n",
        "cols_to_exclude_from_impute = [\n",
        "    'icustay_id', 'timestep_id', 't0_window_start', 'onset_time_T0', 'gender', 'age', 'elixhauser',\n",
        "    'is_readmit', 'morta_hosp', 'recent_death', 'morta_90', 'los', 'input_total', 'uo_total',\n",
        "    'cumulated_balance', 'vaso_step_max', 'vaso_step_median',\n",
        "]\n",
        "\n",
        "# Get all numeric columns with NaN (excluding excluded columns)\n",
        "numeric_nan_cols = final_merged_df2.select_dtypes(include=np.number).columns[\n",
        "    final_merged_df2.select_dtypes(include=np.number).isnull().any()\n",
        "].tolist()\n",
        "clinical_impute_cols = [col for col in numeric_nan_cols if col not in cols_to_exclude_from_impute]\n",
        "\n",
        "if not clinical_impute_cols:\n",
        "    print(\"WARNING: All core clinical columns are already fully imputed or not found. Skipping Interpolation/kNN.\")\n",
        "\n",
        "else:\n",
        "    # ----------------------------------------------------------------------\n",
        "    ## 1. LINEAR INTERPOLATION (for missingness < 5%)\n",
        "    # ----------------------------------------------------------------------\n",
        "    print(\"\\n[STEP 1/5] Performing Linear Interpolation (< 5% Missingness)...\")\n",
        "\n",
        "    initial_missingness = final_merged_df2[clinical_impute_cols].isnull().sum() / len(final_merged_df2)\n",
        "    linear_interp_cols = initial_missingness[initial_missingness < 0.05].index.tolist()\n",
        "\n",
        "    for col in tqdm(linear_interp_cols, desc=\"Linear Interpolation\"):\n",
        "        # Interpolate per icustay_id group\n",
        "        final_merged_df2[col] = final_merged_df2.groupby('icustay_id')[col].apply(\n",
        "            lambda x: x.interpolate(method='linear', limit_direction='both').fillna(x.median())\n",
        "        )\n",
        "\n",
        "    print(f\"✅ Linear Interpolation applied to {len(linear_interp_cols)} columns.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 2. k-NN IMPUTATION (WITH LOCAL MEDIAN FALLBACK PER CHUNK)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Recalculate columns that still have NaN after Linear Interpolation\n",
        "knn_impute_cols = final_merged_df2[clinical_impute_cols].columns[\n",
        "    final_merged_df2[clinical_impute_cols].isnull().any()\n",
        "].tolist()\n",
        "\n",
        "if not knn_impute_cols:\n",
        "    print(\"\\nWARNING: All clinical columns are already clean. Proceeding to Step 3.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n[STEP 2/5] Performing k-NN Imputation (1-NN, Euclidean, Chunking) - LOCAL MEDIAN PATCH...\")\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=1, weights='uniform', metric='nan_euclidean')\n",
        "\n",
        "    # 1. Extract data to be imputed (Expected ~64 Columns)\n",
        "    ref_data = final_merged_df2[knn_impute_cols].copy()\n",
        "\n",
        "    current_knn_cols = ref_data.columns.tolist()\n",
        "    total_rows = len(ref_data)\n",
        "    total_cols = len(current_knn_cols)  # Expected 64\n",
        "    imputed_array = np.empty((total_rows, total_cols), dtype=np.float64)\n",
        "\n",
        "    CHUNK_SIZE = 10000\n",
        "\n",
        "    print(f\"Total rows to impute: {total_rows}. Total columns to process: {total_cols}. Chunk size: {CHUNK_SIZE}\")\n",
        "\n",
        "    # Compute global medians for fallback (efficient)\n",
        "    global_medians = ref_data.median()\n",
        "\n",
        "    # Iterate with 10K Chunking\n",
        "    for i in tqdm(range(0, total_rows, CHUNK_SIZE), desc=\"k-NN Chunk Imputation\"):\n",
        "        start_idx = i\n",
        "        end_idx = min(i + CHUNK_SIZE, total_rows)\n",
        "\n",
        "        # Komorowski logic for the last chunk\n",
        "        if end_idx == total_rows and total_rows > CHUNK_SIZE:\n",
        "            start_idx = total_rows - CHUNK_SIZE\n",
        "\n",
        "        chunk = ref_data.iloc[start_idx:end_idx].copy()  # Work on a copy of the chunk\n",
        "\n",
        "        imputed_chunk = imputer.fit_transform(chunk)\n",
        "\n",
        "        # Check for dimension mismatch\n",
        "        if imputed_chunk.shape[1] != total_cols:\n",
        "            missing_cols = chunk.columns[chunk.isnull().all()].tolist()\n",
        "\n",
        "            print(f\"\\n⚠️ DIMENSION MISMATCH FOUND in chunk {i//CHUNK_SIZE} ({len(missing_cols)} missing columns).\")\n",
        "            # 1. Apply median fallback to missing columns in chunk\n",
        "            for col in missing_cols:\n",
        "                if col in global_medians:\n",
        "                    chunk[col] = chunk[col].fillna(global_medians[col])\n",
        "\n",
        "            # 2. Re-run k-NN on chunk after median fallback\n",
        "            imputed_chunk = imputer.fit_transform(chunk)\n",
        "\n",
        "            if imputed_chunk.shape[1] != total_cols:\n",
        "                raise ValueError(\n",
        "                    f\"FATAL ERROR: Dimension mismatch remains after median fallback. Expected {total_cols} but got {imputed_chunk.shape[1]}\"\n",
        "                )\n",
        "\n",
        "            print(f\"✅ Chunk {i//CHUNK_SIZE} fixed with median fallback. Continuing...\")\n",
        "\n",
        "        array_start_idx = i\n",
        "        array_end_idx = end_idx\n",
        "\n",
        "        if end_idx == total_rows and total_rows > CHUNK_SIZE:\n",
        "            array_start_idx = total_rows - len(imputed_chunk)\n",
        "            array_end_idx = total_rows\n",
        "\n",
        "        imputed_array[array_start_idx:array_end_idx, :] = imputed_chunk\n",
        "\n",
        "    # Replace original columns with imputed results\n",
        "    final_merged_df2[current_knn_cols] = pd.DataFrame(imputed_array, columns=current_knn_cols, index=final_merged_df2.index)\n",
        "\n",
        "    missing_after_knn = final_merged_df2[current_knn_cols].isnull().sum().sum()\n",
        "    if missing_after_knn == 0:\n",
        "        print(\"✅ k-NN Imputation completed successfully. 0 NaN remaining in clinical columns.\")\n",
        "    else:\n",
        "        print(f\"🛑 WARNING: {missing_after_knn} NaN remaining after k-NN. Performing final median imputation.\")\n",
        "        final_merged_df2[current_knn_cols] = final_merged_df2[current_knn_cols].fillna(final_merged_df2[current_knn_cols].median())\n",
        "\n",
        "    del ref_data, imputer, imputed_array\n",
        "    gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 3. STATIC DATA & VASOPRESSOR CLEANING\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"\\n[STEP 3/5] Cleaning Static Data...\")\n",
        "\n",
        "# CORRECT GENDER (gender=gender-1) -> Convert M/F to 0/1\n",
        "if 'gender' in final_merged_df2.columns and final_merged_df2['gender'].dtype == 'object':\n",
        "    final_merged_df2['gender_numeric'] = final_merged_df2['gender'].map({'M': 0, 'F': 1})\n",
        "    final_merged_df2.drop(columns=['gender'], inplace=True, errors='ignore')\n",
        "elif 'gender' in final_merged_df2.columns:\n",
        "    final_merged_df2.rename(columns={'gender': 'gender_numeric'}, inplace=True, errors='ignore')\n",
        "\n",
        "# CAP AGE > 150*365.25\n",
        "AGE_CAP = 91.4\n",
        "final_merged_df2['age'] = np.where(final_merged_df2['age'] > AGE_CAP, AGE_CAP, final_merged_df2['age'])\n",
        "\n",
        "# FIX missing Elixhauser values (Median Imputation)\n",
        "elixhauser_median = final_merged_df2['elixhauser'].median()\n",
        "final_merged_df2['elixhauser'] = final_merged_df2['elixhauser'].fillna(elixhauser_median)\n",
        "\n",
        "# FIX Vasopressors (fill NaN with 0.0)\n",
        "final_merged_df2['vaso_step_median'] = final_merged_df2['vaso_step_median'].fillna(0.0)\n",
        "final_merged_df2['vaso_step_max'] = final_merged_df2['vaso_step_max'].fillna(0.0)\n",
        "\n",
        "print(\"✅ Static data cleaned and capped.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 4. DERIVED VARIABLE CALCULATION (P/F, Shock Index)\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"\\n[STEP 4/5] Calculating Derived Features...\")\n",
        "\n",
        "# 4.1. PaO2/FiO2 Ratio (P/F)\n",
        "final_merged_df2['PaO2_FiO2'] = final_merged_df2['paO2'] / final_merged_df2['FiO2']\n",
        "final_merged_df2['PaO2_FiO2'] = final_merged_df2['PaO2_FiO2'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# 4.2. Shock Index\n",
        "final_merged_df2['Shock_Index'] = final_merged_df2['HR'] / final_merged_df2['SysBP']\n",
        "final_merged_df2['Shock_Index'] = final_merged_df2['Shock_Index'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Impute NaN Shock Index with mean\n",
        "shock_index_mean = final_merged_df2['Shock_Index'].mean()\n",
        "final_merged_df2['Shock_Index'] = final_merged_df2['Shock_Index'].fillna(shock_index_mean)\n",
        "\n",
        "print(\"✅ P/F Ratio and Shock Index calculated.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 5. SOFA & SIRS CALCULATION\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"\\n[STEP 5/5] Calculating SOFA and SIRS Scores...\")\n",
        "\n",
        "# Compute 4-hourly UO for SOFA Renal Score\n",
        "final_merged_df2['uo_step_4h'] = final_merged_df2.groupby('icustay_id')['uo_total'].diff().fillna(0)\n",
        "\n",
        "# SIRS Score\n",
        "def calculate_sirs(row):\n",
        "    \"\"\"Calculate SIRS score.\"\"\"\n",
        "    sirs_count = 0\n",
        "    if row['Temp_C'] >= 38 or row['Temp_C'] <= 36: sirs_count += 1\n",
        "    if row['HR'] > 90: sirs_count += 1\n",
        "    if row['RR'] >= 20 or row['paCO2'] <= 32: sirs_count += 1\n",
        "    if row['WBC_Count'] >= 12 or row['WBC_Count'] < 4: sirs_count += 1\n",
        "    return sirs_count\n",
        "\n",
        "final_merged_df2['SIRS'] = final_merged_df2.apply(calculate_sirs, axis=1)\n",
        "\n",
        "# SOFA Score\n",
        "def calculate_sofa(row):\n",
        "    \"\"\"Calculate SOFA score.\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # 1. Respiratory (PaO2/FiO2 ratio)\n",
        "    pf = row['PaO2_FiO2']\n",
        "    if pd.isna(pf): respiratory_score = 0\n",
        "    elif pf < 100: respiratory_score = 4\n",
        "    elif pf < 200: respiratory_score = 3\n",
        "    elif pf < 300: respiratory_score = 2\n",
        "    elif pf < 400: respiratory_score = 1\n",
        "    else: respiratory_score = 0\n",
        "    score += respiratory_score\n",
        "\n",
        "    # 2. Coagulation (Platelets)\n",
        "    plt = row['Platelets_count']\n",
        "    if plt < 20: score += 4\n",
        "    elif plt < 50: score += 3\n",
        "    elif plt < 100: score += 2\n",
        "    elif plt < 150: score += 1\n",
        "\n",
        "    # 3. Liver (Total Bilirubin)\n",
        "    bili = row['Total_bili']\n",
        "    if bili > 12: score += 4\n",
        "    elif bili >= 6: score += 3\n",
        "    elif bili >= 2: score += 2\n",
        "    elif bili >= 1.2: score += 1\n",
        "\n",
        "    # 4. Cardiovascular (MAP & Vasopressors)\n",
        "    map_val = row['MeanBP']\n",
        "    vaso_max = row['vaso_step_max']\n",
        "    cardio_score = 0\n",
        "    if vaso_max > 0.1: cardio_score = 4\n",
        "    elif vaso_max > 0 and vaso_max <= 0.1: cardio_score = 3\n",
        "    elif map_val < 65: cardio_score = 2\n",
        "    elif map_val < 70 and map_val >= 65: cardio_score = 1\n",
        "    score += cardio_score\n",
        "\n",
        "    # 5. Neurological (GCS)\n",
        "    gcs = row['GCS']\n",
        "    if gcs <= 5: score += 4\n",
        "    elif gcs <= 9: score += 3\n",
        "    elif gcs <= 12: score += 2\n",
        "    elif gcs <= 14: score += 1\n",
        "\n",
        "    # 6. Renal (Creatinine & UO)\n",
        "    cr = row['Creatinine']\n",
        "    uo = row['uo_step_4h']\n",
        "    renal_score = 0\n",
        "    if cr > 5.0 or uo < 34: renal_score = 4\n",
        "    elif (cr >= 3.5 and cr < 5) or (uo < 84 and uo >= 0): renal_score = 3\n",
        "    elif cr >= 2: renal_score = 2\n",
        "    elif cr >= 1.2: renal_score = 1\n",
        "    score += renal_score\n",
        "\n",
        "    return score\n",
        "\n",
        "final_merged_df2['SOFA'] = final_merged_df2.apply(calculate_sofa, axis=1)\n",
        "final_merged_df2.drop(columns=['uo_step_4h'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"✅ SOFA and SIRS Scores calculated.\")\n",
        "print(\"\\n=== Komorowski Process Completed ===\")\n",
        "\n",
        "# --- FINAL VERIFICATION ---\n",
        "print(\"\\n--- Verifying Key Columns (final_merged_df2) ---\")\n",
        "print(\n",
        "    final_merged_df2[\n",
        "        ['icustay_id', 'timestep_id', 'PaO2_FiO2', 'Shock_Index', 'SOFA', 'SIRS', 'HR', 'SysBP', 'Creatinine', 'Total_bili']\n",
        "    ].head(10).to_markdown(index=False)\n",
        ")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZEdN9Jdwvf5",
        "outputId": "87541493-2673-45e8-892c-dca76048a315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STARTING KOMOROWSKI IMPUTATION & FEATURE ENGINEERING ===\n",
            "DataFrame duplicated to 'final_merged_df2'.\n",
            "\n",
            "[STEP 1/5] Performing Linear Interpolation (< 5% Missingness)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Linear Interpolation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Linear Interpolation applied to 0 columns.\n",
            "\n",
            "[STEP 2/5] Performing k-NN Imputation (1-NN, Euclidean, Chunking) - LOCAL MEDIAN PATCH...\n",
            "Total rows to impute: 448521. Total columns to process: 64. Chunk size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   0%|          | 0/45 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 0 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   2%|▏         | 1/45 [01:00<44:12, 60.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 0 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 1 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   4%|▍         | 2/45 [02:01<43:32, 60.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 1 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 2 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   7%|▋         | 3/45 [02:49<38:25, 54.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 2 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 3 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   9%|▉         | 4/45 [03:56<40:53, 59.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 3 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 4 (5 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  11%|█         | 5/45 [04:58<40:19, 60.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 4 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 5 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  13%|█▎        | 6/45 [05:58<39:14, 60.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 5 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 6 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  16%|█▌        | 7/45 [06:45<35:33, 56.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 6 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 7 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  18%|█▊        | 8/45 [07:51<36:30, 59.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 7 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 8 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  20%|██        | 9/45 [08:51<35:34, 59.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 8 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 9 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  22%|██▏       | 10/45 [09:36<32:08, 55.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 9 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 10 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  24%|██▍       | 11/45 [10:39<32:27, 57.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 10 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 11 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  27%|██▋       | 12/45 [11:44<32:46, 59.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 11 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 12 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  29%|██▉       | 13/45 [12:44<32:00, 60.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 12 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 13 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  31%|███       | 14/45 [13:41<30:32, 59.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 13 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 14 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  33%|███▎      | 15/45 [14:40<29:28, 58.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 14 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 15 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  36%|███▌      | 16/45 [15:27<26:44, 55.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 15 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 16 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  38%|███▊      | 17/45 [16:32<27:07, 58.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 16 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 17 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  40%|████      | 18/45 [17:19<24:40, 54.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 17 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 18 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  42%|████▏     | 19/45 [18:18<24:22, 56.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 18 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 19 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  44%|████▍     | 20/45 [19:15<23:31, 56.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 19 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 20 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  47%|████▋     | 21/45 [20:17<23:14, 58.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 20 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 21 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  49%|████▉     | 22/45 [21:08<21:25, 55.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 21 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 22 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  51%|█████     | 23/45 [22:12<21:22, 58.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 22 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 23 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  53%|█████▎    | 24/45 [22:58<19:06, 54.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 23 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 24 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  56%|█████▌    | 25/45 [23:46<17:30, 52.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 24 fixed with median fallback. Continuing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  58%|█████▊    | 26/45 [24:08<13:46, 43.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 26 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  60%|██████    | 27/45 [25:10<14:42, 49.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 26 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 27 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  62%|██████▏   | 28/45 [26:08<14:37, 51.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 27 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 28 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  64%|██████▍   | 29/45 [26:55<13:27, 50.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 28 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 29 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  67%|██████▋   | 30/45 [27:41<12:17, 49.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 29 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 30 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  69%|██████▉   | 31/45 [28:41<12:11, 52.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 30 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 31 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  71%|███████   | 32/45 [29:42<11:53, 54.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 31 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 32 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  73%|███████▎  | 33/45 [30:29<10:29, 52.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 32 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 33 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  76%|███████▌  | 34/45 [31:32<10:11, 55.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 33 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 34 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  78%|███████▊  | 35/45 [32:34<09:36, 57.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 34 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 35 (5 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  80%|████████  | 36/45 [33:33<08:41, 57.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 35 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 36 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  82%|████████▏ | 37/45 [34:34<07:52, 59.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 36 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 37 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  84%|████████▍ | 38/45 [35:22<06:29, 55.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 37 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 38 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  87%|████████▋ | 39/45 [36:24<05:45, 57.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 38 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 39 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  89%|████████▉ | 40/45 [37:12<04:34, 54.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 39 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 40 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  91%|█████████ | 41/45 [38:15<03:48, 57.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 40 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 41 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  93%|█████████▎| 42/45 [39:02<02:42, 54.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 41 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 42 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  96%|█████████▌| 43/45 [40:01<01:51, 55.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 42 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 43 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  98%|█████████▊| 44/45 [40:49<00:53, 53.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 43 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 44 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "k-NN Chunk Imputation: 100%|██████████| 45/45 [41:47<00:00, 55.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 44 fixed with median fallback. Continuing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ k-NN Imputation completed successfully. 0 NaN remaining in clinical columns.\n",
            "\n",
            "[STEP 3/5] Cleaning Static Data...\n",
            "✅ Static data cleaned and capped.\n",
            "\n",
            "[STEP 4/5] Calculating Derived Features...\n",
            "✅ P/F Ratio and Shock Index calculated.\n",
            "\n",
            "[STEP 5/5] Calculating SOFA and SIRS Scores...\n",
            "✅ SOFA and SIRS Scores calculated.\n",
            "\n",
            "=== Komorowski Process Completed ===\n",
            "\n",
            "--- Verifying Key Columns (final_merged_df2) ---\n",
            "|   icustay_id |   timestep_id |   PaO2_FiO2 |   Shock_Index |   SOFA |   SIRS |    HR |   SysBP |   Creatinine |   Total_bili |\n",
            "|-------------:|--------------:|------------:|--------------:|-------:|-------:|------:|--------:|-------------:|-------------:|\n",
            "|       200001 |             1 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             2 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             3 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             4 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             5 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             6 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             7 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             8 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             9 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |            10 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# 0. SETUP\n",
        "# ======================================================================\n",
        "# Define the list of 47 time-series features for missingness inspection\n",
        "TIME_SERIES_FEATURES = [\n",
        "    'SOFA', 'SIRS', 'RR', 'Temp_C', 'SpO2', 'HR', 'MeanBP', 'GCS',\n",
        "    'Lactate', 'Arterial_pH', 'paO2', 'paCO2', 'FiO2', 'PaO2_FiO2', 'Sodium', 'Potassium',\n",
        "    'Chloride', 'HCO3', 'BUN', 'Creatinine', 'Total_bili', 'Albumin', 'Platelets_count',\n",
        "    'WBC_Count', 'HCT', 'Glucose', 'input_total', 'uo_total', 'SysBP', 'DiaBP', 'Shock_Index',\n",
        "    'Arterial_BE', 'Ionized Calcium', 'SGOT(AST)', 'SGPT(ALT)', 'PT', 'PTT', 'INR', 'Hb',\n",
        "    'Magnesium', 'Calcium', 'cumulated_balance', 'Weight_kg', 'elixhauser', 'age', 'gender_numeric'\n",
        "]\n",
        "# Ensure all 47 time-series features are included.\n",
        "\n",
        "# Create a copy of 'final_merged_df2'\n",
        "reformat4t = final_merged_df2.copy()\n",
        "print(\"Starting with DataFrame 'reformat4t' (copy of final_merged_df2).\")\n",
        "\n",
        "# ======================================================================\n",
        "# 1. PREPARATION OF REQUIRED COLUMNS\n",
        "# ======================================================================\n",
        "# Ensure 4-hourly input/output columns exist before exclusions\n",
        "reformat4t['input_4hourly'] = reformat4t.groupby('icustay_id')['input_total'].diff().fillna(0)\n",
        "reformat4t['output_4hourly'] = reformat4t.groupby('icustay_id')['uo_total'].diff().fillna(0)\n",
        "\n",
        "# Rename columns for consistency\n",
        "reformat4t.rename(columns={'morta_90': 'mortality_90d', 'vaso_step_max': 'max_dose_vaso', 'timestep_id': 'bloc'}, inplace=True)\n",
        "reformat4t['max_dose_vaso'] = reformat4t['max_dose_vaso'].fillna(0)\n",
        "\n",
        "# ======================================================================\n",
        "# 2. UTILITY FUNCTIONS (VECTORIZED)\n",
        "# ======================================================================\n",
        "def numel_unique(df, col):\n",
        "    \"\"\"Count the number of unique patient IDs.\"\"\"\n",
        "    return len(df[col].unique())\n",
        "\n",
        "def remove_patients(df, condition):\n",
        "    \"\"\"Remove all rows belonging to patients (icustay_id) that meet a given condition.\"\"\"\n",
        "    ids_to_remove = df.loc[condition, 'icustay_id'].unique()\n",
        "    mask = df['icustay_id'].isin(ids_to_remove)\n",
        "    return df[~mask].reset_index(drop=True)\n",
        "\n",
        "# ======================================================================\n",
        "# 3. HIERARCHICAL MORTALITY IMPUTATION (Fill NaN -> 1 if confirmed dead)\n",
        "# ======================================================================\n",
        "print(\"\\n--- MORTALITY IMPUTATION (Hierarchical Filling NaN -> 1) ---\")\n",
        "\n",
        "# 3.1 Ensure 'onset_time_T0', 'dod', 'expire_flag', 'outtime', and 'endtime' are available\n",
        "if 'onset_time_T0' not in reformat4t.columns:\n",
        "    onset_df_full = final_merged_df2[['icustay_id', 'onset_time_T0']].drop_duplicates()\n",
        "    reformat4t = pd.merge(reformat4t, onset_df_full, on='icustay_id', how='left')\n",
        "\n",
        "# ENSURE that 'outtime' and 'endtime' columns are merged from demog or another source\n",
        "if (\n",
        "    'dod' not in reformat4t.columns\n",
        "    or 'expire_flag' not in reformat4t.columns\n",
        "    or 'outtime' not in reformat4t.columns\n",
        "    or 'intime' not in reformat4t.columns\n",
        "):\n",
        "    # Replace demog_for_merge with all required time-related columns\n",
        "    demog_for_merge = demog[['icustay_id', 'dod', 'expire_flag', 'outtime', 'intime']].drop_duplicates().copy()\n",
        "    reformat4t = pd.merge(reformat4t, demog_for_merge, on='icustay_id', how='left')\n",
        "\n",
        "# 3.2 Convert date columns\n",
        "try:\n",
        "    reformat4t['onset_time_T0'] = pd.to_datetime(reformat4t['onset_time_T0'], errors='coerce')\n",
        "    reformat4t['dod'] = pd.to_datetime(reformat4t['dod'], errors='coerce')\n",
        "    # Also convert outtime and endtime\n",
        "    reformat4t['outtime'] = pd.to_datetime(reformat4t['outtime'], errors='coerce')\n",
        "    reformat4t['intime'] = pd.to_datetime(reformat4t['intime'], errors='coerce')\n",
        "except Exception as e:\n",
        "    print(f\"ERROR converting time columns: {e}. Cannot proceed with time calculations.\")\n",
        "\n",
        "\n",
        "# 3.3 Impute 'morta_hosp' using 'expire_flag'\n",
        "reformat4t.loc[reformat4t['morta_hosp'].isnull() & (reformat4t['expire_flag'] == 1), 'morta_hosp'] = 1\n",
        "print(f\"Filled NaN morta_hosp with 1 where expire_flag=1: {reformat4t.loc[reformat4t['morta_hosp'] == 1, 'icustay_id'].nunique()} patients now have morta_hosp=1.\")\n",
        "\n",
        "# 3.4 Impute 'mortality_90d' using DOD within 90 days of onset\n",
        "ninety_days = pd.Timedelta(days=90)\n",
        "condition_morta_90d_calc = (\n",
        "    reformat4t['mortality_90d'].isnull() &\n",
        "    reformat4t['dod'].notna() &\n",
        "    ((reformat4t['dod'] - reformat4t['onset_time_T0']) <= ninety_days) &\n",
        "    ((reformat4t['dod'] - reformat4t['onset_time_T0']) >= pd.Timedelta(seconds=0))\n",
        ")\n",
        "reformat4t.loc[condition_morta_90d_calc, 'mortality_90d'] = 1\n",
        "print(\"Filled NaN mortality_90d with 1 based on DOD check.\")\n",
        "\n",
        "# 3.5 Calculate time variables for early mortality exclusion\n",
        "# A. died_within_48h_of_out_time (boolean)\n",
        "# Criteria: (Patient died) AND (Death time - ICU discharge time ≤ 48 hours)\n",
        "hours_48 = pd.Timedelta(hours=48)\n",
        "reformat4t['died_within_48h_of_out_time'] = (\n",
        "    reformat4t['dod'].notna() &\n",
        "    (reformat4t['dod'] - reformat4t['outtime'] <= hours_48) &\n",
        "    (reformat4t['dod'] > reformat4t['outtime'])  # Ensure DOD occurs after OUTTIME\n",
        ")\n",
        "\n",
        "# B. delay_end_of_record_and_discharge_or_death (in hours)\n",
        "# The time difference between the end of recorded data (endtime) and the time of discharge/death (outtime/dod)\n",
        "# Take the earlier of outtime and dod as 'discharge_or_death_time'\n",
        "hours_4 = pd.Timedelta(hours=4)\n",
        "reformat4t['record_endtime'] = reformat4t['intime'] + (reformat4t['bloc'] * hours_4)\n",
        "\n",
        "reformat4t['discharge_or_death_time'] = np.where(\n",
        "    reformat4t['dod'].notna(),\n",
        "    reformat4t['dod'],\n",
        "    reformat4t['outtime']\n",
        ")\n",
        "\n",
        "# Compute the time difference in hours\n",
        "reformat4t['delay_end_of_record_and_discharge_or_death'] = (\n",
        "    reformat4t['discharge_or_death_time'] - reformat4t['record_endtime']\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# Note: 'endtime' is used as a proxy for the end of the record since 'bloc' only captures 4-hour intervals\n",
        "\n",
        "print(\"Calculation complete.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 4. PATIENT EXCLUSION\n",
        "# ======================================================================\n",
        "print(\"\\n--- PATIENT EXCLUSION ---\")\n",
        "count_before = numel_unique(reformat4t, 'icustay_id')\n",
        "print(f\"Number of patients before exclusion: {count_before}\")\n",
        "\n",
        "# 4.1 Extreme adn unrealistic HR, RR, urine output, Total Bilirubin, and intake (> 10000 ml per 4 hours)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['HR'] < 10)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['RR'] > 60)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['output_4hourly'] > 12000)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['Total_bili'] > 10000)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['input_4hourly'] > 10000)\n",
        "print(f\"Number of patients after UO/Bili/Intake outlier exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.2 Exclude patients under 18 years old\n",
        "print(\"Excluding patients younger than 18 years old...\")\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['age'] < 18)\n",
        "print(f\"Number of patients after age <18 exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.3 Exclude patients who did not reach T=0 (did not reach Bloc 7)\n",
        "print(\"Excluding patients who did not reach T=0 (Bloc 7)...\")\n",
        "max_bloc_per_patient = reformat4t.groupby('icustay_id')['bloc'].max().reset_index()\n",
        "MIN_BLOC_ONSET = 7\n",
        "ids_to_remove_before_t0 = max_bloc_per_patient.loc[max_bloc_per_patient['bloc'] < MIN_BLOC_ONSET, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_before_t0)].reset_index(drop=True)\n",
        "print(f\"Number of patients after exclusion before T=0: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.4 Exclude patients with ICU stay duration < 24 hours (Min 6 blocs) AFTER ONSET (T=0)\n",
        "print(\"Excluding patients with LOS < 24 hours (Min 6 blocs) after T=0...\")\n",
        "MIN_BLOC_AFTER_ONSET = 6  # 24 hours\n",
        "MIN_BLOC_TOTAL = MIN_BLOC_ONSET + MIN_BLOC_AFTER_ONSET  # Bloc 7 + 6 = Bloc 13\n",
        "ids_to_remove_los = max_bloc_per_patient.loc[max_bloc_per_patient['bloc'] < MIN_BLOC_TOTAL, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_los)].reset_index(drop=True)\n",
        "print(f\"Number of patients after LOS < {MIN_BLOC_TOTAL} blocs exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# Create patient_stats\n",
        "patient_stats = reformat4t.groupby('icustay_id').agg(\n",
        "    max_mortality_90d=('mortality_90d', 'max'),\n",
        "    max_vaso=('max_dose_vaso', 'max'),\n",
        "    max_sofa=('SOFA', 'max'),\n",
        "    max_bloc=('bloc', 'max')\n",
        ").reset_index()\n",
        "\n",
        "# Patients with mortality_90d=1 dan max_bloc less than 13\n",
        "ids_to_remove_early_death_absolute = patient_stats.loc[\n",
        "    (patient_stats['max_mortality_90d'] == 1) &\n",
        "    (patient_stats['max_bloc'] < MIN_BLOC_TOTAL), # MIN_BLOC_TOTAL=13\n",
        "    'icustay_id'\n",
        "].unique()\n",
        "\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_early_death_absolute)].reset_index(drop=True)\n",
        "print(f\"Number of patients after absolute early death exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.5 Exclude patients with high missingness (≥8 out of 46 variables)\n",
        "print(\"Excluding patients with high missing data (≥8 out of 46 features)...\")\n",
        "missing_counts = reformat4t.groupby('icustay_id')[TIME_SERIES_FEATURES].apply(\n",
        "    lambda x: x.isnull().sum(axis=1).max()\n",
        ").reset_index(name='max_missing_features')\n",
        "MAX_MISSING_ALLOWED = 7\n",
        "ids_to_remove_high_missing = missing_counts.loc[missing_counts['max_missing_features'] > MAX_MISSING_ALLOWED, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_high_missing)].reset_index(drop=True)\n",
        "print(f\"Number of patients after high missing data exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.6 Exclude patients whose treatment was withdrawn\n",
        "# Create withdrawal_stats\n",
        "withdrawal_stats = reformat4t.groupby('icustay_id').agg(\n",
        "    max_mortality_90d=('mortality_90d', 'max'),\n",
        "    max_vaso=('max_dose_vaso', 'max'),\n",
        "    max_sofa=('SOFA', 'max'),\n",
        "    max_bloc=('bloc', 'max')\n",
        ").reset_index()\n",
        "\n",
        "last_rows = reformat4t[reformat4t['bloc'] == reformat4t.groupby('icustay_id')['bloc'].transform('max')].copy()\n",
        "\n",
        "# Merge using withdrawal_stats\n",
        "merged_df = last_rows.merge(\n",
        "    withdrawal_stats[['icustay_id', 'max_mortality_90d', 'max_vaso', 'max_sofa', 'max_bloc']],\n",
        "    on='icustay_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "condition_withdrawal = (\n",
        "    (merged_df['max_mortality_90d'] == 1) &\n",
        "    (merged_df['max_dose_vaso'] == 0) &\n",
        "    (merged_df['max_vaso'] > 0.3) &\n",
        "    (merged_df['SOFA'] >= merged_df['max_sofa'] / 2) &\n",
        "    (merged_df['max_bloc'] < 20)\n",
        ")\n",
        "\n",
        "ids_to_remove_withdrawal = merged_df.loc[condition_withdrawal, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_withdrawal)].reset_index(drop=True)\n",
        "print(f\"Number of patients after treatment withdrawal exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.7 Exclude patients who died in ICU during the data collection period\n",
        "print(\"Excluding patients who died within 48h of discharge AND had a short delay between end of record and discharge/death...\")\n",
        "\n",
        "# Only one row per patient is needed; take the last record per icustay_id\n",
        "last_rows_to_check = reformat4t[reformat4t['bloc'] == reformat4t.groupby('icustay_id')['bloc'].transform('max')].copy()\n",
        "\n",
        "condition_early_death_after_record = (\n",
        "    (last_rows_to_check['died_within_48h_of_out_time'] == True) &\n",
        "    (last_rows_to_check['delay_end_of_record_and_discharge_or_death'] < 24)\n",
        ")\n",
        "\n",
        "# Get IDs of patients meeting the exclusion criteria\n",
        "ids_to_remove_early_death = last_rows_to_check.loc[condition_early_death_after_record, 'icustay_id'].unique()\n",
        "\n",
        "# Remove those patients from the main DataFrame\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_early_death)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Number of patients after early death exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# Final count after all exclusions\n",
        "count_after = numel_unique(reformat4t, 'icustay_id')\n",
        "print(f\"Number of patients remaining after all exclusions: {count_after}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 5. FINAL MORTALITY IMPUTATION (Conservative Censor: NaN -> 0)\n",
        "# ======================================================================\n",
        "print(\"\\n--- FINAL MORTALITY IMPUTATION (Conservative Censor: NaN -> 0) ---\")\n",
        "reformat4t['morta_hosp'] = reformat4t['morta_hosp'].fillna(0)\n",
        "reformat4t['mortality_90d'] = reformat4t['mortality_90d'].fillna(0)\n",
        "print(\"All remaining NaN mortality labels are set to 0.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 6. TRUNCATE RECORDS TO T=20\n",
        "# ======================================================================\n",
        "print(\"\\n--- TRUNCATING RECORDS TO T=20 (80 HOURS) ---\")\n",
        "MAX_BLOC = 20\n",
        "reformat4t = reformat4t[reformat4t['bloc'] <= MAX_BLOC].reset_index(drop=True)\n",
        "print(f\"Total records after truncation (Max bloc={MAX_BLOC}): {len(reformat4t)}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 7. CREATE SEPSIS COHORT & SAVE TIME-SERIES DATA\n",
        "# ======================================================================\n",
        "print(\"\\n--- SEPSIS COHORT CREATION & DATA SAVING (Filtering T=0 SOFA) ---\")\n",
        "\n",
        "# 7.1 Get SOFA at T=0 (Bloc 7)\n",
        "t0_records = reformat4t[reformat4t['bloc'] == 7].copy()\n",
        "final_patient_t0_summary = t0_records.groupby('icustay_id').agg(\n",
        "    morta_90d=('mortality_90d', 'max'),\n",
        "    sofa_at_t0=('SOFA', 'first'),\n",
        "    max_sofa_overall=('SOFA', 'max')\n",
        ").reset_index()\n",
        "\n",
        "# 7.2 Apply Sepsis-3 filter (SOFA at T=0 ≥ 2)\n",
        "sepsis_cohort_ids = final_patient_t0_summary.loc[final_patient_t0_summary['sofa_at_t0'] >= 2, 'icustay_id'].unique()\n",
        "\n",
        "# 7.3 Ensure there's no critical features missing in T=0\n",
        "print(\"Excluding patients missing critical T=0 features (SOFA, Lactate, PaO2/FiO2)...\")\n",
        "critical_t0_missing = reformat4t[reformat4t['bloc'] == MIN_BLOC_ONSET].copy()\n",
        "condition_critical_missing = (\n",
        "    critical_t0_missing['SOFA'].isnull() |\n",
        "    critical_t0_missing['Lactate'].isnull() |\n",
        "    critical_t0_missing['PaO2_FiO2'].isnull()\n",
        ")\n",
        "ids_to_remove_critical = critical_t0_missing.loc[condition_critical_missing, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_critical)].reset_index(drop=True)\n",
        "print(f\"Number of patients after critical T=0 missing exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 7.4 Save time-series data\n",
        "final_sepsis_df = reformat4t[reformat4t['icustay_id'].isin(sepsis_cohort_ids)].reset_index(drop=True)\n",
        "output_data_filename = 'SEPSIS-COHORT-TIMESERIES.csv'\n",
        "output_data_path = os.path.join(data_dir, output_data_filename)\n",
        "\n",
        "final_sepsis_df.to_csv(output_data_path, index=False)\n",
        "\n",
        "print(f\"Sepsis Cohort Time-Series Data saved to: {output_data_path}\")\n",
        "print(f\"Total records in this file: {len(final_sepsis_df)}\")\n",
        "\n",
        "# 7.4 Save patient summary\n",
        "sepsis_cohort_summary = final_patient_t0_summary.loc[final_patient_t0_summary['icustay_id'].isin(sepsis_cohort_ids)].copy()\n",
        "onset_times = final_merged_df2.drop_duplicates(subset=['icustay_id'])[['icustay_id', 'onset_time_T0']].set_index('icustay_id')\n",
        "sepsis_cohort_summary = sepsis_cohort_summary.merge(onset_times, on='icustay_id', how='left')\n",
        "sepsis_cohort_summary.rename(columns={'onset_time_T0': 'sepsis_time'}, inplace=True)\n",
        "\n",
        "print(f\"Number of final Sepsis patients (SOFA at T=0 ≥ 2): {len(sepsis_cohort_summary)}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVRqSx2wUqeR",
        "outputId": "d352bc7d-8ceb-428c-8fbc-d6a804c593ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with DataFrame 'reformat4t' (copy of final_merged_df2).\n",
            "\n",
            "--- MORTALITY IMPUTATION (Hierarchical Filling NaN -> 1) ---\n",
            "Filled NaN morta_hosp with 1 where expire_flag=1: 3223 patients now have morta_hosp=1.\n",
            "Filled NaN mortality_90d with 1 based on DOD check.\n",
            "Calculation complete.\n",
            "\n",
            "--- PATIENT EXCLUSION ---\n",
            "Number of patients before exclusion: 24948\n",
            "Number of patients after UO/Bili/Intake outlier exclusion: 24919\n",
            "Excluding patients younger than 18 years old...\n",
            "Number of patients after age <18 exclusion: 21965\n",
            "Excluding patients who did not reach T=0 (Bloc 7)...\n",
            "Number of patients after exclusion before T=0: 21349\n",
            "Excluding patients with LOS < 24 hours (Min 6 blocs) after T=0...\n",
            "Number of patients after LOS < 13 blocs exclusion: 19323\n",
            "Number of patients after absolute early death exclusion: 19323\n",
            "Excluding patients with high missing data (≥8 out of 46 features)...\n",
            "Number of patients after high missing data exclusion: 19323\n",
            "Number of patients after treatment withdrawal exclusion: 19293\n",
            "Excluding patients who died within 48h of discharge AND had a short delay between end of record and discharge/death...\n",
            "Number of patients after early death exclusion: 19192\n",
            "Number of patients remaining after all exclusions: 19192\n",
            "\n",
            "--- FINAL MORTALITY IMPUTATION (Conservative Censor: NaN -> 0) ---\n",
            "All remaining NaN mortality labels are set to 0.\n",
            "\n",
            "--- TRUNCATING RECORDS TO T=20 (80 HOURS) ---\n",
            "Total records after truncation (Max bloc=20): 371574\n",
            "\n",
            "--- SEPSIS COHORT CREATION & DATA SAVING (Filtering T=0 SOFA) ---\n",
            "Excluding patients missing critical T=0 features (SOFA, Lactate, PaO2/FiO2)...\n",
            "Number of patients after critical T=0 missing exclusion: 19192\n",
            "Sepsis Cohort Time-Series Data saved to: /content/drive/MyDrive/mimic3/SEPSIS-COHORT-TIMESERIES.csv\n",
            "Total records in this file: 364563\n",
            "Number of final Sepsis patients (SOFA at T=0 ≥ 2): 18830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Pivoting relevant columns for final DataFrame\n",
        "# ======================================================================\n",
        "\n",
        "print(\"--- STARTING FINAL COHORT REFINEMENT AND MERGE ---\")\n",
        "\n",
        "# ======================================================================\n",
        "# 1. PREPARE MECHANICAL VENTILATION DATA (MechVent)\n",
        "# ======================================================================\n",
        "mechvent_df = MechVent[['icustay_id', 'MechVent']].dropna(subset=['icustay_id']).copy()\n",
        "mechvent_df['icustay_id'] = mechvent_df['icustay_id'].astype(int)\n",
        "\n",
        "# Aggregate: Determine if the patient was ever intubated (max MechVent = 1)\n",
        "mechvent_status = mechvent_df.groupby('icustay_id')['MechVent'].max().reset_index()\n",
        "mechvent_status.rename(columns={'MechVent': 'on_mechvent'}, inplace=True)\n",
        "\n",
        "# ======================================================================\n",
        "# 2. PREPARE READMISSION DATA (is_readmit)\n",
        "# ======================================================================\n",
        "# Extract 'is_readmit' from the original merged dataset (final_merged_df2)\n",
        "readmit_status = final_merged_df[['icustay_id', 'is_readmit']].drop_duplicates().copy()\n",
        "\n",
        "# ======================================================================\n",
        "# 3. MERGE STATIC FEATURES INTO FINAL SEPSIS DATAFRAME\n",
        "# ======================================================================\n",
        "# Merge ventilation status\n",
        "if 'on_mechvent' in final_sepsis_df.columns:\n",
        "    final_sepsis_df.drop(columns=['on_mechvent'], inplace=True)\n",
        "final_sepsis_df = pd.merge(\n",
        "    final_sepsis_df,\n",
        "    mechvent_status,\n",
        "    on='icustay_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merge readmission status\n",
        "if 'is_readmit' in final_sepsis_df.columns:\n",
        "    final_sepsis_df.drop(columns=['is_readmit'], inplace=True)\n",
        "\n",
        "final_sepsis_df = pd.merge(\n",
        "    final_sepsis_df,\n",
        "    readmit_status,\n",
        "    on='icustay_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Clean missing static feature values (if any)\n",
        "# Assumption: Missing on_mechvent or is_readmit means 0\n",
        "final_sepsis_df['on_mechvent'] = final_sepsis_df['on_mechvent'].fillna(0).astype(int)\n",
        "final_sepsis_df['is_readmit'] = final_sepsis_df['is_readmit'].fillna(0).astype(int)\n",
        "\n",
        "# ======================================================================\n",
        "# 4. FILTER REQUIRED COLUMNS AND SAVE FINAL OUTPUT\n",
        "# ======================================================================\n",
        "# Add back 'bloc' and 'mortality_90d' because they are fundamental for\n",
        "# time-series structure and outcome labeling (even if not in the 47 features)\n",
        "FINAL_COLUMNS = ['icustay_id', 'bloc', 'morta_hosp', 'mortality_90d', 'is_readmit', 'on_mechvent', 'max_dose_vaso'] + TIME_SERIES_FEATURES\n",
        "\n",
        "# Ensure all required columns exist before saving\n",
        "missing_cols = [col for col in FINAL_COLUMNS if col not in final_sepsis_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"⚠️ Warning: Missing columns in final_sepsis_df before saving: {missing_cols}\")\n",
        "\n",
        "# Select only the required columns\n",
        "final_output_df = final_sepsis_df[FINAL_COLUMNS].copy()\n",
        "\n",
        "# Save final cleaned dataset\n",
        "output_data_filename_final = 'SEPSIS-COHORT-FINAL-DF.csv'\n",
        "output_data_path_final = os.path.join(data_dir, output_data_filename_final)\n",
        "\n",
        "final_output_df.to_csv(output_data_path_final, index=False)\n",
        "\n",
        "print(f\"\\nFinal Sepsis Cohort data successfully saved to:{output_data_path_final}\")\n",
        "print(f\"Total records: {len(final_output_df)}\")\n",
        "print(f\"Total columns: {len(final_output_df.columns)}\")\n",
        "print(f\"Saved columns: {final_output_df.columns.tolist()}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "matga7PkqAkf",
        "outputId": "b3d0e714-d3c0-4c82-a920-a03219750ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STARTING FINAL COHORT REFINEMENT AND MERGE ---\n",
            "\n",
            "Final Sepsis Cohort data successfully saved to:/content/drive/MyDrive/mimic3/SEPSIS-COHORT-FINAL-DF.csv\n",
            "Total records: 364563\n",
            "Total columns: 53\n",
            "Saved columns: ['icustay_id', 'bloc', 'morta_hosp', 'mortality_90d', 'is_readmit', 'on_mechvent', 'max_dose_vaso', 'SOFA', 'SIRS', 'RR', 'Temp_C', 'SpO2', 'HR', 'MeanBP', 'GCS', 'Lactate', 'Arterial_pH', 'paO2', 'paCO2', 'FiO2', 'PaO2_FiO2', 'Sodium', 'Potassium', 'Chloride', 'HCO3', 'BUN', 'Creatinine', 'Total_bili', 'Albumin', 'Platelets_count', 'WBC_Count', 'HCT', 'Glucose', 'input_total', 'uo_total', 'SysBP', 'DiaBP', 'Shock_Index', 'Arterial_BE', 'Ionized Calcium', 'SGOT(AST)', 'SGPT(ALT)', 'PT', 'PTT', 'INR', 'Hb', 'Magnesium', 'Calcium', 'cumulated_balance', 'Weight_kg', 'elixhauser', 'age', 'gender_numeric']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- STATISTIK DESKRIPTIF UNTUK FINAL SEPSIS COHORT (final_output_df) ---\")\n",
        "\n",
        "# 1. Statistik Dasar (Count, Mean, Std, Min, Max, Quartiles) untuk Kolom Numerik\n",
        "print(\"\\n[1] RINGKASAN STATISTIK KOLOM NUMERIK:\")\n",
        "print(final_output_df.describe(include=np.number).transpose().to_markdown())\n",
        "\n",
        "# 2. Statistik Kolom Kategorial/Biner (gender_numeric, is_readmit, ever_on_mechvent)\n",
        "print(\"\\n[2] RINGKASAN STATISTIK KOLOM BINARY/KATEGORIAL (Mode, Frekuensi):\")\n",
        "# Kolom Biner/Kategorikal yang relevan:\n",
        "BINARY_COLS = ['gender_numeric', 'mortality_90d', 'is_readmit', 'on_mechvent']\n",
        "for col in BINARY_COLS:\n",
        "    if col in final_output_df.columns:\n",
        "        print(f\"\\n--- {col} ---\")\n",
        "        print(final_output_df[col].value_counts(normalize=True).mul(100).round(2).to_markdown())\n",
        "\n",
        "# 3. Informasi Missingness (secara keseluruhan)\n",
        "print(\"\\n[3] JUMLAH MISSING VALUES PER KOLOM:\")\n",
        "# Hanya periksa 47 fitur time-series + 5 konteks\n",
        "ALL_FEATURE_COLUMNS = ['icustay_id', 'bloc', 'mortality_90d', 'is_readmit', 'on_mechvent'] + TIME_SERIES_FEATURES\n",
        "\n",
        "# Hitung jumlah missing values dan persentasenya\n",
        "missing_info = final_output_df[ALL_FEATURE_COLUMNS].isnull().sum().reset_index()\n",
        "missing_info.columns = ['Kolom', 'Missing Count']\n",
        "missing_info['Missing Percentage'] = (missing_info['Missing Count'] / len(final_output_df)) * 100\n",
        "missing_info = missing_info[missing_info['Missing Count'] > 0].sort_values(by='Missing Percentage', ascending=False)\n",
        "\n",
        "if not missing_info.empty:\n",
        "    print(missing_info.round(2).to_markdown(index=False))\n",
        "else:\n",
        "    print(\"Tidak ada missing values (NaN) pada fitur-fitur yang diperiksa.\")\n",
        "\n",
        "print(f\"\\nTotal Records dalam DataFrame: {len(final_output_df)}\")\n",
        "print(f\"Total Unique Patients: {final_output_df['icustay_id'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-ZVdH8htcwD",
        "outputId": "ad0cb0ce-52a6-4bd9-af5f-48f0ee21750b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- STATISTIK DESKRIPTIF UNTUK FINAL SEPSIS COHORT (final_output_df) ---\n",
            "\n",
            "[1] RINGKASAN STATISTIK KOLOM NUMERIK:\n",
            "|                   |   count |           mean |           std |           min |           25% |           50% |           75% |         max |\n",
            "|:------------------|--------:|---------------:|--------------:|--------------:|--------------:|--------------:|--------------:|------------:|\n",
            "| icustay_id        |  364563 | 249627         | 28881.6       | 200001        | 224440        | 249690        | 274339        | 299995      |\n",
            "| bloc              |  364563 |     10.2493    |     5.68175   |      1        |      5        |     10        |     15        |     20      |\n",
            "| morta_hosp        |  364563 |      0.1411    |     0.348126  |      0        |      0        |      0        |      0        |      1      |\n",
            "| mortality_90d     |  364563 |      0.26311   |     0.440322  |      0        |      0        |      0        |      1        |      1      |\n",
            "| is_readmit        |  364563 |      0.339341  |     0.473486  |      0        |      0        |      0        |      1        |      1      |\n",
            "| on_mechvent       |  364563 |      0.566399  |     0.495572  |      0        |      0        |      1        |      1        |      1      |\n",
            "| max_dose_vaso     |  364563 |      0.0353979 |     0.512798  |      0        |      0        |      0        |      0        |    175.325  |\n",
            "| SOFA              |  364563 |      7.49742   |     3.15696   |      0        |      5        |      7        |     10        |     23      |\n",
            "| SIRS              |  364563 |      1.93398   |     1.04982   |      0        |      1        |      2        |      3        |      4      |\n",
            "| RR                |  364563 |     21.0027    |     5.58467   |      5        |     17        |     20.2      |     23.8571   |     38      |\n",
            "| Temp_C            |  364563 |     36.5008    |     1.13614   |     34.6      |     35.5      |     36.6741   |     37.38     |     38.9    |\n",
            "| SpO2              |  364563 |     96.9348    |     2.38538   |     89        |     95.3333   |     97.3333   |     99        |    100      |\n",
            "| HR                |  364563 |     87.7632    |    15.9106    |     36        |     76        |     86.6      |     98.5      |    140      |\n",
            "| MeanBP            |  364563 |     77.8576    |    12.9322    |     34        |     68.1      |     77.6      |     86.0714   |    120      |\n",
            "| GCS               |  364563 |     12.4801    |     3.44646   |      3        |     10        |     14.4286   |     15        |     15      |\n",
            "| Lactate           |  364563 |      1.81837   |     1.17848   |      0.4      |      1.1      |      1.4      |      2.16667  |      7      |\n",
            "| Arterial_pH       |  364563 |      7.39569   |     0.0777209 |      7.17     |      7.35     |      7.4      |      7.45     |      7.6    |\n",
            "| paO2              |  364563 |    114.778     |    41.6102    |     32        |     83        |    107        |    140        |    228      |\n",
            "| paCO2             |  364563 |     41.7858    |     9.47618   |     17        |     35        |     40        |     46.875    |     65      |\n",
            "| FiO2              |  364563 |      0.416768  |     0.0981048 |      0.24     |      0.330909 |      0.4      |      0.5      |      0.75   |\n",
            "| PaO2_FiO2         |  364563 |    294.226     |   136.52      |     45.3333   |    194        |    280        |    342.857    |    908.333  |\n",
            "| Sodium            |  364563 |    138.685     |     5.06781   |    127        |    135.6      |    139        |    142        |    151      |\n",
            "| Potassium         |  364563 |      4.09533   |     0.576657  |      2.6      |      3.7      |      4.1      |      4.4      |      5.6    |\n",
            "| Chloride          |  364563 |    104.849     |     6.76393   |     85        |    100        |    105        |    108        |    125      |\n",
            "| HCO3              |  364563 |     24.5502    |     5.37295   |     10.75     |     21        |     24.6      |     27.4286   |     38      |\n",
            "| BUN               |  364563 |     31.3688    |    20.3589    |      0        |     17        |     25        |     39.5714   |     88      |\n",
            "| Creatinine        |  364563 |      1.34381   |     0.880713  |      0.1      |      0.7      |      1        |      1.8      |      3.8    |\n",
            "| Total_bili        |  364563 |      2.69978   |     4.60381   |      0.2      |      0.4      |      0.7      |      2.6      |     56.8    |\n",
            "| Albumin           |  364563 |      2.8528    |     0.635178  |      1.1      |      2.4      |      2.8      |      3.4      |      4.3    |\n",
            "| Platelets_count   |  364563 |    209.494     |   120.884     |     10        |    113        |    192        |    271.333    |    519      |\n",
            "| WBC_Count         |  364563 |     10.8312    |     5.80627   |      0        |      6.9      |      9.9      |     13.8      |     28.1    |\n",
            "| HCT               |  364563 |     29.9607    |     4.40496   |     17.5      |     26.8912   |     29.9      |     33        |     41.5    |\n",
            "| Glucose           |  364563 |    133.2       |    40.0037    |     34        |    104        |    132.818    |    157.75     |    257      |\n",
            "| input_total       |  364563 |   2753.51      |  4863         | -16479.1      |      0        |    653        |   3741        |  84879      |\n",
            "| uo_total          |  364563 |   2018.42      |  3230.97      |      0        |      0        |    650        |   2900        |  90750      |\n",
            "| SysBP             |  364563 |    121.292     |    21.202     |     10.9999   |    104.667    |    118        |    132.6      |    223      |\n",
            "| DiaBP             |  364563 |     56.2969    |    12.7108    |      0        |     48.6429   |     54.8      |     64        |    111      |\n",
            "| Shock_Index       |  364563 |      0.745     |     0.18603   |      0.243523 |      0.628125 |      0.710728 |      0.865793 |     10.3637 |\n",
            "| Arterial_BE       |  364563 |      3.10655   |     3.28526   |      0        |      0        |      2        |      5        |     13      |\n",
            "| Ionized Calcium   |  364563 |      1.13001   |     0.0741883 |      0.89     |      1.08     |      1.13     |      1.18     |      1.35   |\n",
            "| SGOT(AST)         |  364563 |     46.7082    |    35.5108    |      6        |     20        |     35        |     62        |    157      |\n",
            "| SGPT(ALT)         |  364563 |     35.2827    |    26.0946    |      2        |     15        |     28        |     49        |    112      |\n",
            "| PT                |  364563 |     14.745     |     2.45388   |     10        |     13.2      |     14        |     15.7      |     23.7    |\n",
            "| PTT               |  364563 |     36.848     |    13.9151    |     13.7      |     27.8      |     31.7      |     40.6      |     85.7    |\n",
            "| INR               |  364563 |      1.43343   |     0.404501  |      0.8      |      1.2      |      1.3      |      1.5      |      2.9    |\n",
            "| Hb                |  364563 |      9.97386   |     1.54052   |      5.68973  |      8.9      |      9.9      |     11        |     14.0755 |\n",
            "| Magnesium         |  364563 |      2.01101   |     0.339155  |      1.1      |      1.8      |      2        |      2.23333  |      3      |\n",
            "| Calcium           |  364563 |      8.30561   |     0.704022  |      6.3      |      7.8      |      8.3      |      8.8      |     10.2    |\n",
            "| cumulated_balance |  364563 |    735.091     |  4425.29      | -87023.8      |   -197        |      0        |   1175        |  75818.1    |\n",
            "| Weight_kg         |  364563 |     76.5846    |    19.9276    |     25.6      |     61.8      |     74.907    |     90.8      |    134.5    |\n",
            "| elixhauser        |  364563 |      4.06956   |     2.15255   |      0        |      2        |      4        |      5        |     14      |\n",
            "| age               |  364563 |     65.1602    |    16.2756    |     18        |     54        |     67        |     78        |     90      |\n",
            "| gender_numeric    |  364563 |      0.442971  |     0.496738  |      0        |      0        |      0        |      1        |      1      |\n",
            "\n",
            "[2] RINGKASAN STATISTIK KOLOM BINARY/KATEGORIAL (Mode, Frekuensi):\n",
            "\n",
            "--- gender_numeric ---\n",
            "|   gender_numeric |   proportion |\n",
            "|-----------------:|-------------:|\n",
            "|                0 |         55.7 |\n",
            "|                1 |         44.3 |\n",
            "\n",
            "--- mortality_90d ---\n",
            "|   mortality_90d |   proportion |\n",
            "|----------------:|-------------:|\n",
            "|               0 |        73.69 |\n",
            "|               1 |        26.31 |\n",
            "\n",
            "--- is_readmit ---\n",
            "|   is_readmit |   proportion |\n",
            "|-------------:|-------------:|\n",
            "|            0 |        66.07 |\n",
            "|            1 |        33.93 |\n",
            "\n",
            "--- on_mechvent ---\n",
            "|   on_mechvent |   proportion |\n",
            "|--------------:|-------------:|\n",
            "|             1 |        56.64 |\n",
            "|             0 |        43.36 |\n",
            "\n",
            "[3] JUMLAH MISSING VALUES PER KOLOM:\n",
            "Tidak ada missing values (NaN) pada fitur-fitur yang diperiksa.\n",
            "\n",
            "Total Records dalam DataFrame: 364563\n",
            "Total Unique Patients: 18830\n"
          ]
        }
      ]
    }
  ]
}