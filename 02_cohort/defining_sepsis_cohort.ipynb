{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP9vjQeacWMxGj7D5b5ickD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatasfaps/duelingddqn-for-sepsis/blob/main/defining_sepsis_cohort.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 style=\"font-size:32px; font-weight:600; margin-bottom:12px;\">\n",
        "  <b>SEPSIS-3 COHORT DEFINITION</b>\n",
        "</h1>\n",
        "\n",
        "<p style=\"font-size:14px; line-height:1.6;\">\n",
        "  This notebook defines the patient cohort used for downstream modeling,\n",
        "  following the <b>Sepsis-3</b> clinical criteria.\n",
        "  <br><br>\n",
        "  The cohort construction is based on the methodology described in:\n",
        "  <br>\n",
        "  <a href=\"https://www.nature.com/articles/s41591-018-0213-5\"\n",
        "     target=\"_blank\"\n",
        "     style=\"color:#444; text-decoration:none;\">\n",
        "    Komorowski et al., Nature Medicine (2018)\n",
        "  </a>\n",
        "  and is adapted from the\n",
        "  <a href=\"https://github.com/matthieukomorowski/AI_Clinician\"\n",
        "     target=\"_blank\"\n",
        "     style=\"color:#444; text-decoration:none;\">\n",
        "    AI-Clinician GitHub repository.</a>\n",
        "</p>\n",
        "\n",
        "<p style=\"font-size:14px; line-height:1.6;\">\n",
        "  Cohort inclusion is determined using the following logic:\n",
        "</p>\n",
        "\n",
        "<ul style=\"font-size:14px; line-height:1.6;\">\n",
        "  <li>\n",
        "    <b>Suspected infection</b> is identified by the temporal co-occurrence\n",
        "    of antibiotic administration and microbiological culture sampling.\n",
        "  </li>\n",
        "  <li>\n",
        "    <b>Sepsis onset</b> is defined as the first time point at which suspected\n",
        "    infection is accompanied by an increase in <b>SOFA score ≥ 2</b> from baseline.\n",
        "  </li>\n",
        "  <li>\n",
        "    Only adult ICU stays are considered, and each patient contributes a single ICU admission.\n",
        "  </li>\n",
        "</ul>\n",
        "\n",
        "<p style=\"font-size:14px; line-height:1.6;\">\n",
        "  The resulting cohort represents ICU patients meeting Sepsis-3 criteria\n",
        "  and forms the basis for subsequent state space construction and MDP modeling.\n",
        "</p>\n",
        "\n",
        "<p style=\"font-size:13px; color:#555;\">\n",
        "  This notebook focuses on cohort definition only; feature extraction and model training\n",
        "  are performed in subsequent stages of the pipeline.\n",
        "</p>"
      ],
      "metadata": {
        "id": "p8tef5A_5TU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGu4uZbIlCY4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "import multiprocessing as mp\n",
        "from tqdm import tqdm\n",
        "from datetime import timedelta\n",
        "import itertools\n",
        "from itertools import tee\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from sklearn.impute import KNNImputer\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gomByQcalU9K",
        "outputId": "e6fb6811-ae79-4aab-e44a-3022a4a71d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/mimic3'"
      ],
      "metadata": {
        "id": "1zXL_aRklYWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()"
      ],
      "metadata": {
        "id": "M5hkjxAwlvLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"font-size:16px; font-weight:600;\">\n",
        "<b>Load Clinical Tables</b>\n",
        "</h3>\n",
        "\n",
        "<p style=\"font-size:14px;\">\n",
        "Load antibiotics, microbiology, demographics, vitals, and laboratory data.<br>\n",
        "All timestamps are converted to a unified datetime format.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Ar1zaqlesXTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all data tables\n",
        "abx = pd.read_csv('/content/drive/MyDrive/mimic3/abx.csv')\n",
        "culture = pd.read_csv('/content/drive/MyDrive/mimic3/culture.csv')\n",
        "microbio = pd.read_csv('/content/drive/MyDrive/mimic3/microbio.csv')\n",
        "demog = pd.read_csv('/content/drive/MyDrive/mimic3/demog.csv')\n",
        "all_vitals = pd.concat([\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce010.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce1020.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce2030.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce3040.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce4050.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce5060.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce6070.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce7080.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce8090.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/ce90100.csv'),\n",
        "], ignore_index=True)\n",
        "labU = pd.concat([\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/labs_ce.csv'),\n",
        "    pd.read_csv('/content/drive/MyDrive/mimic3/labs_le.csv')\n",
        "], ignore_index=True)\n",
        "MechVent = pd.read_csv('/content/drive/MyDrive/mimic3/mechvent.csv')\n",
        "inputpreadm = pd.read_csv('/content/drive/MyDrive/mimic3/preadm_fluid.csv')\n",
        "inputMV = pd.read_csv('/content/drive/MyDrive/mimic3/fluid_mv.csv')\n",
        "inputCV = pd.read_csv('/content/drive/MyDrive/mimic3/fluid_cv.csv')\n",
        "vasoMV = pd.read_csv('/content/drive/MyDrive/mimic3/vaso_mv.csv')\n",
        "vasoCV = pd.read_csv('/content/drive/MyDrive/mimic3/vaso_cv.csv')\n",
        "UOpreadm = pd.read_csv('/content/drive/MyDrive/mimic3/preadm_uo.csv')\n",
        "UO = pd.read_csv('/content/drive/MyDrive/mimic3/uo.csv')\n",
        "\n",
        "print(f\"Data imported in {time.time() - start_time:.1f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44calpgilfDP",
        "outputId": "4f6e1b78-fcc2-4947-b9b4-a4ea09e437f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data imported in 37.9 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all time columns to datetime type (with unit='s')\n",
        "demog['intime'] = pd.to_datetime(demog['intime'], unit='s', errors='coerce')\n",
        "demog['outtime'] = pd.to_datetime(demog['outtime'], unit='s', errors='coerce')\n",
        "demog['dod'] = pd.to_datetime(demog['dod'], unit='s', errors='coerce')\n",
        "abx['startdate'] = pd.to_datetime(abx['startdate'], unit='s', errors='coerce')\n",
        "abx['enddate'] = pd.to_datetime(abx['enddate'], unit='s', errors='coerce')\n",
        "microbio['charttime'] = pd.to_datetime(microbio['charttime'], unit='s', errors='coerce')\n",
        "culture['charttime'] = pd.to_datetime(culture['charttime'], unit='s', errors='coerce')\n",
        "labU['charttime'] = pd.to_datetime(labU['charttime'], unit='s', errors='coerce')\n",
        "MechVent['charttime'] = pd.to_datetime(MechVent['charttime'], unit='s', errors='coerce')\n",
        "all_vitals['charttime'] = pd.to_datetime(all_vitals['charttime'], unit='s', errors='coerce')\n",
        "inputMV['starttime'] = pd.to_datetime(inputMV['starttime'], unit='s', errors='coerce')\n",
        "inputCV['charttime'] = pd.to_datetime(inputCV['charttime'], unit='s', errors='coerce')\n",
        "vasoMV['starttime'] = pd.to_datetime(vasoMV['starttime'], unit='s', errors='coerce')\n",
        "vasoCV['charttime'] = pd.to_datetime(vasoCV['charttime'], unit='s', errors='coerce')\n",
        "UO['charttime'] = pd.to_datetime(UO['charttime'], unit='s', errors='coerce')\n",
        "\n",
        "print(f\"Conversion to datetime has finished in {time.time() - start_time:.1f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkAvXKS1lw_z",
        "outputId": "c79fc833-e83c-4ca0-8751-e87abe14f7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion to datetime has finished in 62.0 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"font-size:16px; font-weight:600;\">\n",
        "<b>ICU Stay Assignment</b>\n",
        "</h3>\n",
        "\n",
        "<p style=\"font-size:14px;\">\n",
        "Assign ICU stay identifiers and resolve missing ICU information.\n",
        "</p>"
      ],
      "metadata": {
        "id": "zlkuTfMaseuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge microbio and culture into bacterio\n",
        "bacterio = pd.concat([microbio, culture], ignore_index=True)\n",
        "\n",
        "# Handle missing icustay_id in bacterio\n",
        "bacterio_missing_icustay = bacterio[bacterio['icustay_id'].isnull()].copy()\n",
        "bacterio_has_icustay = bacterio[bacterio['icustay_id'].notnull()].copy()\n",
        "\n",
        "# Merge missing data with demog for finding the matches\n",
        "bacterio_missing_icustay = pd.merge(bacterio_missing_icustay.drop(columns='icustay_id'),\n",
        "                                     demog[['subject_id', 'hadm_id', 'icustay_id', 'intime', 'outtime']],\n",
        "                                     on=['subject_id', 'hadm_id'], how='left')\n",
        "\n",
        "# Implementing Komorowski's (2018)\n",
        "# 1. If charttime falls within the ICU intime and outtime range (+/- 48 hours)\n",
        "valid_match = (bacterio_missing_icustay['charttime'] >= bacterio_missing_icustay['intime'] - pd.Timedelta(hours=48)) & \\\n",
        "              (bacterio_missing_icustay['charttime'] <= bacterio_missing_icustay['outtime'] + pd.Timedelta(hours=48))\n",
        "\n",
        "bacterio_missing_icustay.loc[valid_match, 'filled_icustay_id'] = bacterio_missing_icustay.loc[valid_match, 'icustay_id']\n",
        "\n",
        "# 2. If there is only one hadm_id for the matching subject_id, assign that icustay_id\n",
        "grouped_by_hadm = bacterio_missing_icustay.groupby('hadm_id')['icustay_id'].nunique()\n",
        "single_match_hadm = grouped_by_hadm[grouped_by_hadm == 1].index\n",
        "bacterio_missing_icustay.loc[bacterio_missing_icustay['hadm_id'].isin(single_match_hadm), 'filled_icustay_id'] = bacterio_missing_icustay.loc[bacterio_missing_icustay['hadm_id'].isin(single_match_hadm), 'icustay_id']\n",
        "\n",
        "# Merge back the filled and complete data\n",
        "bacterio_missing_icustay['icustay_id'] = bacterio_missing_icustay['filled_icustay_id']\n",
        "bacterio = pd.concat([bacterio_has_icustay, bacterio_missing_icustay.drop(columns=['intime', 'outtime', 'filled_icustay_id'])], ignore_index=True)\n",
        "\n",
        "\n",
        "# Handle missing icustay_id in abx\n",
        "abx_missing_icustay = abx[abx['icustay_id'].isnull()].copy()\n",
        "abx_has_icustay = abx[abx['icustay_id'].notnull()].copy()\n",
        "\n",
        "abx_missing_icustay = pd.merge(abx_missing_icustay.drop(columns='icustay_id'),\n",
        "                                demog[['hadm_id', 'icustay_id', 'intime', 'outtime']],\n",
        "                                on='hadm_id', how='left')\n",
        "\n",
        "# Use 'startdate'\n",
        "valid_match_abx = (abx_missing_icustay['startdate'] >= abx_missing_icustay['intime'] - pd.Timedelta(hours=48)) & \\\n",
        "                  (abx_missing_icustay['startdate'] <= abx_missing_icustay['outtime'] + pd.Timedelta(hours=48))\n",
        "\n",
        "abx_missing_icustay.loc[valid_match_abx, 'filled_icustay_id'] = abx_missing_icustay.loc[valid_match_abx, 'icustay_id']\n",
        "\n",
        "grouped_by_hadm_abx = abx_missing_icustay.groupby('hadm_id')['icustay_id'].nunique()\n",
        "single_match_hadm_abx = grouped_by_hadm_abx[grouped_by_hadm_abx == 1].index\n",
        "abx_missing_icustay.loc[abx_missing_icustay['hadm_id'].isin(single_match_hadm_abx), 'filled_icustay_id'] = abx_missing_icustay.loc[abx_missing_icustay['hadm_id'].isin(single_match_hadm_abx), 'icustay_id']\n",
        "\n",
        "abx_missing_icustay['icustay_id'] = abx_missing_icustay['filled_icustay_id']\n",
        "abx = pd.concat([abx_has_icustay, abx_missing_icustay.drop(columns=['intime', 'outtime', 'filled_icustay_id'])], ignore_index=True)\n",
        "\n",
        "print(f\"Missing icustay_ids have been founded in {time.time() - start_time:.1f} seconds.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wbkTC_vmWNg",
        "outputId": "7ca6a4b7-3139-43d3-899c-906bfa815dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing icustay_ids have been founded in 68.2 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"font-size:16px; font-weight:600;\">\n",
        "<b>Suspected Infection</b>\n",
        "</h3>\n",
        "\n",
        "<p style=\"font-size:14px;\">\n",
        "Suspected infection is defined according to the <b>Sepsis-3</b> criteria,\n",
        "based on the temporal relationship between antibiotic administration\n",
        "and microbiological culture sampling following this logic:\n",
        "\n",
        "- If **antibiotic** administration **precedes culture** sampling within **24 hours**,\n",
        "  the infection onset time is defined as the **antibiotic administration time**.\n",
        "- If **culture** sampling **precedes antibiotic** administration **and antibiotics** are\n",
        "  administered within **72 hours**, the infection onset time is defined as the\n",
        "  **culture sampling time**.\n",
        "\n",
        "</p>"
      ],
      "metadata": {
        "id": "2dvLifv2ssEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the onset time of Sepsis\n",
        "def find_onset_for_patient(icustayid, abx_data, bacterio_data):\n",
        "    \"\"\"\n",
        "    Processes a single icustay_id to find the onset_time\n",
        "    Args:\n",
        "          icustayid_chunk (list): List of icustay_ids to be processed\n",
        "          abx_data (pd.DataFrame): Global ABx DataFrame\n",
        "          bacterio_data (pd.DataFrame): Global Bacterio DataFrame\n",
        "      Returns:\n",
        "          list: List of onset_time results\n",
        "    \"\"\"\n",
        "    # Filter ABx and Bacterio data for the current patient\n",
        "    ab_patient = abx_data[abx_data['icustay_id'] == icustayid].copy()\n",
        "    bact_patient = bacterio_data[bacterio_data['icustay_id'] == icustayid].copy()\n",
        "\n",
        "    # If either table has no data, return None\n",
        "    if ab_patient.empty or bact_patient.empty:\n",
        "        return None\n",
        "\n",
        "    # Perform merge only for this patient’s data\n",
        "    patient_events = pd.merge(ab_patient, bact_patient, on='icustay_id', suffixes=('_abx', '_bact'))\n",
        "\n",
        "    # Compute time differences\n",
        "    patient_events['diff_time'] = (patient_events['startdate'] - patient_events['charttime']).dt.total_seconds() / 3600\n",
        "\n",
        "    # Sort by absolute time differences\n",
        "    patient_events['abs_diff'] = patient_events['diff_time'].abs()\n",
        "    patient_events = patient_events.sort_values(by='abs_diff')\n",
        "\n",
        "    for _, row in patient_events.iterrows():\n",
        "        # Rule 1: Antibiotics (ABx) given first\n",
        "        if row['diff_time'] <= 0 and row['abs_diff'] <= 24:\n",
        "            return {'icustay_id': icustayid, 'onset_time': row['startdate']}\n",
        "        # Rule 2: Culture taken first\n",
        "        elif row['diff_time'] > 0 and row['diff_time'] <= 72:\n",
        "            return {'icustay_id': icustayid, 'onset_time': row['charttime']}\n",
        "\n",
        "    return None\n",
        "\n",
        "# Wrapper function to process one patient and update progress\n",
        "def process_and_update(icustayid, abx_data, bacterio_data, progress_counter):\n",
        "    result = find_onset_for_patient(icustayid, abx_data, bacterio_data)\n",
        "    progress_counter.value += 1\n",
        "    return result\n",
        "\n",
        "# --- Main Program ---\n",
        "if __name__ == '__main__':\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Remove rows with NaN values in primary keys before processing\n",
        "    abx = abx.dropna(subset=['icustay_id', 'startdate']).copy()\n",
        "    bacterio = bacterio.dropna(subset=['icustay_id', 'charttime']).copy()\n",
        "\n",
        "    # Get unique list of icustay_id from both tables\n",
        "    all_icustayids = pd.concat([abx['icustay_id'], bacterio['icustay_id']]).dropna().unique()\n",
        "\n",
        "    print(f\"Total unique icustay_ids to process: {len(all_icustayids)}\")\n",
        "\n",
        "    num_cores = mp.cpu_count()\n",
        "    print(f\"Using {num_cores} CPU cores for parallel processing.\")\n",
        "\n",
        "   # Use multiprocessing.Manager to create shareable objects\n",
        "    with mp.Manager() as manager:\n",
        "        progress_counter = manager.Value('i', 0) # 'i' for integer\n",
        "\n",
        "        # Prepare arguments for each process; each task processes ONE icustay_id\n",
        "        task_args = [(icustayid, abx, bacterio, progress_counter) for icustayid in all_icustayids]\n",
        "\n",
        "        with mp.Pool(processes=num_cores) as pool:\n",
        "            # Use starmap_async to run processes asynchronously\n",
        "            result_async = pool.starmap_async(process_and_update, task_args)\n",
        "\n",
        "            # Print progress periodically in the main process\n",
        "            while not result_async.ready():\n",
        "                time.sleep(5)  # Wait 5 seconds before checking again\n",
        "                progress = progress_counter.value\n",
        "                total = len(all_icustayids)\n",
        "                percentage = (progress / total) * 100 if total > 0 else 0\n",
        "                print(f\"Processing... {progress}/{total} ({percentage:.1f}%)\")\n",
        "\n",
        "            # Retrieve results after all processes have completed\n",
        "            results = result_async.get()\n",
        "\n",
        "    final_results = [res for res in results if res is not None]\n",
        "\n",
        "    onset = pd.DataFrame(final_results)\n",
        "\n",
        "    if not onset.empty:\n",
        "        onset['icustay_id'] = onset['icustay_id'].astype(int)\n",
        "        onset['onset_time'] = pd.to_datetime(onset['onset_time'])\n",
        "        onset = onset.drop_duplicates(subset=['icustay_id'], keep='first')\n",
        "\n",
        "    # Print first 50 rows for verification\n",
        "    print(f\"\\nNumber of records with onset_time found: {len(onset)}\")\n",
        "    print(f\"Total time taken: {time.time() - start_time:.1f} seconds\")\n",
        "    print(\"\\n--- Onset Time Findings (first 50) ---\")\n",
        "    print(onset[['icustay_id', 'onset_time']].head(50).to_markdown(index=False))\n",
        "\n",
        "    # Saving results\n",
        "    onset.to_csv('onset.csv', index=False)\n",
        "    onset.to_csv(os.path.join(data_dir, 'onset.csv'), index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIItJsOYmcaG",
        "outputId": "c8b44c03-9649-43c8-c87d-c7e6e2e9db24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique icustay_ids to process: 52953\n",
            "Using 8 CPU cores for parallel processing.\n",
            "Processing... 2053/52953 (3.9%)\n",
            "Processing... 6529/52953 (12.3%)\n",
            "Processing... 11739/52953 (22.2%)\n",
            "Processing... 17385/52953 (32.8%)\n",
            "Processing... 22363/52953 (42.2%)\n",
            "Processing... 29292/52953 (55.3%)\n",
            "Processing... 36719/52953 (69.3%)\n",
            "Processing... 37446/52953 (70.7%)\n",
            "\n",
            "Number of records with onset_time found: 26299\n",
            "Total time taken: 40.4 seconds\n",
            "\n",
            "--- Onset Time Findings (first 50) ---\n",
            "|   icustay_id | onset_time          |\n",
            "|-------------:|:--------------------|\n",
            "|       291788 | 2108-04-12 00:00:00 |\n",
            "|       253656 | 2162-05-16 21:10:00 |\n",
            "|       214619 | 2177-09-04 16:15:00 |\n",
            "|       239289 | 2177-03-15 00:00:00 |\n",
            "|       217590 | 2188-05-27 22:59:00 |\n",
            "|       252772 | 2109-08-22 00:00:00 |\n",
            "|       201668 | 2170-09-19 17:47:00 |\n",
            "|       222038 | 2185-04-17 12:15:00 |\n",
            "|       210325 | 2140-11-16 00:00:00 |\n",
            "|       200853 | 2194-07-06 00:00:00 |\n",
            "|       214267 | 2164-04-23 00:00:00 |\n",
            "|       245719 | 2115-02-27 15:50:00 |\n",
            "|       221136 | 2183-04-20 21:53:00 |\n",
            "|       270105 | 2183-03-23 15:40:00 |\n",
            "|       200580 | 2174-05-07 10:37:00 |\n",
            "|       289655 | 2195-08-12 00:00:00 |\n",
            "|       260971 | 2176-02-06 00:00:00 |\n",
            "|       279769 | 2179-09-22 00:00:00 |\n",
            "|       217069 | 2124-07-14 00:00:00 |\n",
            "|       201906 | 2178-12-26 00:00:00 |\n",
            "|       270174 | 2176-04-09 23:30:00 |\n",
            "|       296887 | 2106-10-01 15:40:00 |\n",
            "|       297065 | 2156-11-17 16:00:00 |\n",
            "|       220320 | 2108-09-29 17:13:00 |\n",
            "|       292080 | 2156-07-21 22:30:00 |\n",
            "|       216749 | 2108-04-23 00:00:00 |\n",
            "|       240176 | 2201-06-27 00:00:00 |\n",
            "|       254601 | 2201-06-21 23:56:00 |\n",
            "|       295421 | 2153-01-05 00:00:00 |\n",
            "|       291389 | 2157-09-30 11:30:00 |\n",
            "|       256633 | 2166-05-01 00:00:00 |\n",
            "|       202030 | 2198-09-08 10:11:00 |\n",
            "|       229201 | 2179-03-16 09:36:00 |\n",
            "|       282685 | 2196-09-28 15:29:00 |\n",
            "|       232928 | 2151-09-15 00:00:00 |\n",
            "|       276931 | 2179-09-22 00:00:00 |\n",
            "|       243747 | 2173-10-01 00:00:00 |\n",
            "|       241092 | 2171-08-17 22:54:00 |\n",
            "|       249287 | 2153-02-04 00:00:00 |\n",
            "|       289313 | 2103-05-20 18:30:00 |\n",
            "|       291381 | 2121-02-01 08:19:00 |\n",
            "|       260488 | 2135-12-27 00:00:00 |\n",
            "|       268122 | 2162-10-10 12:59:00 |\n",
            "|       286165 | 2107-01-22 00:00:00 |\n",
            "|       286268 | 2107-02-03 00:00:00 |\n",
            "|       233457 | 2146-11-17 19:30:00 |\n",
            "|       268883 | 2185-11-15 00:00:00 |\n",
            "|       295727 | 2195-10-10 00:00:00 |\n",
            "|       267595 | 2174-10-23 00:00:00 |\n",
            "|       274281 | 2115-10-08 20:03:00 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"font-size:16px; font-weight:600;\">\n",
        "<b>Base Cohort Assembly</b>\n",
        "</h3>\n",
        "\n",
        "<p style=\"font-size:14px;\">\n",
        "\n",
        "*   Static and Onset alignment\n",
        "*   Time-series filtering around Onset Time\n",
        "*   Feature mapping and reshaping\n",
        "\n",
        "</p>"
      ],
      "metadata": {
        "id": "atxd_TeYJCp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## 1. COMBINE BASE DATA\n",
        "# ----------------------------------------------------------------------\n",
        "# Use 'demog' and 'onset' from previous cells\n",
        "combined_df = pd.merge(onset, demog, on=['icustay_id'], how='left')\n",
        "combined_df['onset_time'] = pd.to_datetime(combined_df['onset_time'])\n",
        "combined_df = combined_df[['icustay_id', 'onset_time',\n",
        "                           'gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los']]\n",
        "print(f\"[DEBUG] Patients in combined_df (based on onset valid): {combined_df['icustay_id'].nunique()}\")\n",
        "# combined_filter_cols (pasien 20.115)\n",
        "combined_filter_cols = combined_df[['icustay_id', 'onset_time']].copy()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 2. FILTER AND MERGE DATA\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"Filtering and merging data...\")\n",
        "\n",
        "# Take icustay_id and onset_time for time-series filtering\n",
        "combined_filter_cols = combined_df[['icustay_id', 'onset_time']].copy()\n",
        "\n",
        "# Filter large tables first using inner merge\n",
        "vitals_filtered = pd.merge(all_vitals, combined_filter_cols, on='icustay_id', how='inner')\n",
        "lab_filtered = pd.merge(labU, combined_filter_cols, on='icustay_id', how='inner')\n",
        "\n",
        "# Drop all_vitals and labU, which are large, since they’ve already been filtered\n",
        "del all_vitals, labU\n",
        "gc.collect()\n",
        "\n",
        "# Standardize column names of value columns before concatenation\n",
        "if 'value' in lab_filtered.columns:\n",
        "    lab_filtered = lab_filtered.rename(columns={'value': 'valuenum'})\n",
        "elif 'valuenum' not in lab_filtered.columns:\n",
        "    print(\"WARNING: Laboratory value column not found (neither 'value' nor 'valuenum').\")\n",
        "\n",
        "# Now, merge the filtered data\n",
        "all_data_filtered = pd.concat([vitals_filtered, lab_filtered], ignore_index=True)\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del vitals_filtered, lab_filtered\n",
        "gc.collect()\n",
        "\n",
        "# Perform time-based filtering and deduplication on the smaller dataset\n",
        "all_data_filtered['charttime'] = pd.to_datetime(all_data_filtered['charttime'])\n",
        "all_data_filtered = all_data_filtered.drop_duplicates(subset=['icustay_id', 'charttime', 'itemid'], keep='first')\n",
        "all_data_filtered['time_diff_hours'] = (all_data_filtered['charttime'] - all_data_filtered['onset_time']).dt.total_seconds() / 3600\n",
        "\n",
        "# Create the final fully filtered DataFrame for 80 hrs\n",
        "filtered_df = all_data_filtered[(all_data_filtered['time_diff_hours'] >= -48) & (all_data_filtered['time_diff_hours'] <= 60)].copy()\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del all_data_filtered\n",
        "gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 3. MAPPING AND RESHAPING\n",
        "# ----------------------------------------------------------------------\n",
        "# Utilize BigQuery and MIMIC-3 Demo Ver. to check itemid labels\n",
        "column_name_map = {\n",
        "    # 1. Demographics and Scales\n",
        "    580: 'Weight_kg', 581: 'Weight_kg', 224639: 'Weight_kg', 226512: 'Weight_kg',\n",
        "    226707: 'Height_cm', 226730: 'Height_cm',\n",
        "    228096: 'RASS',\n",
        "\n",
        "    # 2. Vital Signs\n",
        "    211: 'HR', 220045: 'HR',\n",
        "    6: 'SysBP', 51: 'SysBP', 455: 'SysBP', 220179: 'SysBP', 225309: 'SysBP', 6701: 'SysBP', 224167: 'SysBP', 227243: 'SysBP',\n",
        "    52: 'MeanBP', 443: 'MeanBP', 456: 'MeanBP', 6702: 'MeanBP', 220052: 'MeanBP', 220181: 'MeanBP', 224322: 'MeanBP', 225312: 'MeanBP',\n",
        "    8368: 'DiaBP', 8440: 'DiaBP', 8441: 'DiaBP', 8555: 'DiaBP', 225310: 'DiaBP',\n",
        "    615: 'RR', 618: 'RR', 3337: 'RR', 3603: 'RR', 220210: 'RR', 224422: 'RR',\n",
        "    678: 'Temp_C', 3655: 'Temp_C', 223761: 'Temp_C', 223762: 'Temp_C',\n",
        "    646: 'SpO2', 220228: 'SpO2', 220277: 'SpO2', 834: 'SpO2',\n",
        "    190: 'FiO2', 3420: 'FiO2', 223835: 'FiO2', 727: 'FiO2',\n",
        "    535: 'Peak_Ins_Pressure', 224695: 'Peak_Ins_Pressure',\n",
        "    543: 'Plateau_Pressure', 224696: 'Plateau_Pressure',\n",
        "    444: 'Mean_Airway_Pressure', 224697: 'Mean_Airway_Pressure',\n",
        "    445: 'Minute_Volume', 448: 'Minute_Volume', 450: 'Minute_Volume', 224687: 'Minute_Volume',\n",
        "    654: 'Tidal_Volume', 681: 'Tidal_Volume', 684: 'Tidal_Volume', 96: 'Tidal_Volume', 97: 'Tidal_Volume', 98: 'Tidal_Volume', 2566: 'Tidal_Volume', 3050: 'Tidal_Volume', 3083: 'Tidal_Volume', 224421: 'Tidal_Volume', 224684: 'Tidal_Volume', 224686: 'Tidal_Volume',\n",
        "    113: 'CVP', 220074: 'CVP',\n",
        "    116: 'Cardiac_Index', 228177: 'Cardiac_Index', 228368: 'Cardiac_Index', 1366: 'Cardiac_Index', 1372: 'Cardiac_Index',\n",
        "    491: 'PAP', 492: 'PAP', 8448: 'PAP',\n",
        "\n",
        "    # 3. Lab Values\n",
        "    829: 'Potassium', 1535: 'Potassium', 3725: 'Potassium', 3792: 'Potassium', 4194: 'Potassium', 227442: 'Potassium', 227464: 'Potassium', 50971: 'Potassium', 50822: 'Potassium',\n",
        "    837: 'Sodium', 1536: 'Sodium', 3726: 'Sodium', 3803: 'Sodium', 4195: 'Sodium', 220645: 'Sodium', 226534: 'Sodium', 50983: 'Sodium', 50824: 'Sodium',\n",
        "    788: 'Chloride', 1523: 'Chloride', 3724: 'Chloride', 3747: 'Chloride', 4193: 'Chloride', 220602: 'Chloride', 226536: 'Chloride', 50902: 'Chloride', 50806: 'Chloride',\n",
        "    807: 'Glucose', 811: 'Glucose', 1529: 'Glucose', 3744: 'Glucose', 220621: 'Glucose', 225664: 'Glucose', 226537: 'Glucose', 50931: 'Glucose', 50809: 'Glucose',\n",
        "    821: 'Magnesium', 1532: 'Magnesium', 220635: 'Magnesium', 50960: 'Magnesium',\n",
        "    786: 'Calcium', 1522: 'Calcium', 112: 'Calcium', 3746: 'Calcium', 3766: 'Calcium', 225625: 'Calcium', 50808: 'Calcium', 50893: 'Calcium',\n",
        "    816: 'Ionized Calcium', 225667: 'Ionized Calcium',\n",
        "    814: 'Hb', 220228: 'Hb', 51222: 'Hb', 50811: 'Hb',\n",
        "    1127: 'WBC_Count', 1542: 'WBC_Count', 3834: 'WBC_Count', 4200: 'WBC_Count', 220546: 'WBC_Count', 51301: 'WBC_Count', 51300: 'WBC_Count',\n",
        "    828: 'Platelets_count', 3789: 'Platelets_count', 227457: 'Platelets_count', 51265: 'Platelets_count',\n",
        "    825: 'PTT', 1533: 'PTT', 3796: 'PTT', 227466: 'PTT', 51275: 'PTT',\n",
        "    824: 'PT', 1286: 'PT', 51274: 'PT',\n",
        "    780: 'Arterial_pH', 1126: 'Arterial_pH', 132: 'Arterial_pH', 139: 'Arterial_pH', 3839: 'Arterial_pH', 4753: 'Arterial_pH', 50820: 'Arterial_pH',\n",
        "    490: 'paO2', 3785: 'paO2', 3837: 'paO2', 3838: 'paO2', 50821: 'paO2', 779: 'paO2',\n",
        "    3784: 'paCO2', 3835: 'paCO2', 3836: 'paCO2', 778: 'paCO2',\n",
        "    74: 'Arterial_BE', 776: 'Arterial_BE', 108: 'Arterial_BE', 110: 'Arterial_BE', 136: 'Arterial_BE', 3736: 'Arterial_BE', 3740: 'Arterial_BE', 4196: 'Arterial_BE', 224828: 'Arterial_BE', 50802: 'Arterial_BE',\n",
        "    51: 'HCO3', 777: 'HCO3', 787: 'HCO3', 227443: 'HCO3', 50803: 'HCO3', 50804: 'HCO3',\n",
        "    86: 'Lactate', 1531: 'Lactate', 225668: 'Lactate', 50813: 'Lactate', 50882: 'Lactate',\n",
        "    781: 'BUN', 1162: 'BUN', 3737: 'BUN', 225624: 'BUN',\n",
        "    791: 'Creatinine', 1525: 'Creatinine', 114: 'Creatinine', 3750: 'Creatinine', 220615: 'Creatinine', 50912: 'Creatinine', 51081: 'Creatinine',\n",
        "    769: 'SGPT(ALT)', 3802: 'SGPT(ALT)', 50861: 'SGPT(ALT)',\n",
        "    770: 'SGOT(AST)', 3801: 'SGOT(AST)', 50878: 'SGOT(AST)',\n",
        "    848: 'Total_bili', 1538: 'Total_bili', 225690: 'Total_bili', 51464: 'Total_bili',\n",
        "    815: 'INR', 1530: 'INR', 227467: 'INR', 51237: 'INR',\n",
        "    813: 'HCT', 115: 'HCT', 3761: 'HCT', 220545: 'HCT', 226540: 'HCT', 51221: 'HCT', 50810: 'HCT',\n",
        "    803: 'Direct_Bili', 1527: 'Direct_Bili', 225651: 'Direct_Bili', 50883: 'Direct_Bili',\n",
        "    772: 'Albumin', 1521: 'Albumin', 107: 'Albumin', 3727: 'Albumin', 227456: 'Albumin', 50862: 'Albumin',\n",
        "    851: 'Troponin', 227429: 'Troponin', 51003: 'Troponin', 51002: 'Troponin',\n",
        "\n",
        "    # 4. Oxygenation\n",
        "    823: 'Mixed_Venous_O2', 227686: 'Mixed_Venous_O2',\n",
        "\n",
        "    # 5. Coagulation\n",
        "    768: 'ACT', 1520: 'ACT', 1671: 'ACT', 220507: 'ACT',\n",
        "\n",
        "    # 6. Others\n",
        "    626: 'SVR',\n",
        "    198: 'GCS',\n",
        "    1817: 'ETCO2',\n",
        "    470: 'O2_Flow_Main', 471: 'O2_Flow_Main', 223834: 'O2_Flow_Main',\n",
        "    467: 'O2_Delivery_Device',\n",
        "    227287: 'O2_Flow_Add',\n",
        "    160: 'Ectopy_Frequency',\n",
        "    849: 'Total_Protein', 1539: 'Total_Protein', 126: 'Total_Protein', 3807: 'Total_Protein', 50976: 'Total_Protein',\n",
        "    227444: 'CRP', 50889: 'CRP',\n",
        "    3799: 'RBC', 4197: 'RBC', 51279: 'RBC',\n",
        "    224691: 'Flow_Rate'\n",
        "}\n",
        "\n",
        "# Create mapping dictionary with default itemid\n",
        "itemid_map_with_default = {k: v for k, v in column_name_map.items()}\n",
        "unique_itemids = filtered_df['itemid'].unique()\n",
        "for itemid in unique_itemids:\n",
        "    if itemid not in itemid_map_with_default:\n",
        "        itemid_map_with_default[itemid] = f'ITEMID_{itemid}'\n",
        "\n",
        "# Apply mapping to filtered_df\n",
        "unique_itemids = filtered_df['itemid'].unique()\n",
        "for itemid in unique_itemids:\n",
        "    if itemid not in itemid_map_with_default:\n",
        "        itemid_map_with_default[itemid] = f'ITEMID_{itemid}'\n",
        "filtered_df['itemid_name'] = filtered_df['itemid'].map(itemid_map_with_default)\n",
        "\n",
        "# Isolate the required time-series data columns\n",
        "pivot_cols_base = ['icustay_id', 'charttime', 'onset_time', 'itemid_name', 'valuenum']\n",
        "\n",
        "# Extract necessary columns from filtered_df for pivot operation\n",
        "df_time_series_base = filtered_df[pivot_cols_base].copy()\n",
        "\n",
        "# Delete unused filtered_df\n",
        "del filtered_df\n",
        "gc.collect()\n",
        "\n",
        "# Merge static data into df_time_series_base\n",
        "static_cols_needed = combined_df.copy()\n",
        "\n",
        "# Merge all static columns + onset_time (which may be duplicated; handled below)\n",
        "df_time_series_only = pd.merge(df_time_series_base, static_cols_needed, on='icustay_id', how='left', suffixes=('_base', '_static'))\n",
        "\n",
        "# Handle duplicate 'onset_time' columns that may be created by Pandas\n",
        "if 'onset_time_base' in df_time_series_only.columns and 'onset_time_static' in df_time_series_only.columns:\n",
        "    # Remove onset_time from the static data, keep the one from time-series (already filtered in Part 2)\n",
        "    df_time_series_only = df_time_series_only.drop(columns=['onset_time_static'])\n",
        "    df_time_series_only = df_time_series_only.rename(columns={'onset_time_base': 'onset_time'})\n",
        "elif 'onset_time_static' in df_time_series_only.columns:\n",
        "    # If there are only suffixes, remove the suffixes\n",
        "    df_time_series_only = df_time_series_only.rename(columns={'onset_time_static': 'onset_time'})\n",
        "\n",
        "# Hapus kolom statis ganda lainnya yang mungkin tidak sengaja terbawa dengan suffix\n",
        "cols_to_drop = [col for col in df_time_series_only.columns if col.endswith('_static')]\n",
        "df_time_series_only = df_time_series_only.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del df_time_series_base\n",
        "gc.collect()\n",
        "\n",
        "# Perform pivot table operation (ONLY on data containing time-series)\n",
        "reformat_valid = pd.pivot_table(\n",
        "    df_time_series_only,\n",
        "    index=['icustay_id', 'onset_time', 'charttime',\n",
        "           'gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los'],\n",
        "    columns='itemid_name',\n",
        "    values='valuenum',\n",
        "    aggfunc='mean'\n",
        ").reset_index()\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del df_time_series_only\n",
        "gc.collect()\n",
        "\n",
        "# Identify missing patients\n",
        "all_patient_ids = combined_df['icustay_id'].unique()\n",
        "existing_ids = reformat_valid['icustay_id'].unique()\n",
        "missing_ids = np.setdiff1d(all_patient_ids, existing_ids)\n",
        "print(f\"[DEBUG] Missing patients (static data only): {len(missing_ids)}\")\n",
        "\n",
        "# Create a DataFrame for missing patients (Static Data Only)\n",
        "missing_patients_df = combined_df[combined_df['icustay_id'].isin(missing_ids)].copy()\n",
        "\n",
        "# Generate dummy rows with the same columns as reformat_valid, but all time-series features as NaN\n",
        "# Retrieve the list of time-series columns from reformat_valid\n",
        "time_series_cols = [col for col in reformat_valid.columns if col not in combined_df.columns]\n",
        "\n",
        "# Add time-series columns with NaN values\n",
        "for col in time_series_cols:\n",
        "    missing_patients_df[col] = np.nan\n",
        "\n",
        "# Ensure missing_patients_df has charttime = NaT (for consistency)\n",
        "missing_patients_df['charttime'] = pd.NaT\n",
        "missing_patients_df = missing_patients_df[reformat_valid.columns]\n",
        "# Combine all data back together\n",
        "reformat = pd.concat([reformat_valid, missing_patients_df], ignore_index=True)\n",
        "\n",
        "# Delete intermediate DataFrames\n",
        "del reformat_valid, missing_patients_df\n",
        "gc.collect()\n",
        "\n",
        "# Saving results\n",
        "reformat.to_csv(os.path.join(data_dir, 'reformat_pivot.csv'), index=False)\n",
        "print(f\"DataFrame 'reformat_pivot.csv' has been made in {time.time() - start_time:.1f} seconds.\")\n",
        "print(f\"Total unique icustay_ids: {reformat['icustay_id'].nunique()}\")\n",
        "\n",
        "# Delete all large variables\n",
        "del itemid_map_with_default, unique_itemids, column_name_map\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "dPIesmMensoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48ba1c9-f0a1-4dcb-a45e-006fd47fcf38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Patients in combined_df (based on onset valid): 26299\n",
            "Filtering and merging data...\n",
            "[DEBUG] Missing patients (static data only): 16425\n",
            "DataFrame 'reformat_pivot.csv' has been made in 108.8 seconds.\n",
            "Total unique icustay_ids: 26299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"font-size:16px; font-weight:600;\">\n",
        "<b>P95 Capping </b>\n",
        "\n",
        "Cap some continuous variables at the 95th percentile to reduce the influence\n",
        "of extreme values while preserving distributional structure.</b>\n",
        "</h3>"
      ],
      "metadata": {
        "id": "ccqnX6B7Kf8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## P95 CAPPING FUNCTION\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "def cap_p95_upper(df, col_names):\n",
        "    \"\"\"\n",
        "    Applies P95 upper limit capping to specified columns.\n",
        "    Values > P95 are replaced by the P95 value.\n",
        "    \"\"\"\n",
        "    print(f\"Applying P95 upper capping to: {col_names}\")\n",
        "\n",
        "    for col in col_names:\n",
        "        if col in df.columns:\n",
        "            # Calculate value P95\n",
        "            p95_value = df[col].quantile(0.95)\n",
        "\n",
        "            # Apply capping\n",
        "            df.loc[df[col] > p95_value, col] = p95_value\n",
        "\n",
        "            print(f\"  - {col}: Max value capped at {p95_value:.2f}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "features_to_cap = ['SGOT(AST)', 'SGPT(ALT)', 'BUN']"
      ],
      "metadata": {
        "id": "etAG0OTNK9Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3 style=\"font-size:16px; font-weight:600;\">\n",
        "<b>Outlier Handling</b>\n",
        "\n",
        "</h3>"
      ],
      "metadata": {
        "id": "QWzOGefgLEmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## OUTLIER CLEANING\n",
        "# ----------------------------------------------------------------------\n",
        "# --- Main Program ---\n",
        "def delout(df, col_name, threshold, direction):\n",
        "    \"\"\"\n",
        "    Replace values outside the threshold with NaN.\n",
        "    The 'direction' parameter can be either 'above' or 'below'.\n",
        "    \"\"\"\n",
        "    if col_name not in df.columns:\n",
        "        return df\n",
        "\n",
        "    if direction == 'above':\n",
        "        df.loc[df[col_name] > threshold, col_name] = np.nan\n",
        "    elif direction == 'below':\n",
        "        df.loc[df[col_name] < threshold, col_name] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "# Clinical Outlier Rules (Komorowski (2018) & MIMIC-Extract)\n",
        "outlier_rules_revised = {\n",
        "    # 1. Demographics and Scales\n",
        "    'Weight_kg': [(0, 'below'), (250, 'above')],\n",
        "    'Height_cm': [(0, 'below'), (240, 'above')],\n",
        "    'GCS': [(3, 'below'), (15, 'above')],\n",
        "\n",
        "    # 2. Vital Signs\n",
        "    'HR': [(20, 'below'), (350, 'above')],\n",
        "    'SysBP': [(10, 'below'), (375, 'above')],\n",
        "    'DiaBP': [(10, 'below'), (375, 'above')],\n",
        "    'MeanBP': [(14, 'below'), (330, 'above')],\n",
        "    'RR': [(5, 'below'), (100, 'above')],\n",
        "    'SpO2': [(70, 'below'), (100, 'above')],\n",
        "    'Temp_C': [(26, 'below'), (45, 'above')],\n",
        "    'CVP': [(-10, 'below'), (50, 'above')],\n",
        "\n",
        "    # 3. Lab Values\n",
        "    'PEEP_Set': [(0, 'below'), (25, 'above')],\n",
        "    'paO2': [(32, 'below'), (700, 'above')],\n",
        "    'paCO2': [(10, 'below'), (200, 'above')],\n",
        "    'Arterial_pH': [(6.3, 'below'), (8.4, 'above')],\n",
        "    'Lactate': [(0.4, 'below'), (100, 'above')],\n",
        "    'Arterial_BE': [(-50, 'below'), (50, 'above')],\n",
        "    'Tidal_Volume': [(50, 'below'), (1800, 'above')],\n",
        "    'Minute_Volume': [(0.1, 'below'), (50, 'above')],\n",
        "    'Flow_Rate': [(0, 'below'), (70, 'above')],\n",
        "\n",
        "    # b. Metabolic\n",
        "    'Potassium': [(2, 'below'), (12, 'above')],\n",
        "    'Sodium': [(50, 'below'), (225, 'above')],\n",
        "    'Chloride': [(50, 'below'), (175, 'above')],\n",
        "    'Glucose': [(33, 'below'), (2000, 'above')],\n",
        "    'Creatinine': [(0.1, 'below'), (60, 'above')],\n",
        "    'Magnesium': [(0, 'below'), (20, 'above')],\n",
        "    'Calcium': [(0, 'below'), (20, 'above')],\n",
        "    'Ionized Calcium': [(0, 'below'), (5, 'above')],\n",
        "    'BUN': [(0, 'below'), (150, 'above')],\n",
        "    'HCO3': [(0, 'below'), (50, 'above')],\n",
        "\n",
        "    # c. Liver function\n",
        "    'SGOT(AST)': [(6, 'below'), (200, 'above')],\n",
        "    'SGPT(ALT)': [(2, 'below'), (150, 'above')],\n",
        "    'PT': [(9.9, 'below'), (97.1, 'above')],\n",
        "    'INR': [(0.8, 'below'), (15, 'above')],\n",
        "    'PTT': [(10, 'below'), (150, 'above')],\n",
        "\n",
        "    # d. Hematology\n",
        "    'Hb': [(2, 'below'), (25, 'above')],\n",
        "    'HCT': [(5, 'below'), (75, 'above')],\n",
        "    'WBC_Count': [(0, 'below'), (1000, 'above')],\n",
        "    'Platelets_count': [(10, 'below'), (2000, 'above')],\n",
        "\n",
        "    # 6. Fluid Balance\n",
        "    'input_total': [(0, 'below'), (300000, 'above')],\n",
        "    'uo_total': [(0, 'below'), (300000, 'above')],\n",
        "    'cumulated_balance': [(-100000, 'below'), (300000, 'above')]\n",
        "}\n",
        "\n",
        "\n",
        "# IQR Function\n",
        "def iqr_outlier_removal_robust(df, factor=3.0):\n",
        "    \"\"\"Replace statistical outliers (IQR 3.0) with NaN and ensure non-negative values.\"\"\"\n",
        "\n",
        "    cols_to_exclude = ['icustay_id', 'charttime', 'onset_time', 'timestep_id', 'Total_bili', 'Direct_Bili',\n",
        "                       'gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los', 'Temp_F']\n",
        "\n",
        "    NON_NEGATIVE_COLS = list(outlier_rules_revised.keys())\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Ensure icustay_id is never included in cols_to_clean\n",
        "    cols_to_clean = [col for col in numeric_cols if col not in cols_to_exclude]\n",
        "\n",
        "    for col in cols_to_clean:\n",
        "        if df[col].isnull().all() or df[col].nunique() < 2: continue\n",
        "\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - factor * IQR\n",
        "        upper_bound = Q3 + factor * IQR\n",
        "\n",
        "        if IQR > 0 and df[col].std() > 0.001:\n",
        "            # 1. Apply Upper IQR Limit\n",
        "            df.loc[df[col] > upper_bound, col] = np.nan\n",
        "            # 2. Apply Lower IQR Limit (with non-negative protection/floor capping)\n",
        "            current_lower_bound = lower_bound\n",
        "\n",
        "            if col in NON_NEGATIVE_COLS and lower_bound < 0:\n",
        "                current_lower_bound = 0\n",
        "            df.loc[df[col] < current_lower_bound, col] = np.nan\n",
        "\n",
        "    return df\n",
        "\n",
        "# Outlier Cleaning Execution\n",
        "available_features = reformat.columns.tolist()\n",
        "\n",
        "print(\"Outlier cleaning starts...\")\n",
        "\n",
        "# 1. Create a Copy of the Data\n",
        "reformat_cleaned = reformat.copy()\n",
        "\n",
        "# 2. Standardize FiO2\n",
        "if 'FiO2' in available_features:\n",
        "    print(\"Standardizing FiO2 (converting % to fraction and capping)...\")\n",
        "\n",
        "    # a. Convert percentage values (>1.0 up to 100) into fractions\n",
        "    mask_persen = (reformat_cleaned['FiO2'] > 1.0) & (reformat_cleaned['FiO2'] <= 100.0)\n",
        "    reformat_cleaned.loc[mask_persen, 'FiO2'] /= 100\n",
        "\n",
        "    # b. Cap maximum FiO2 at 1.0\n",
        "    reformat_cleaned.loc[reformat_cleaned['FiO2'] > 1.0, 'FiO2'] = 1.0\n",
        "\n",
        "    # c. Cap minimum FiO2 at 0.21\n",
        "    reformat_cleaned.loc[reformat_cleaned['FiO2'] < 0.21, 'FiO2'] = 0.21\n",
        "\n",
        "\n",
        "# 3. Iterate and Clean Outliers based on Clinical Rules (Hard Capping)\n",
        "print(\"Applying improved clinical outlier rules (hard capping)...\")\n",
        "for col, rules in outlier_rules_revised.items():\n",
        "    if col in available_features:\n",
        "        for threshold, direction in rules:\n",
        "            reformat_cleaned = delout(reformat_cleaned, col, threshold, direction)\n",
        "\n",
        "# Apply P95 Capping\n",
        "reformat_cleaned = cap_p95_upper(reformat_cleaned, features_to_cap)\n",
        "\n",
        "# 4. Remove Statistical Outliers (IQR)\n",
        "print(\"Applying IQR (factor 3.0) outlier removal and floor capping...\")\n",
        "reformat_cleaned = iqr_outlier_removal_robust(reformat_cleaned, factor=1.5)\n",
        "\n",
        "# Saving results\n",
        "try:\n",
        "    final_unique_icustayids = reformat_cleaned['icustay_id'].nunique()\n",
        "    reformat_cleaned.to_csv(os.path.join(data_dir, 'reformat_cleaned.csv'), index=False)\n",
        "\n",
        "    try:\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print(f\"\\nOutlier clearance complete. 'reformat_cleaned.csv' saved in {time_elapsed:.1f} seconds.\")\n",
        "    except NameError:\n",
        "        print(\"\\nOutlier clearance complete. 'reformat_cleaned.csv' saved.\")\n",
        "\n",
        "    print(f\"Total unique icustay_ids: {final_unique_icustayids}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nWarning: 'data_dir' or 'start_time' is not defined. Skipping time logging and saving.\")\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del outlier_rules_revised, available_features\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f3lMNbbvBEk",
        "outputId": "8b922e12-ab19-46bb-d8d0-4f5e5e9da00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlier cleaning starts...\n",
            "Standardizing FiO2 (converting % to fraction and capping)...\n",
            "Applying improved clinical outlier rules (hard capping)...\n",
            "Applying P95 upper capping to: ['SGOT(AST)', 'SGPT(ALT)', 'BUN']\n",
            "  - SGOT(AST): Max value capped at 158.00\n",
            "  - SGPT(ALT): Max value capped at 116.00\n",
            "  - BUN: Max value capped at 88.00\n",
            "Applying IQR (factor 3.0) outlier removal and floor capping...\n",
            "\n",
            "Outlier clearance complete. 'reformat_cleaned.csv' saved in 145.7 seconds.\n",
            "Total unique icustay_ids: 26299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global SAH Hold Limits\n",
        "# Create the SAH function according to:\n",
        "# \"Detecting Hazardous Intensive Care PatientEpisodes Using Real-time Mortality Models\" (Hug, 2006)\n",
        "\n",
        "HOLD_LIMITS_SEC = {\n",
        "    # 1. Demographics and Scales\n",
        "    'GCS': 28 * 3600,             # From Table 3.1\n",
        "    'Weight_kg': 28 * 3600,       # Taken from Weight (581) in Table 3.1\n",
        "    'Height_cm': 28 * 3600,       # Considered constant, use 28 hours\n",
        "    'RASS': 5 * 3600,             # RASS/Sedation changes frequently, use the second shortest limit (5 hours)\n",
        "\n",
        "    # 2. Vital Signs (Usually 4 hours)\n",
        "    'HR': 4 * 3600,               # From Table 3.1\n",
        "    'RR': 4 * 3600,               # Resp Rate (RESP) in Table 3.1 is 4 hours\n",
        "    'SysBP': 4 * 3600,            # SBP in Table 3.1 is 4 hours\n",
        "    'DiaBP': 4 * 3600,            # DBP in Table 3.1 is 4 hours\n",
        "    'MeanBP': 4 * 3600,           # MAP in Table 3.1 is 4 hours\n",
        "    'SpO2': 4 * 3600,             # From Table 3.1\n",
        "    'CVP': 4 * 3600,              # From Table 3.1\n",
        "    'Temp_C': 28 * 3600,          # Temp (678) in Table 3.1 is 28 hours\n",
        "    'FiO2': 28 * 3600,            # Taken from FiO2Set in Table 3.1\n",
        "    'Peak_Ins_Pressure': 28 * 3600, # Taken from PIP in Table 3.1\n",
        "    'Plateau_Pressure': 28 * 3600, # Taken from PlateauPres in Table 3.1\n",
        "    'Mean_Airway_Pressure': 28 * 3600, # Assumed equivalent to PlateauPres (28 hours)\n",
        "    'Minute_Volume': 28 * 3600,   # Not listed in table, use 28 hours (default)\n",
        "    'Tidal_Volume': 28 * 3600,    # Taken from TidVolObs/Set/Spon in Table 3.1\n",
        "    'PAP': 4 * 3600,              # PAPMean in Table 3.1 is 4 hours\n",
        "    'Cardiac_Index': 10 * 3600,   # From Table 3.1 (10 hours)\n",
        "    'SVR': 10 * 3600,             # From Table 3.1 (10 hours)\n",
        "\n",
        "    # 3. a. Lab Values & Hematology (Generally 28 hours)\n",
        "    'Potassium': 28 * 3600,       # From Table 3.1\n",
        "    'Sodium': 28 * 3600,          # From Table 3.1\n",
        "    'Chloride': 28 * 3600,        # From Table 3.1\n",
        "    'Glucose': 28 * 3600,         # From Table 3.1\n",
        "    'Magnesium': 28 * 3600,       # From Table 3.1\n",
        "    'Calcium': 28 * 3600,         # From Table 3.1\n",
        "    'Ionized Calcium': 28 * 3600, # From Table 3.1 (IonCa)\n",
        "    'BUN': 28 * 3600,             # From Table 3.1\n",
        "    'Creatinine': 28 * 3600,      # From Table 3.1\n",
        "    'SGOT(AST)': 28 * 3600,       # AST in Table 3.1\n",
        "    'SGPT(ALT)': 28 * 3600,       # ALT in Table 3.1\n",
        "    'Total_bili': 28 * 3600,      # Total Bilirubin in Table 3.1\n",
        "    'Direct_Bili': 28 * 3600,     # Direct Bilirubin in Table 3.1\n",
        "    'Total_Protein': 28 * 3600,   # From Table 3.1\n",
        "    'Albumin': 28 * 3600,         # From Table 3.1\n",
        "    'Lactate': 28 * 3600,         # From Table 3.1\n",
        "    'HCT': 28 * 3600,             # Hematocrit in Table 3.1\n",
        "    'Hb': 28 * 3600,              # Hemoglobin in Table 3.1\n",
        "    'WBC_Count': 28 * 3600,       # From Table 3.1\n",
        "    'Platelets_count': 28 * 3600, # Platelets in Table 3.1\n",
        "    'INR': 28 * 3600,             # From Table 3.1\n",
        "    'PTT': 28 * 3600,             # From Table 3.1\n",
        "    'PT': 28 * 3600,              # From Table 3.1\n",
        "    'RBC': 28 * 3600,             # RBC Count in Table 3.1\n",
        "\n",
        "    # 3. b. ABG (28 hours)\n",
        "    'Arterial_pH': 28 * 3600,     # Art pH in Table 3.1\n",
        "    'paCO2': 28 * 3600,           # Art PaCO2 in Table 3.1\n",
        "    'paO2': 28 * 3600,            # Art PaO2 in Table 3.1\n",
        "    'Arterial_BE': 28 * 3600,     # Art Base Excess in Table 3.1\n",
        "    'HCO3': 28 * 3600,            # Art CO2 in Table 3.1\n",
        "\n",
        "    # 6. Variables Not Found in Table 3.1 (Use 28 hours, except RASS/Flow)\n",
        "    'Ionized Calcium': 28 * 3600,\n",
        "    'Troponin': 28 * 3600,\n",
        "    'Mixed_Venous_O2': 28 * 3600,\n",
        "    'Ectopy_Frequency': 3 * 3600, # From Table 3.2 (3 hours)\n",
        "    'ACT': 28 * 3600,\n",
        "    'ETCO2': 28 * 3600,\n",
        "    'O2_Flow_Main': 4 * 3600,     # Assumed same as HR/BP (4 hours)\n",
        "    'O2_Flow_Add': 4 * 3600,      # Assumed same as HR/BP (4 hours)\n",
        "    'Flow_Rate': 4 * 3600,        # Assumed same as HR/BP (4 hours)\n",
        "    'CRP': 28 * 3600,\n",
        "}\n",
        "\n",
        "# Default limit for variables not listed\n",
        "DEFAULT_HOLD_LIMIT = 28 * 3600"
      ],
      "metadata": {
        "id": "7RgaqL5o3yUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------------------------------------------------\n",
        "## IMPUTATION FROM EXISTING VALUE\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"Data imputation from existing values starts...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create DataFrame copy to store imputation results\n",
        "# Use 'reformat_cleaned' DataFrame in memory from previous cell run\n",
        "temp_reformat_impute = reformat_cleaned.copy()\n",
        "available_columns = temp_reformat_impute.columns\n",
        "\n",
        "# Isolate Active and Static Patients\n",
        "original_rows = len(temp_reformat_impute)\n",
        "print(f\"Original rows count: {original_rows}\")\n",
        "\n",
        "# 1. Identify Dummy Patients to be EXCLUDED from SAH/Relational Imputation\n",
        "missing_patient_ids = temp_reformat_impute[temp_reformat_impute['charttime'].isnull()]['icustay_id'].unique()\n",
        "\n",
        "# 2. Active Patients (Only those with non-NaN 'charttime') - These will be imputed\n",
        "reformat_active = temp_reformat_impute.dropna(subset=['charttime']).copy()\n",
        "print(f\"Total rows for Imputation/SAH (Active Patients): {len(reformat_active)}\")\n",
        "\n",
        "# 3. Static Dummy Patients (No need for SAH/Relational Imputation)\n",
        "# Take ONLY one row per icustay_id\n",
        "reformat_static_dummy = temp_reformat_impute[temp_reformat_impute['icustay_id'].isin(missing_patient_ids)].drop_duplicates(subset=['icustay_id']).copy()\n",
        "print(f\"Total unique icustay_ids to SKIP SAH (Static Patients): {len(reformat_static_dummy['icustay_id'].unique())}\")\n",
        "\n",
        "# Delete unused temporary DataFrames\n",
        "del temp_reformat_impute\n",
        "gc.collect()\n",
        "\n",
        "# Imputation for reformat_active\n",
        "# 1. GCS imputation from RASS (map RASS value to GCS)\n",
        "if 'GCS' in available_columns and 'RASS' in available_columns:\n",
        "    print(\"Applying GCS Imputation from RASS...\")\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] >= 0), 'GCS'] = 15\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -1), 'GCS'] = 14\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -2), 'GCS'] = 12\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -3), 'GCS'] = 11\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -4), 'GCS'] = 6\n",
        "    reformat_active.loc[reformat_active['GCS'].isnull() & (reformat_active['RASS'] == -5), 'GCS'] = 3\n",
        "\n",
        "# 2. FiO2 imputation and conversion (percentage vs fraction)\n",
        "def harmonize_fio2(df):\n",
        "    \"\"\"\n",
        "    Converts FiO2 values from percentage (21–100) to fraction (0.21–1.0)\n",
        "    and applies minimum/maximum capping\n",
        "    \"\"\"\n",
        "    if 'FiO2' in df.columns:\n",
        "        # 1. CONVERSION: Divide values > 1.0 and <= 100 by 100\n",
        "        mask_persen = (df['FiO2'] > 1.0) & (df['FiO2'] <= 100.0)\n",
        "        df.loc[mask_persen, 'FiO2'] /= 100\n",
        "\n",
        "        # 2. Cap maximum FiO2 at 1.0 (for cases > 1.0 that remain uncorrected)\n",
        "        df.loc[df['FiO2'] > 1.0, 'FiO2'] = 1.0\n",
        "\n",
        "        # 3. Cap minimum FiO2 at 0.21\n",
        "        df.loc[df['FiO2'] < 0.21, 'FiO2'] = 0.21\n",
        "    return df\n",
        "\n",
        "# 3. Imputation of FiO2 from O2 Flow and Interface Type\n",
        "if 'FiO2' in available_columns:\n",
        "    print(\"Applying FiO2 Imputation from O2 Flow...\")\n",
        "    # Combine O2 Flow columns\n",
        "    reformat_active['O2_Flow_Combined'] = reformat_active.get('Flow_Rate').fillna(\n",
        "        reformat_active.get('O2_Flow_Main')).fillna(\n",
        "        reformat_active.get('O2_Flow_Add'))\n",
        "\n",
        "    # NO FiO2, YES O2 flow, no interface (0) OR cannula (2)\n",
        "    if 'O2_Delivery_Device' in available_columns:\n",
        "        mask = reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].notnull() & (reformat_active['O2_Delivery_Device'].isin([0, 2]))\n",
        "        # Commands must be ordered from highest to lowest threshold\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 15), 'FiO2'] = 70\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 12), 'FiO2'] = 62\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 10), 'FiO2'] = 55\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 8), 'FiO2'] = 50\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 6), 'FiO2'] = 44\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 5), 'FiO2'] = 40\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 4), 'FiO2'] = 36\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 3), 'FiO2'] = 32\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 2), 'FiO2'] = 28\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 1), 'FiO2'] = 24\n",
        "\n",
        "        # NO FiO2, YES O2 flow, face mask OR...\n",
        "        mask = reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].notnull() & (reformat_active['O2_Delivery_Device'].isin([1, 3, 4, 5, 6, 9, 10]))\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 15), 'FiO2'] = 75\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 12), 'FiO2'] = 69\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 10), 'FiO2'] = 66\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 8), 'FiO2'] = 58\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 6), 'FiO2'] = 40\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 4), 'FiO2'] = 36\n",
        "\n",
        "        # NO FiO2, YES O2 flow, Non rebreather mask\n",
        "        mask = reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].notnull() & (reformat_active['O2_Delivery_Device'] == 7)\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] >= 15), 'FiO2'] = 100\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] >= 10), 'FiO2'] = 90\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] < 10), 'FiO2'] = 80\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 8), 'FiO2'] = 70\n",
        "        reformat_active.loc[mask & (reformat_active['O2_Flow_Combined'] <= 6), 'FiO2'] = 60\n",
        "\n",
        "        # NO FiO2, NO O2 flow\n",
        "        # reformat_active.loc[reformat_active['FiO2'].isnull() & reformat_active['O2_Flow_Combined'].isnull(), 'FiO2'] = 21\n",
        "\n",
        "    # Re-run FiO2 harmonization/capping\n",
        "    reformat_active = harmonize_fio2(reformat_active)\n",
        "\n",
        "    # Hapus kolom pembantu\n",
        "    reformat_active.drop(columns=['O2_Flow_Combined'], inplace=True)\n",
        "\n",
        "# 4. Imputation of BP (using 2 of 3 available values)\n",
        "if all(col in available_columns for col in ['SysBP', 'MeanBP', 'DiaBP']):\n",
        "    print(\"Applying BP Imputation (3-point method)...\")\n",
        "    reformat_active.loc[reformat_active['DiaBP'].isnull() & reformat_active['SysBP'].notnull() & reformat_active['MeanBP'].notnull(), 'DiaBP'] = (3 * reformat_active['MeanBP'] - reformat_active['SysBP']) / 2\n",
        "    reformat_active.loc[reformat_active['MeanBP'].isnull() & reformat_active['SysBP'].notnull() & reformat_active['DiaBP'].notnull(), 'MeanBP'] = (reformat_active['SysBP'] + 2 * reformat_active['DiaBP']) / 3\n",
        "    reformat_active.loc[reformat_active['SysBP'].isnull() & reformat_active['MeanBP'].notnull() & reformat_active['DiaBP'].notnull(), 'SysBP'] = 3 * reformat_active['MeanBP'] - 2 * reformat_active['DiaBP']\n",
        "\n",
        "    for col in ['SysBP', 'MeanBP', 'DiaBP']:\n",
        "        # Floor capping setelah kalkulasi\n",
        "        reformat_active.loc[reformat_active[col] < 0, col] = 0\n",
        "\n",
        "# 5. Imputation of Temp (Convert °C to °F and vice versa)\n",
        "if 'Temp_C' in available_columns and 'Temp_F' in available_columns:\n",
        "    print(\"Applying Temperature Conversion/Correction...\")\n",
        "    # a. Move Temp_C values that were recorded in Fahrenheit\n",
        "    mask_c_is_f = (reformat_active['Temp_C'] > 45) & (reformat_active['Temp_C'] < 100) & reformat_active['Temp_F'].isnull()\n",
        "    reformat_active.loc[mask_c_is_f, 'Temp_F'] = reformat_active['Temp_C']\n",
        "    reformat_active.loc[mask_c_is_f, 'Temp_C'] = np.nan\n",
        "\n",
        "    # b. Move Temp_F values that were recorded in Celsius\n",
        "    mask_f_is_c = (reformat_active['Temp_F'] > 15) & (reformat_active['Temp_F'] < 45) & reformat_active['Temp_C'].isnull()\n",
        "    reformat_active.loc[mask_f_is_c, 'Temp_C'] = reformat_active['Temp_F']\n",
        "    reformat_active.loc[mask_f_is_c, 'Temp_F'] = np.nan\n",
        "\n",
        "    # c. Temperature Conversion\n",
        "    reformat_active.loc[reformat_active['Temp_F'].isnull() & reformat_active['Temp_C'].notnull(), 'Temp_F'] = reformat_active['Temp_C'] * 1.8 + 32\n",
        "    reformat_active.loc[reformat_active['Temp_C'].isnull() & reformat_active['Temp_F'].notnull(), 'Temp_C'] = (reformat_active['Temp_F'] - 32) / 1.8\n",
        "\n",
        "# 6. Imputation of Hb/HCT\n",
        "if 'Hb' in available_columns and 'HCT' in available_columns:\n",
        "    print(\"Applying Hb/HCT Imputation...\")\n",
        "    # Gunakan regresi empiris/klinis untuk mengimputasi\n",
        "    reformat_active.loc[reformat_active['HCT'].isnull() & reformat_active['Hb'].notnull(), 'HCT'] = reformat_active['Hb'] * 2.862 + 1.216\n",
        "    reformat_active.loc[reformat_active['Hb'].isnull() & reformat_active['HCT'].notnull(), 'Hb'] = (reformat_active['HCT'] - 1.216) / 2.862\n",
        "\n",
        "    for col in ['Hb', 'HCT']:\n",
        "        reformat_active.loc[reformat_active[col] < 0, col] = 0\n",
        "\n",
        "# 7. Imputation of Bilirubin\n",
        "# Lower threshold (mg/dL)\n",
        "TOTAL_BILI_MIN_CAP = 0.2\n",
        "DIRECT_BILI_MIN_CAP = 0.1\n",
        "\n",
        "if 'Total_bili' in available_columns and 'Direct_Bili' in available_columns:\n",
        "    print(\"Applying Bilirubin Imputation...\")\n",
        "    reformat_active.loc[reformat_active['Direct_Bili'].isnull() & reformat_active['Total_bili'].notnull(), 'Direct_Bili'] = reformat_active['Total_bili'] * 0.6934 - 0.1752\n",
        "    reformat_active.loc[reformat_active['Total_bili'].isnull() & reformat_active['Direct_Bili'].notnull(), 'Total_bili'] = (reformat_active['Direct_Bili'] + 0.1752) / 0.6934\n",
        "\n",
        "    for col in ['Total_bili', 'Direct_Bili']:\n",
        "        # Convert negative values into lower threshold (0.2 / 0.1 mg/dL)\n",
        "        if col == 'Total_bili':\n",
        "            min_cap = TOTAL_BILI_MIN_CAP\n",
        "        else:\n",
        "            min_cap = DIRECT_BILI_MIN_CAP\n",
        "\n",
        "        reformat_active.loc[reformat_active[col] < min_cap, col] = min_cap\n",
        "\n",
        "        print(f\"Minimum Capping for {col} applied at {min_cap} mg/dL (eliminates 0s and negatives).\")\n",
        "\n",
        "# Save the imputed 'reformat_active' to a CSV\n",
        "reformat_active.to_csv(os.path.join(data_dir, 'reformat_imputed.csv'), index=False)\n",
        "print(f\"\\nImputation from the existing value (Pasien Aktif only) has been done in {time.time() - start_time:.1f} seconds.\")\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del reformat_cleaned\n",
        "gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## TIME-LIMITED SAMPLE AND HOLD (SAH) IMPUTATION\n",
        "# ----------------------------------------------------------------------\n",
        "# Apply Sample and Hold (SAH) imputation to all variables\n",
        "print(\"\\nRunning Time-Limited Sample and Hold (SAH) imputation on active patients...\")\n",
        "sah_start_time = time.time()\n",
        "\n",
        "# Ensure the data is sorted by patient and time (use reformat_active)\n",
        "reformat_active = reformat_active.sort_values(by=['icustay_id', 'charttime'])\n",
        "\n",
        "# Set icustay_id and charttime as indices\n",
        "reformat_imputed_temp = reformat_active.set_index(['icustay_id', 'charttime'])\n",
        "\n",
        "# Initialize the output DataFrame\n",
        "reformat_imputed_sah = reformat_imputed_temp.copy()\n",
        "\n",
        "# Create charttime series for delta calculation\n",
        "charttime_series = reformat_imputed_temp.index.get_level_values('charttime').to_series()\n",
        "charttime_series.index = reformat_imputed_temp.index\n",
        "\n",
        "# Apply Time-Limited SAH (ffill dan bfill) for each column\n",
        "for col in reformat_imputed_temp.columns:\n",
        "    # Process only columns that contain missing values\n",
        "    if reformat_imputed_temp[col].isnull().any():\n",
        "\n",
        "        # Use the given function: HOLD_LIMITS_SEC and DEFAULT_HOLD_LIMIT\n",
        "        hold_limit = HOLD_LIMITS_SEC.get(col, DEFAULT_HOLD_LIMIT)\n",
        "\n",
        "        # 1. Forward Fill (ffill) Time-Limited\n",
        "        ffill_result = reformat_imputed_temp[col].groupby(level='icustay_id').ffill()\n",
        "        last_valid_time = charttime_series.where(reformat_imputed_temp[col].notnull()).groupby(level='icustay_id').ffill()\n",
        "        time_delta = (charttime_series - last_valid_time).dt.total_seconds()\n",
        "        mask_time_limit_exceeded = time_delta > hold_limit\n",
        "\n",
        "        reformat_imputed_sah[col] = ffill_result\n",
        "        reformat_imputed_sah.loc[mask_time_limit_exceeded, col] = np.nan\n",
        "\n",
        "\n",
        "        # 2. Backward Fill (bfill) Time-Limited\n",
        "        bfill_result = reformat_imputed_temp[col].groupby(level='icustay_id').bfill()\n",
        "        next_valid_time = charttime_series.where(reformat_imputed_temp[col].notnull()).groupby(level='icustay_id').bfill()\n",
        "        time_delta_bfill = (next_valid_time - charttime_series).dt.total_seconds()\n",
        "        mask_bfill_limit_exceeded = time_delta_bfill > hold_limit\n",
        "        mask_is_nan_after_ffill = reformat_imputed_sah[col].isnull()\n",
        "\n",
        "        bfill_valid = bfill_result.copy()\n",
        "        bfill_valid.loc[mask_bfill_limit_exceeded] = np.nan\n",
        "\n",
        "        # Merge bfill_valid results into columns that are still NaN after ffill\n",
        "        reformat_imputed_sah[col].fillna(bfill_valid, inplace=True)\n",
        "\n",
        "\n",
        "# Reset index for Active Patients after SAH\n",
        "reformat_imputed_sah = reformat_imputed_sah.reset_index()\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del reformat_imputed_temp, reformat_active\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "# Final merge (Cohort reconciliation)\n",
        "print(\"\\nRecombining Active and Static Patients...\")\n",
        "\n",
        "time_series_cols_imputed = [col for col in reformat_imputed_sah.columns if col not in reformat_static_dummy.columns]\n",
        "\n",
        "# Only add columns that are MISSING in dummy\n",
        "for col in time_series_cols_imputed:\n",
        "    if col not in reformat_static_dummy.columns:\n",
        "        reformat_static_dummy[col] = np.nan\n",
        "\n",
        "# Ensure the column order is consistent\n",
        "reformat_static_dummy = reformat_static_dummy[reformat_imputed_sah.columns]\n",
        "\n",
        "# Combine SAH results with Static Patients\n",
        "reformat_imputed_final = pd.concat([reformat_imputed_sah, reformat_static_dummy], ignore_index=True)\n",
        "\n",
        "# Save the final imputed DataFrame\n",
        "reformat_imputed_final.to_csv(os.path.join(data_dir, 'reformat_imputed_final.csv'), index=False)\n",
        "print(f\"Final imputed data saved to 'reformat_imputed_final.csv' in {time.time() - sah_start_time:.1f} seconds.\")\n",
        "print(f\"Total unique icustay_ids in final file: {reformat_imputed_final['icustay_id'].nunique()}\")\n",
        "\n",
        "# Delete unused DataFrames\n",
        "del reformat_imputed_sah, reformat_static_dummy\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYPBTez532pi",
        "outputId": "7363606e-76e8-47ce-95f6-c151666c23e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data imputation from existing values starts...\n",
            "Original rows count: 911266\n",
            "Total rows for Imputation/SAH (Active Patients): 894841\n",
            "Total unique icustay_ids to SKIP SAH (Static Patients): 16425\n",
            "Applying GCS Imputation from RASS...\n",
            "Applying FiO2 Imputation from O2 Flow...\n",
            "Applying BP Imputation (3-point method)...\n",
            "Applying Hb/HCT Imputation...\n",
            "Applying Bilirubin Imputation...\n",
            "Minimum Capping for Total_bili applied at 0.2 mg/dL (eliminates 0s and negatives).\n",
            "Minimum Capping for Direct_Bili applied at 0.1 mg/dL (eliminates 0s and negatives).\n",
            "\n",
            "Imputation from the existing value (Pasien Aktif only) has been done in 21.8 seconds.\n",
            "\n",
            "Running Time-Limited Sample and Hold (SAH) imputation on active patients...\n",
            "\n",
            "Recombining Active and Static Patients...\n",
            "Final imputed data saved to 'reformat_imputed_final.csv' in 37.2 seconds.\n",
            "Total unique icustay_ids in final file: 26299\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DEFINE TIME WINDOW PARAMETERS ---\n",
        "window_size_hours = 4\n",
        "TOTAL_STEPS = 20\n",
        "MIN_PRE_ONSET_HOURS = -24 # -24 jam (Inclusive)\n",
        "MAX_POST_ONSET_HOURS = 56 # +56 jam (Exclusive)\n",
        "\n",
        "# Offset calculation for Timestep IDs 1–20\n",
        "TIMESTEP_OFFSET = abs(MIN_PRE_ONSET_HOURS) // window_size_hours\n",
        "T0_BASELINE_STEP_ID = 1\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## DATA COMBINATION & ONSET PREPARATION\n",
        "# --------------------------------------------------------------------------------------\n",
        "print(\"Starting data combination and onset preparation...\")\n",
        "\n",
        "# 1. Combine Input Fluids (input_combined)\n",
        "inputMV_renamed = inputMV.rename(columns={'amount': 'input_volume', 'starttime': 'charttime'}).copy()\n",
        "inputCV_renamed = inputCV.rename(columns={'amount': 'input_volume'}).copy()\n",
        "input_combined = pd.concat([\n",
        "    inputMV_renamed[['icustay_id', 'charttime', 'input_volume']],\n",
        "    inputCV_renamed[['icustay_id', 'charttime', 'input_volume']]\n",
        "], ignore_index=True)\n",
        "input_combined['input_volume'] = input_combined['input_volume'].fillna(0)\n",
        "input_combined['charttime'] = pd.to_datetime(input_combined['charttime'], errors='coerce')\n",
        "\n",
        "# 2. Combine Vasopressors (vaso_combined)\n",
        "vasoMV_renamed = vasoMV.rename(columns={'starttime': 'charttime'}).copy()\n",
        "vaso_combined = pd.concat([\n",
        "    vasoMV_renamed[['icustay_id', 'charttime', 'rate_std']],\n",
        "    vasoCV[['icustay_id', 'charttime', 'rate_std']]\n",
        "], ignore_index=True)\n",
        "vaso_combined['charttime'] = pd.to_datetime(vaso_combined['charttime'], errors='coerce')\n",
        "\n",
        "# 3. Retrieve onset time (T=0)\n",
        "onset_df = reformat_imputed_final[['icustay_id', 'onset_time']].drop_duplicates().copy()\n",
        "onset_df['onset_time'] = pd.to_datetime(onset_df['onset_time'], errors='coerce')\n",
        "onset_df.dropna(subset=['onset_time'], inplace=True)\n",
        "onset_df['icustay_id'] = onset_df['icustay_id'].astype('int64')\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## BINNING & AGREGATION FUNCTION (Fluid, UO, Vaso)\n",
        "# --------------------------------------------------------------------------------------\n",
        "def perform_binning_aggregation(data_df, value_col, agg_funcs, prefix):\n",
        "    \"\"\"Merge data with onset_time, perform binning, and aggregation.\"\"\"\n",
        "    if data_df.empty:\n",
        "        return pd.DataFrame(columns=['icustay_id', 'window_label', f'{prefix}_{list(agg_funcs.keys())[0]}'], dtype=np.float64)\n",
        "\n",
        "    # Merge with onset_time\n",
        "    df_merged = pd.merge(data_df, onset_df, on='icustay_id', how='left')\n",
        "    df_merged.dropna(subset=['charttime', 'onset_time'], inplace=True)\n",
        "\n",
        "    # Calculate duration\n",
        "    df_merged['duration_from_onset_hours'] = (\n",
        "        df_merged['charttime'] - df_merged['onset_time']\n",
        "    ).dt.total_seconds() / 3600\n",
        "\n",
        "    # Filter duration\n",
        "    df_binned = df_merged[\n",
        "        (df_merged['duration_from_onset_hours'] >= MIN_PRE_ONSET_HOURS) &\n",
        "        (df_merged['duration_from_onset_hours'] < MAX_POST_ONSET_HOURS)\n",
        "    ].copy()\n",
        "\n",
        "    if df_binned.empty:\n",
        "         return pd.DataFrame(columns=['icustay_id', 'window_label', f'{prefix}_{list(agg_funcs.keys())[0]}'], dtype=np.float64)\n",
        "\n",
        "    # Label time windows (window_label, can be negative)\n",
        "    df_binned['window_label'] = np.floor(\n",
        "        df_binned['duration_from_onset_hours'] / window_size_hours\n",
        "    ).astype('int64') # Use standard int64\n",
        "\n",
        "    # Perform aggregation\n",
        "    agg_result = df_binned.groupby(\n",
        "        ['icustay_id', 'window_label']\n",
        "    ).agg(\n",
        "        **{f'{prefix}_{func}': (value_col, func) for func in agg_funcs}\n",
        "    ).reset_index()\n",
        "\n",
        "    return agg_result\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## INTERVAL AGREGATION & FLUID/VASO MERGE\n",
        "# --------------------------------------------------------------------------------------\n",
        "bin_start_time = time.time()\n",
        "\n",
        "input_4h = perform_binning_aggregation(input_combined, 'input_volume', {'sum': 'sum'}, 'input_step')\n",
        "uo_4h = perform_binning_aggregation(UO, 'value', {'sum': 'sum'}, 'uo_step')\n",
        "vaso_4h = perform_binning_aggregation(vaso_combined, 'rate_std', {'max': 'max', 'median': 'median'}, 'vaso_step')\n",
        "\n",
        "# Merge Fluid/UO/Vaso Step Data\n",
        "fluid_vaso_agg_final = pd.merge(input_4h, uo_4h, on=['icustay_id', 'window_label'], how='outer')\n",
        "fluid_vaso_agg_final = pd.merge(fluid_vaso_agg_final, vaso_4h, on=['icustay_id', 'window_label'], how='outer')\n",
        "\n",
        "print(f\"Binning aggregation (Fluid/Vaso) complete in {time.time() - bin_start_time:.1f} seconds.\")\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## MERGING T0 (PRE-ADMISSION) AND PRE-ONSET\n",
        "# --------------------------------------------------------------------------------------\n",
        "cum_start_time = time.time()\n",
        "\n",
        "# Merge Pre-adm\n",
        "preadm_input = inputpreadm.groupby('icustay_id').agg(preadm_input=('inputpreadm', 'sum')).reset_index()\n",
        "preadm_uo = UOpreadm.groupby('icustay_id').agg(preadm_uo=('value', 'sum')).reset_index()\n",
        "\n",
        "fluid_vaso_agg_final = pd.merge(fluid_vaso_agg_final, preadm_input, on='icustay_id', how='left').fillna({'preadm_input': 0})\n",
        "fluid_vaso_agg_final = pd.merge(fluid_vaso_agg_final, preadm_uo, on='icustay_id', how='left').fillna({'preadm_uo': 0})\n",
        "\n",
        "# Impute NaN steps with 0 (representing zero input/output in that time window)\n",
        "fluid_vaso_agg_final['input_step_sum'] = fluid_vaso_agg_final['input_step_sum'].fillna(0)\n",
        "fluid_vaso_agg_final['uo_step_sum'] = fluid_vaso_agg_final['uo_step_sum'].fillna(0)\n",
        "vaso_step_cols = ['vaso_step_max', 'vaso_step_median']\n",
        "fluid_vaso_agg_final[vaso_step_cols] = fluid_vaso_agg_final[vaso_step_cols].fillna(0)\n",
        "\n",
        "\n",
        "# Calculate TOTAL T0 (Pre-Adm + Pre-Onset)\n",
        "def calculate_t0_only(group):\n",
        "    # Sum only for window_label < 0 (pre-onset)\n",
        "    input_t0_total = group['preadm_input'].iloc[0] + group.loc[group['window_label'] < 0, 'input_step_sum'].sum()\n",
        "    uo_t0_total = group['preadm_uo'].iloc[0] + group.loc[group['window_label'] < 0, 'uo_step_sum'].sum()\n",
        "    return pd.Series({'input_t0_total': input_t0_total, 'uo_t0_total': uo_t0_total})\n",
        "\n",
        "t0_totals_df = fluid_vaso_agg_final.groupby('icustay_id').apply(calculate_t0_only).reset_index()\n",
        "\n",
        "# Extract Step Data and Merge T0\n",
        "fluid_vaso_step_data = fluid_vaso_agg_final.copy()\n",
        "fluid_vaso_step_data.drop(columns=['preadm_input', 'preadm_uo'], inplace=True, errors='ignore')\n",
        "\n",
        "# Convert window_label to timestep_id (1 to 20)\n",
        "fluid_vaso_step_data.rename(columns={'window_label': 'timestep_id_raw'}, inplace=True)\n",
        "fluid_vaso_step_data['timestep_id'] = fluid_vaso_step_data['timestep_id_raw'] + TIMESTEP_OFFSET + 1\n",
        "fluid_vaso_step_data = fluid_vaso_step_data[\n",
        "    (fluid_vaso_step_data['timestep_id'] >= 1) &\n",
        "    (fluid_vaso_step_data['timestep_id'] <= TOTAL_STEPS)\n",
        "].copy()\n",
        "fluid_vaso_step_data['timestep_id'] = fluid_vaso_step_data['timestep_id'].astype('int64')\n",
        "\n",
        "# Merge total T0 data into the first timestep (T=1)\n",
        "t0_to_merge = t0_totals_df.copy()\n",
        "t0_to_merge['timestep_id'] = T0_BASELINE_STEP_ID\n",
        "\n",
        "fluid_vaso_agg_pre_t0 = pd.merge(\n",
        "    fluid_vaso_step_data,\n",
        "    t0_to_merge,\n",
        "    on=['icustay_id', 'timestep_id'],\n",
        "    how='left'\n",
        ")\n",
        "fluid_vaso_agg_pre_t0.drop(columns=['timestep_id_raw'], inplace=True, errors='ignore')\n",
        "\n",
        "print(f\"Pre-T0 calculation and step-data extraction complete in {time.time() - cum_start_time:.1f} seconds.\")\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## CLINICAL FEATURE AGGREGATION & FINAL MERGE\n",
        "# --------------------------------------------------------------------------------------\n",
        "final_merge_start = time.time()\n",
        "\n",
        "# 1. Create Dense Timeline\n",
        "def create_dense_timeline_binned(df_onset):\n",
        "    all_time_steps = []\n",
        "    max_steps = TOTAL_STEPS\n",
        "    for _, row in df_onset.iterrows():\n",
        "        icustay_id = row['icustay_id']\n",
        "        start_time = row['onset_time']\n",
        "        temp_df = pd.DataFrame({\n",
        "            'icustay_id': icustay_id,\n",
        "            'timestep_id': np.arange(1, max_steps + 1)\n",
        "        })\n",
        "        temp_df['t0_window_start'] = start_time + pd.to_timedelta(\n",
        "            MIN_PRE_ONSET_HOURS + (temp_df['timestep_id'] - 1) * window_size_hours,\n",
        "            unit='h'\n",
        "        )\n",
        "        temp_df['onset_time'] = start_time\n",
        "        all_time_steps.append(temp_df)\n",
        "    dense_timeline_df = pd.concat(all_time_steps, ignore_index=True)\n",
        "    dense_timeline_df['icustay_id'] = dense_timeline_df['icustay_id'].astype('int64')\n",
        "    dense_timeline_df['timestep_id'] = dense_timeline_df['timestep_id'].astype('int64')\n",
        "    return dense_timeline_df.sort_values(by=['icustay_id', 't0_window_start'])\n",
        "\n",
        "dense_timeline_df = create_dense_timeline_binned(onset_df)\n",
        "\n",
        "# 2. Prepare Clinical Features for Aggregation (reformat_imputed_final)\n",
        "reformat_imputed_final_merge = pd.merge(reformat_imputed_final.drop(columns=['onset_time'], errors='ignore'),\n",
        "                                   onset_df, on='icustay_id', how='left')\n",
        "\n",
        "# Convert time and filter duration\n",
        "reformat_imputed_final_merge['charttime'] = pd.to_datetime(reformat_imputed_final_merge['charttime'], errors='coerce')\n",
        "reformat_imputed_final_merge['onset_time'] = pd.to_datetime(reformat_imputed_final_merge['onset_time'], errors='coerce')\n",
        "reformat_imputed_final_merge.dropna(subset=['charttime', 'onset_time'], inplace=True)\n",
        "\n",
        "reformat_imputed_final_merge['duration_from_onset_hours'] = (\n",
        "    reformat_imputed_final_merge['charttime'] - reformat_imputed_final_merge['onset_time']\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# Filter duration\n",
        "reformat_imputed_final_merge = reformat_imputed_final_merge[\n",
        "    (reformat_imputed_final_merge['duration_from_onset_hours'] < MAX_POST_ONSET_HOURS) &\n",
        "    (reformat_imputed_final_merge['duration_from_onset_hours'] >= MIN_PRE_ONSET_HOURS)\n",
        "].copy()\n",
        "\n",
        "# Correct Clinical Timestep ID (ensure consistency in type and calculation)\n",
        "reformat_imputed_final_merge['timestep_id'] = (\n",
        "    np.floor(reformat_imputed_final_merge['duration_from_onset_hours'] / window_size_hours).astype(int)\n",
        "    + TIMESTEP_OFFSET\n",
        "    + 1\n",
        ")\n",
        "reformat_imputed_final_merge['timestep_id'] = reformat_imputed_final_merge['timestep_id'].astype('int64')\n",
        "\n",
        "# 3. Aggregate Laboratory/Vital Sign Features (Mean) per Timestep\n",
        "clinical_skip_cols = ['icustay_id', 'timestep_id', 'onset_time', 'charttime', 'duration_from_onset_hours']\n",
        "demog_target_cols = ['gender', 'age', 'elixhauser', 'morta_hosp', 'morta_90', 'los']\n",
        "clinical_skip_cols.extend(demog_target_cols)\n",
        "clinical_mean_cols = [col for col in reformat_imputed_final_merge.columns if col not in clinical_skip_cols]\n",
        "\n",
        "# Convert clinical columns to numeric before mean aggregation\n",
        "for col in clinical_mean_cols:\n",
        "    if col in reformat_imputed_final_merge.columns:\n",
        "        reformat_imputed_final_merge[col] = pd.to_numeric(reformat_imputed_final_merge[col], errors='coerce')\n",
        "\n",
        "if not clinical_mean_cols:\n",
        "    print(\"!!! ERROR: clinical_mean_cols are empty. No clinical features are known.\")\n",
        "    clinical_features_agg = pd.DataFrame(columns=['icustay_id', 'timestep_id'])\n",
        "else:\n",
        "    # Perform Aggregation\n",
        "    clinical_features_agg = reformat_imputed_final_merge.groupby(['icustay_id', 'timestep_id'])[clinical_mean_cols].mean().reset_index()\n",
        "\n",
        "    # Filter only valid timesteps (1 to 20)\n",
        "    clinical_features_agg = clinical_features_agg[\n",
        "        (clinical_features_agg['timestep_id'] >= 1) &\n",
        "        (clinical_features_agg['timestep_id'] <= TOTAL_STEPS)\n",
        "    ].copy()\n",
        "\n",
        "    # Ensure int64 key type\n",
        "    clinical_features_agg['icustay_id'] = clinical_features_agg['icustay_id'].astype('int64')\n",
        "    clinical_features_agg['timestep_id'] = clinical_features_agg['timestep_id'].astype('int64')\n",
        "\n",
        "    print(f\"[DEBUG] Number of clinical/lab features to be aggregated (mean): {len(clinical_mean_cols)}\")\n",
        "    print(f\"[DEBUG] Number of rows in clinical_features_agg: {len(clinical_features_agg)}\")\n",
        "\n",
        "# 4. Final Merge Data\n",
        "final_merged_df = pd.merge(dense_timeline_df, clinical_features_agg,\n",
        "                           on=['icustay_id', 'timestep_id'], how='left')\n",
        "\n",
        "# Merge Fluid/Vaso data\n",
        "fluid_vaso_cols = [col for col in fluid_vaso_agg_pre_t0.columns if col not in ['icustay_id', 'timestep_id']]\n",
        "final_merged_df = pd.merge(final_merged_df,\n",
        "                           fluid_vaso_agg_pre_t0[['icustay_id', 'timestep_id'] + fluid_vaso_cols],\n",
        "                           on=['icustay_id', 'timestep_id'], how='left')\n",
        "\n",
        "present_clinical_cols = [col for col in clinical_mean_cols if col in final_merged_df.columns]\n",
        "print(f\"[DEBUG] Number of clinical features successfully merged into final_merged_df: {len(present_clinical_cols)}\")\n",
        "gc.collect()\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## FFILL/BFILL & CUMULATIVE BALANCE IMPUTATION\n",
        "# --------------------------------------------------------------------------------------\n",
        "print(\"\\nApplying LOCF/FFILL and BFILL to clinical/lab features...\")\n",
        "\n",
        "# 1. Apply FFILL/BFILL only on clinical/lab features\n",
        "for col in present_clinical_cols:\n",
        "    final_merged_df[col] = pd.to_numeric(final_merged_df[col], errors='coerce')\n",
        "    final_merged_df[col] = final_merged_df.groupby('icustay_id')[col].ffill()\n",
        "    final_merged_df[col] = final_merged_df.groupby('icustay_id')[col].bfill()\n",
        "\n",
        "#  Zero Imputation for Fluid/Vaso (step)\n",
        "final_merged_df['input_step_sum'] = final_merged_df['input_step_sum'].fillna(0)\n",
        "final_merged_df['uo_step_sum'] = final_merged_df['uo_step_sum'].fillna(0)\n",
        "final_merged_df['vaso_step_max'] = final_merged_df['vaso_step_max'].fillna(0)\n",
        "final_merged_df['vaso_step_median'] = final_merged_df['vaso_step_median'].fillna(0)\n",
        "\n",
        "# 2. Impute and Calculate Cumulative Balance Values\n",
        "final_merged_df['input_t0_total'] = final_merged_df.groupby('icustay_id')['input_t0_total'].ffill().fillna(0)\n",
        "final_merged_df['uo_t0_total'] = final_merged_df.groupby('icustay_id')['uo_t0_total'].ffill().fillna(0)\n",
        "\n",
        "final_merged_df['input_total'] = final_merged_df['input_t0_total'] + final_merged_df.groupby('icustay_id')['input_step_sum'].cumsum()\n",
        "final_merged_df['uo_total'] = final_merged_df['uo_t0_total'] + final_merged_df.groupby('icustay_id')['uo_step_sum'].cumsum()\n",
        "final_merged_df['cumulated_balance'] = final_merged_df['input_total'] - final_merged_df['uo_total']\n",
        "\n",
        "# Drop temporary t0/step columns\n",
        "final_merged_df.drop(columns=['input_step_sum', 'uo_step_sum', 'input_t0_total', 'uo_t0_total'], inplace=True, errors='ignore')\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "## FILTER LOS AND MERGE DEMOGRAPHICS\n",
        "# --------------------------------------------------------------------------------------\n",
        "\n",
        "# Merge Demografi & LOS\n",
        "demog_los = demog[['icustay_id', 'outtime', 'dod', 'expire_flag']].copy()\n",
        "demog_los['icustay_id'] = demog_los['icustay_id'].astype(np.int64)\n",
        "\n",
        "# Create actual_out_time\n",
        "demog_los['actual_out_time'] = demog_los.apply(\n",
        "    lambda row: row['dod'] if row['expire_flag'] == 1 and pd.notna(row['dod']) else row['outtime'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "final_merged_df = pd.merge(final_merged_df, demog_los[['icustay_id', 'actual_out_time']], on='icustay_id', how='left')\n",
        "\n",
        "# ====================================================================================\n",
        "# DEBUG TIME: Use the correct column names\n",
        "# ====================================================================================\n",
        "# Setelah merge LOS dan sebelum filter\n",
        "print(\"\\n[TIME DEBUGGING]\")\n",
        "\n",
        "# Check if actual_out_time is earlier than onset_time (T=0)\n",
        "diff_to_onset = (final_merged_df['actual_out_time'] - final_merged_df['onset_time']).dt.total_seconds() / 3600\n",
        "print(f\"Minimal difference (LOS - Onset): {diff_to_onset.min():.2f} hours\")\n",
        "print(f\"Maksimal difference (LOS - Onset): {diff_to_onset.max():.2f} hours\")\n",
        "print(f\"Median difference (LOS - Onset): {diff_to_onset.median():.2f} hours\")\n",
        "\n",
        "# Check which column causes rows to be dropped\n",
        "rows_to_be_removed = (final_merged_df['t0_window_start'] >= final_merged_df['actual_out_time']) & \\\n",
        "                     (final_merged_df['actual_out_time'].notna())\n",
        "print(f\"Total rows to be REMOVED (based on filter logic): {rows_to_be_removed.sum()}\")\n",
        "\n",
        "if rows_to_be_removed.sum() == len(final_merged_df):\n",
        "    print(\"!!! CONFIRMATION: ALL ROWS MEET THE DELETION CRITERIA (LOS FILTER).\")\n",
        "    # Cek nilai t0_window_start pertama\n",
        "    first_timestep_rows = final_merged_df[final_merged_df['timestep_id'] == 1].copy()\n",
        "    print(\"\\nSample Data (Timestep 1):\")\n",
        "    print(first_timestep_rows[['icustay_id', 't0_window_start', 'actual_out_time']].head(5).to_markdown(index=False))\n",
        "\n",
        "# ====================================================================================\n",
        "\n",
        "# LOS Filter: Remove timesteps occurring AFTER discharge time\n",
        "initial_rows = len(final_merged_df)\n",
        "filter_out_of_los = (final_merged_df['t0_window_start'] >= final_merged_df['actual_out_time']) & \\\n",
        "                    (final_merged_df['actual_out_time'].notna())\n",
        "\n",
        "final_merged_df = final_merged_df[~filter_out_of_los].copy()\n",
        "rows_removed = initial_rows - len(final_merged_df)\n",
        "print(f\"LOS Filter Applied: Removed {rows_removed} timesteps.\")\n",
        "\n",
        "final_merged_df.drop(columns=['actual_out_time'], inplace=True, errors='ignore')\n",
        "\n",
        "# Merge non-temporal demographic features\n",
        "demog['icustay_id'] = demog['icustay_id'].astype(np.int64)\n",
        "\n",
        "# Calculate is_readmit\n",
        "demog['is_readmit'] = demog['adm_order'] > 1\n",
        "\n",
        "time_diff = (demog['dod'] - demog['outtime']).dt.total_seconds()\n",
        "demog['recent_death'] = (time_diff > 0) & (time_diff < (48*3600)) & (demog['expire_flag'] == 1)\n",
        "demog['recent_death'] = demog['recent_death'].fillna(False)\n",
        "\n",
        "demog_cols = ['icustay_id', 'gender', 'age', 'elixhauser', 'is_readmit', 'morta_hosp', 'recent_death', 'morta_90', 'los']\n",
        "demog_constant = demog[demog_cols].drop_duplicates(subset=['icustay_id'])\n",
        "demog_constant['gender'] = demog_constant['gender'].replace({1: 'M', 2: 'F'})\n",
        "\n",
        "# Drop demographic columns already present in final_merged_df before merging\n",
        "demog_cols_to_drop = [col for col in demog_cols if col != 'icustay_id']\n",
        "final_merged_df = final_merged_df.drop(columns=demog_cols_to_drop, errors='ignore')\n",
        "\n",
        "# Merge akhir\n",
        "final_merged_df = pd.merge(final_merged_df, demog_constant, on='icustay_id', how='left')\n",
        "\n",
        "# Final Cleanup and Verification\n",
        "final_merged_df.sort_values(by=['icustay_id', 't0_window_start'], inplace=True)\n",
        "final_merged_df.rename(columns={'onset_time': 'onset_time_T0'}, inplace=True)\n",
        "\n",
        "print(f\"\\nFinal merge and data processing complete in {time.time() - final_merge_start:.1f} seconds.\")\n",
        "print(f\"Final DataFrame has {len(final_merged_df.icustay_id.unique())} patients and {len(final_merged_df)} timesteps.\")\n",
        "print(\"\\nVerifikasi Kolom Penting (t0, onset, kumulatif, Klinis):\")\n",
        "print(final_merged_df[['icustay_id', 'timestep_id', 't0_window_start', 'onset_time_T0', 'input_total', 'cumulated_balance'] + ['HR', 'MeanBP', 'Creatinine']].head(10).to_markdown(index=False))\n",
        "\n",
        "del dense_timeline_df, clinical_features_agg, fluid_vaso_agg_pre_t0\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uT_TQpOgq_qi",
        "outputId": "b90d8c71-8915-4897-dfee-4a939dbdc2bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data combination and onset preparation...\n",
            "Binning aggregation (Fluid/Vaso) complete in 1.2 seconds.\n",
            "Pre-T0 calculation and step-data extraction complete in 15.0 seconds.\n",
            "[DEBUG] Number of clinical/lab features to be aggregated (mean): 64\n",
            "[DEBUG] Number of rows in clinical_features_agg: 119717\n",
            "[DEBUG] Number of clinical features successfully merged into final_merged_df: 64\n",
            "\n",
            "Applying LOCF/FFILL and BFILL to clinical/lab features...\n",
            "\n",
            "[TIME DEBUGGING]\n",
            "Minimal difference (LOS - Onset): -1507.36 hours\n",
            "Maksimal difference (LOS - Onset): 89524.75 hours\n",
            "Median difference (LOS - Onset): 145.87 hours\n",
            "Total rows to be REMOVED (based on filter logic): 77459\n",
            "LOS Filter Applied: Removed 77459 timesteps.\n",
            "\n",
            "Final merge and data processing complete in 35.4 seconds.\n",
            "Final DataFrame has 24948 patients and 448521 timesteps.\n",
            "\n",
            "Verifikasi Kolom Penting (t0, onset, kumulatif, Klinis):\n",
            "|   icustay_id |   timestep_id | t0_window_start     | onset_time_T0       |   input_total |   cumulated_balance |   HR |   MeanBP |   Creatinine |\n",
            "|-------------:|--------------:|:--------------------|:--------------------|--------------:|--------------------:|-----:|---------:|-------------:|\n",
            "|       200001 |             1 | 2181-11-17 00:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             2 | 2181-11-17 04:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             3 | 2181-11-17 08:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             4 | 2181-11-17 12:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             5 | 2181-11-17 16:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             6 | 2181-11-17 20:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             7 | 2181-11-18 00:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             8 | 2181-11-18 04:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |             9 | 2181-11-18 08:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n",
            "|       200001 |            10 | 2181-11-18 12:00:00 | 2181-11-18 00:00:00 |             0 |                   0 |  nan |      nan |          nan |\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURE DISPLAY SETTINGS FOR IMPORTANT COLUMNS/ROWS ---\n",
        "pd.set_option('display.max_rows', 50)\n",
        "pd.set_option('display.max_columns', None)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"=== STARTING KOMOROWSKI IMPUTATION & FEATURE ENGINEERING ===\")\n",
        "\n",
        "# --- 0. DUPLICATE DATAFRAME ---\n",
        "# Create a copy to store results as requested\n",
        "final_merged_df2 = final_merged_df.copy()\n",
        "print(\"DataFrame duplicated to 'final_merged_df2'.\")\n",
        "\n",
        "# --- Prepare Clinical/Lab Columns (Once Only) ---\n",
        "cols_to_exclude_from_impute = [\n",
        "    'icustay_id', 'timestep_id', 't0_window_start', 'onset_time_T0', 'gender', 'age', 'elixhauser',\n",
        "    'is_readmit', 'morta_hosp', 'recent_death', 'morta_90', 'los', 'input_total', 'uo_total',\n",
        "    'cumulated_balance', 'vaso_step_max', 'vaso_step_median',\n",
        "]\n",
        "\n",
        "# Get all numeric columns with NaN (excluding excluded columns)\n",
        "numeric_nan_cols = final_merged_df2.select_dtypes(include=np.number).columns[\n",
        "    final_merged_df2.select_dtypes(include=np.number).isnull().any()\n",
        "].tolist()\n",
        "clinical_impute_cols = [col for col in numeric_nan_cols if col not in cols_to_exclude_from_impute]\n",
        "\n",
        "if not clinical_impute_cols:\n",
        "    print(\"WARNING: All core clinical columns are already fully imputed or not found. Skipping Interpolation/kNN.\")\n",
        "\n",
        "else:\n",
        "    # ----------------------------------------------------------------------\n",
        "    ## 1. LINEAR INTERPOLATION (for missingness < 5%)\n",
        "    # ----------------------------------------------------------------------\n",
        "    print(\"\\n[STEP 1/5] Performing Linear Interpolation (< 5% Missingness)...\")\n",
        "\n",
        "    initial_missingness = final_merged_df2[clinical_impute_cols].isnull().sum() / len(final_merged_df2)\n",
        "    linear_interp_cols = initial_missingness[initial_missingness < 0.05].index.tolist()\n",
        "\n",
        "    for col in tqdm(linear_interp_cols, desc=\"Linear Interpolation\"):\n",
        "        # Interpolate per icustay_id group\n",
        "        final_merged_df2[col] = final_merged_df2.groupby('icustay_id')[col].apply(\n",
        "            lambda x: x.interpolate(method='linear', limit_direction='both').fillna(x.median())\n",
        "        )\n",
        "\n",
        "    print(f\"✅ Linear Interpolation applied to {len(linear_interp_cols)} columns.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 2. k-NN IMPUTATION (WITH LOCAL MEDIAN FALLBACK PER CHUNK)\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "# Recalculate columns that still have NaN after Linear Interpolation\n",
        "knn_impute_cols = final_merged_df2[clinical_impute_cols].columns[\n",
        "    final_merged_df2[clinical_impute_cols].isnull().any()\n",
        "].tolist()\n",
        "\n",
        "if not knn_impute_cols:\n",
        "    print(\"\\nWARNING: All clinical columns are already clean. Proceeding to Step 3.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n[STEP 2/5] Performing k-NN Imputation (1-NN, Euclidean, Chunking) - LOCAL MEDIAN PATCH...\")\n",
        "\n",
        "    imputer = KNNImputer(n_neighbors=1, weights='uniform', metric='nan_euclidean')\n",
        "\n",
        "    # 1. Extract data to be imputed (Expected ~64 Columns)\n",
        "    ref_data = final_merged_df2[knn_impute_cols].copy()\n",
        "\n",
        "    current_knn_cols = ref_data.columns.tolist()\n",
        "    total_rows = len(ref_data)\n",
        "    total_cols = len(current_knn_cols)  # Expected 64\n",
        "    imputed_array = np.empty((total_rows, total_cols), dtype=np.float64)\n",
        "\n",
        "    CHUNK_SIZE = 10000\n",
        "\n",
        "    print(f\"Total rows to impute: {total_rows}. Total columns to process: {total_cols}. Chunk size: {CHUNK_SIZE}\")\n",
        "\n",
        "    # Compute global medians for fallback (efficient)\n",
        "    global_medians = ref_data.median()\n",
        "\n",
        "    # Iterate with 10K Chunking\n",
        "    for i in tqdm(range(0, total_rows, CHUNK_SIZE), desc=\"k-NN Chunk Imputation\"):\n",
        "        start_idx = i\n",
        "        end_idx = min(i + CHUNK_SIZE, total_rows)\n",
        "\n",
        "        # Komorowski logic for the last chunk\n",
        "        if end_idx == total_rows and total_rows > CHUNK_SIZE:\n",
        "            start_idx = total_rows - CHUNK_SIZE\n",
        "\n",
        "        chunk = ref_data.iloc[start_idx:end_idx].copy()  # Work on a copy of the chunk\n",
        "\n",
        "        imputed_chunk = imputer.fit_transform(chunk)\n",
        "\n",
        "        # Check for dimension mismatch\n",
        "        if imputed_chunk.shape[1] != total_cols:\n",
        "            missing_cols = chunk.columns[chunk.isnull().all()].tolist()\n",
        "\n",
        "            print(f\"\\n⚠️ DIMENSION MISMATCH FOUND in chunk {i//CHUNK_SIZE} ({len(missing_cols)} missing columns).\")\n",
        "            # 1. Apply median fallback to missing columns in chunk\n",
        "            for col in missing_cols:\n",
        "                if col in global_medians:\n",
        "                    chunk[col] = chunk[col].fillna(global_medians[col])\n",
        "\n",
        "            # 2. Re-run k-NN on chunk after median fallback\n",
        "            imputed_chunk = imputer.fit_transform(chunk)\n",
        "\n",
        "            if imputed_chunk.shape[1] != total_cols:\n",
        "                raise ValueError(\n",
        "                    f\"FATAL ERROR: Dimension mismatch remains after median fallback. Expected {total_cols} but got {imputed_chunk.shape[1]}\"\n",
        "                )\n",
        "\n",
        "            print(f\"✅ Chunk {i//CHUNK_SIZE} fixed with median fallback. Continuing...\")\n",
        "\n",
        "        array_start_idx = i\n",
        "        array_end_idx = end_idx\n",
        "\n",
        "        if end_idx == total_rows and total_rows > CHUNK_SIZE:\n",
        "            array_start_idx = total_rows - len(imputed_chunk)\n",
        "            array_end_idx = total_rows\n",
        "\n",
        "        imputed_array[array_start_idx:array_end_idx, :] = imputed_chunk\n",
        "\n",
        "    # Replace original columns with imputed results\n",
        "    final_merged_df2[current_knn_cols] = pd.DataFrame(imputed_array, columns=current_knn_cols, index=final_merged_df2.index)\n",
        "\n",
        "    missing_after_knn = final_merged_df2[current_knn_cols].isnull().sum().sum()\n",
        "    if missing_after_knn == 0:\n",
        "        print(\"✅ k-NN Imputation completed successfully. 0 NaN remaining in clinical columns.\")\n",
        "    else:\n",
        "        print(f\"🛑 WARNING: {missing_after_knn} NaN remaining after k-NN. Performing final median imputation.\")\n",
        "        final_merged_df2[current_knn_cols] = final_merged_df2[current_knn_cols].fillna(final_merged_df2[current_knn_cols].median())\n",
        "\n",
        "    del ref_data, imputer, imputed_array\n",
        "    gc.collect()\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 3. STATIC DATA & VASOPRESSOR CLEANING\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"\\n[STEP 3/5] Cleaning Static Data...\")\n",
        "\n",
        "# CORRECT GENDER (gender=gender-1) -> Convert M/F to 0/1\n",
        "if 'gender' in final_merged_df2.columns and final_merged_df2['gender'].dtype == 'object':\n",
        "    final_merged_df2['gender_numeric'] = final_merged_df2['gender'].map({'M': 0, 'F': 1})\n",
        "    final_merged_df2.drop(columns=['gender'], inplace=True, errors='ignore')\n",
        "elif 'gender' in final_merged_df2.columns:\n",
        "    final_merged_df2.rename(columns={'gender': 'gender_numeric'}, inplace=True, errors='ignore')\n",
        "\n",
        "# CAP AGE > 150*365.25\n",
        "AGE_CAP = 91.4\n",
        "final_merged_df2['age'] = np.where(final_merged_df2['age'] > AGE_CAP, AGE_CAP, final_merged_df2['age'])\n",
        "\n",
        "# FIX missing Elixhauser values (Median Imputation)\n",
        "elixhauser_median = final_merged_df2['elixhauser'].median()\n",
        "final_merged_df2['elixhauser'] = final_merged_df2['elixhauser'].fillna(elixhauser_median)\n",
        "\n",
        "# FIX Vasopressors (fill NaN with 0.0)\n",
        "final_merged_df2['vaso_step_median'] = final_merged_df2['vaso_step_median'].fillna(0.0)\n",
        "final_merged_df2['vaso_step_max'] = final_merged_df2['vaso_step_max'].fillna(0.0)\n",
        "\n",
        "print(\"✅ Static data cleaned and capped.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 4. DERIVED VARIABLE CALCULATION (P/F, Shock Index)\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"\\n[STEP 4/5] Calculating Derived Features...\")\n",
        "\n",
        "# 4.1. PaO2/FiO2 Ratio (P/F)\n",
        "final_merged_df2['PaO2_FiO2'] = final_merged_df2['paO2'] / final_merged_df2['FiO2']\n",
        "final_merged_df2['PaO2_FiO2'] = final_merged_df2['PaO2_FiO2'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# 4.2. Shock Index\n",
        "final_merged_df2['Shock_Index'] = final_merged_df2['HR'] / final_merged_df2['SysBP']\n",
        "final_merged_df2['Shock_Index'] = final_merged_df2['Shock_Index'].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Impute NaN Shock Index with mean\n",
        "shock_index_mean = final_merged_df2['Shock_Index'].mean()\n",
        "final_merged_df2['Shock_Index'] = final_merged_df2['Shock_Index'].fillna(shock_index_mean)\n",
        "\n",
        "print(\"✅ P/F Ratio and Shock Index calculated.\")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "## 5. SOFA & SIRS CALCULATION\n",
        "# ----------------------------------------------------------------------\n",
        "print(\"\\n[STEP 5/5] Calculating SOFA and SIRS Scores...\")\n",
        "\n",
        "# Compute 4-hourly UO for SOFA Renal Score\n",
        "final_merged_df2['uo_step_4h'] = final_merged_df2.groupby('icustay_id')['uo_total'].diff().fillna(0)\n",
        "\n",
        "# SIRS Score\n",
        "def calculate_sirs(row):\n",
        "    \"\"\"Calculate SIRS score.\"\"\"\n",
        "    sirs_count = 0\n",
        "    if row['Temp_C'] >= 38 or row['Temp_C'] <= 36: sirs_count += 1\n",
        "    if row['HR'] > 90: sirs_count += 1\n",
        "    if row['RR'] >= 20 or row['paCO2'] <= 32: sirs_count += 1\n",
        "    if row['WBC_Count'] >= 12 or row['WBC_Count'] < 4: sirs_count += 1\n",
        "    return sirs_count\n",
        "\n",
        "final_merged_df2['SIRS'] = final_merged_df2.apply(calculate_sirs, axis=1)\n",
        "\n",
        "# SOFA Score\n",
        "def calculate_sofa(row):\n",
        "    \"\"\"Calculate SOFA score.\"\"\"\n",
        "    score = 0\n",
        "\n",
        "    # 1. Respiratory (PaO2/FiO2 ratio)\n",
        "    pf = row['PaO2_FiO2']\n",
        "    if pd.isna(pf): respiratory_score = 0\n",
        "    elif pf < 100: respiratory_score = 4\n",
        "    elif pf < 200: respiratory_score = 3\n",
        "    elif pf < 300: respiratory_score = 2\n",
        "    elif pf < 400: respiratory_score = 1\n",
        "    else: respiratory_score = 0\n",
        "    score += respiratory_score\n",
        "\n",
        "    # 2. Coagulation (Platelets)\n",
        "    plt = row['Platelets_count']\n",
        "    if plt < 20: score += 4\n",
        "    elif plt < 50: score += 3\n",
        "    elif plt < 100: score += 2\n",
        "    elif plt < 150: score += 1\n",
        "\n",
        "    # 3. Liver (Total Bilirubin)\n",
        "    bili = row['Total_bili']\n",
        "    if bili > 12: score += 4\n",
        "    elif bili >= 6: score += 3\n",
        "    elif bili >= 2: score += 2\n",
        "    elif bili >= 1.2: score += 1\n",
        "\n",
        "    # 4. Cardiovascular (MAP & Vasopressors)\n",
        "    map_val = row['MeanBP']\n",
        "    vaso_max = row['vaso_step_max']\n",
        "    cardio_score = 0\n",
        "    if vaso_max > 0.1: cardio_score = 4\n",
        "    elif vaso_max > 0 and vaso_max <= 0.1: cardio_score = 3\n",
        "    elif map_val < 65: cardio_score = 2\n",
        "    elif map_val < 70 and map_val >= 65: cardio_score = 1\n",
        "    score += cardio_score\n",
        "\n",
        "    # 5. Neurological (GCS)\n",
        "    gcs = row['GCS']\n",
        "    if gcs <= 5: score += 4\n",
        "    elif gcs <= 9: score += 3\n",
        "    elif gcs <= 12: score += 2\n",
        "    elif gcs <= 14: score += 1\n",
        "\n",
        "    # 6. Renal (Creatinine & UO)\n",
        "    cr = row['Creatinine']\n",
        "    uo = row['uo_step_4h']\n",
        "    renal_score = 0\n",
        "    if cr > 5.0 or uo < 34: renal_score = 4\n",
        "    elif (cr >= 3.5 and cr < 5) or (uo < 84 and uo >= 0): renal_score = 3\n",
        "    elif cr >= 2: renal_score = 2\n",
        "    elif cr >= 1.2: renal_score = 1\n",
        "    score += renal_score\n",
        "\n",
        "    return score\n",
        "\n",
        "final_merged_df2['SOFA'] = final_merged_df2.apply(calculate_sofa, axis=1)\n",
        "final_merged_df2.drop(columns=['uo_step_4h'], inplace=True, errors='ignore')\n",
        "\n",
        "print(\"✅ SOFA and SIRS Scores calculated.\")\n",
        "print(\"\\n=== Komorowski Process Completed ===\")\n",
        "\n",
        "# --- FINAL VERIFICATION ---\n",
        "print(\"\\n--- Verifying Key Columns (final_merged_df2) ---\")\n",
        "print(\n",
        "    final_merged_df2[\n",
        "        ['icustay_id', 'timestep_id', 'PaO2_FiO2', 'Shock_Index', 'SOFA', 'SIRS', 'HR', 'SysBP', 'Creatinine', 'Total_bili']\n",
        "    ].head(10).to_markdown(index=False)\n",
        ")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZEdN9Jdwvf5",
        "outputId": "87541493-2673-45e8-892c-dca76048a315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STARTING KOMOROWSKI IMPUTATION & FEATURE ENGINEERING ===\n",
            "DataFrame duplicated to 'final_merged_df2'.\n",
            "\n",
            "[STEP 1/5] Performing Linear Interpolation (< 5% Missingness)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Linear Interpolation: 0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Linear Interpolation applied to 0 columns.\n",
            "\n",
            "[STEP 2/5] Performing k-NN Imputation (1-NN, Euclidean, Chunking) - LOCAL MEDIAN PATCH...\n",
            "Total rows to impute: 448521. Total columns to process: 64. Chunk size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   0%|          | 0/45 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 0 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   2%|▏         | 1/45 [01:00<44:12, 60.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 0 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 1 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   4%|▍         | 2/45 [02:01<43:32, 60.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 1 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 2 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   7%|▋         | 3/45 [02:49<38:25, 54.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 2 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 3 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:   9%|▉         | 4/45 [03:56<40:53, 59.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 3 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 4 (5 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  11%|█         | 5/45 [04:58<40:19, 60.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 4 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 5 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  13%|█▎        | 6/45 [05:58<39:14, 60.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 5 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 6 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  16%|█▌        | 7/45 [06:45<35:33, 56.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 6 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 7 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  18%|█▊        | 8/45 [07:51<36:30, 59.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 7 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 8 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  20%|██        | 9/45 [08:51<35:34, 59.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 8 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 9 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  22%|██▏       | 10/45 [09:36<32:08, 55.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 9 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 10 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  24%|██▍       | 11/45 [10:39<32:27, 57.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 10 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 11 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  27%|██▋       | 12/45 [11:44<32:46, 59.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 11 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 12 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  29%|██▉       | 13/45 [12:44<32:00, 60.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 12 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 13 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  31%|███       | 14/45 [13:41<30:32, 59.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 13 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 14 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  33%|███▎      | 15/45 [14:40<29:28, 58.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 14 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 15 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  36%|███▌      | 16/45 [15:27<26:44, 55.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 15 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 16 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  38%|███▊      | 17/45 [16:32<27:07, 58.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 16 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 17 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  40%|████      | 18/45 [17:19<24:40, 54.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 17 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 18 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  42%|████▏     | 19/45 [18:18<24:22, 56.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 18 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 19 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  44%|████▍     | 20/45 [19:15<23:31, 56.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 19 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 20 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  47%|████▋     | 21/45 [20:17<23:14, 58.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 20 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 21 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  49%|████▉     | 22/45 [21:08<21:25, 55.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 21 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 22 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  51%|█████     | 23/45 [22:12<21:22, 58.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 22 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 23 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  53%|█████▎    | 24/45 [22:58<19:06, 54.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 23 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 24 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  56%|█████▌    | 25/45 [23:46<17:30, 52.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 24 fixed with median fallback. Continuing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  58%|█████▊    | 26/45 [24:08<13:46, 43.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 26 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  60%|██████    | 27/45 [25:10<14:42, 49.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 26 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 27 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  62%|██████▏   | 28/45 [26:08<14:37, 51.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 27 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 28 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  64%|██████▍   | 29/45 [26:55<13:27, 50.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 28 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 29 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  67%|██████▋   | 30/45 [27:41<12:17, 49.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 29 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 30 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  69%|██████▉   | 31/45 [28:41<12:11, 52.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 30 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 31 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  71%|███████   | 32/45 [29:42<11:53, 54.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 31 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 32 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  73%|███████▎  | 33/45 [30:29<10:29, 52.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 32 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 33 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  76%|███████▌  | 34/45 [31:32<10:11, 55.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 33 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 34 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  78%|███████▊  | 35/45 [32:34<09:36, 57.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 34 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 35 (5 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  80%|████████  | 36/45 [33:33<08:41, 57.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 35 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 36 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  82%|████████▏ | 37/45 [34:34<07:52, 59.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 36 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 37 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  84%|████████▍ | 38/45 [35:22<06:29, 55.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 37 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 38 (4 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  87%|████████▋ | 39/45 [36:24<05:45, 57.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 38 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 39 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  89%|████████▉ | 40/45 [37:12<04:34, 54.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 39 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 40 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  91%|█████████ | 41/45 [38:15<03:48, 57.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 40 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 41 (3 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  93%|█████████▎| 42/45 [39:02<02:42, 54.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 41 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 42 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  96%|█████████▌| 43/45 [40:01<01:51, 55.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 42 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 43 (1 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rk-NN Chunk Imputation:  98%|█████████▊| 44/45 [40:49<00:53, 53.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 43 fixed with median fallback. Continuing...\n",
            "\n",
            "⚠️ DIMENSION MISMATCH FOUND in chunk 44 (2 missing columns).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "k-NN Chunk Imputation: 100%|██████████| 45/45 [41:47<00:00, 55.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Chunk 44 fixed with median fallback. Continuing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ k-NN Imputation completed successfully. 0 NaN remaining in clinical columns.\n",
            "\n",
            "[STEP 3/5] Cleaning Static Data...\n",
            "✅ Static data cleaned and capped.\n",
            "\n",
            "[STEP 4/5] Calculating Derived Features...\n",
            "✅ P/F Ratio and Shock Index calculated.\n",
            "\n",
            "[STEP 5/5] Calculating SOFA and SIRS Scores...\n",
            "✅ SOFA and SIRS Scores calculated.\n",
            "\n",
            "=== Komorowski Process Completed ===\n",
            "\n",
            "--- Verifying Key Columns (final_merged_df2) ---\n",
            "|   icustay_id |   timestep_id |   PaO2_FiO2 |   Shock_Index |   SOFA |   SIRS |    HR |   SysBP |   Creatinine |   Total_bili |\n",
            "|-------------:|--------------:|------------:|--------------:|-------:|-------:|------:|--------:|-------------:|-------------:|\n",
            "|       200001 |             1 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             2 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             3 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             4 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             5 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             6 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             7 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             8 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |             9 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n",
            "|       200001 |            10 |         280 |      0.596037 |      6 |      3 | 97.75 |     164 |          1.9 |          0.6 |\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# 0. SETUP\n",
        "# ======================================================================\n",
        "# Define the list of 47 time-series features for missingness inspection\n",
        "TIME_SERIES_FEATURES = [\n",
        "    'SOFA', 'SIRS', 'RR', 'Temp_C', 'SpO2', 'HR', 'MeanBP', 'GCS',\n",
        "    'Lactate', 'Arterial_pH', 'paO2', 'paCO2', 'FiO2', 'PaO2_FiO2', 'Sodium', 'Potassium',\n",
        "    'Chloride', 'HCO3', 'BUN', 'Creatinine', 'Total_bili', 'Albumin', 'Platelets_count',\n",
        "    'WBC_Count', 'HCT', 'Glucose', 'input_total', 'uo_total', 'SysBP', 'DiaBP', 'Shock_Index',\n",
        "    'Arterial_BE', 'Ionized Calcium', 'SGOT(AST)', 'SGPT(ALT)', 'PT', 'PTT', 'INR', 'Hb',\n",
        "    'Magnesium', 'Calcium', 'cumulated_balance', 'Weight_kg', 'elixhauser', 'age', 'gender_numeric'\n",
        "]\n",
        "# Ensure all 47 time-series features are included.\n",
        "\n",
        "# Create a copy of 'final_merged_df2'\n",
        "reformat4t = final_merged_df2.copy()\n",
        "print(\"Starting with DataFrame 'reformat4t' (copy of final_merged_df2).\")\n",
        "\n",
        "# ======================================================================\n",
        "# 1. PREPARATION OF REQUIRED COLUMNS\n",
        "# ======================================================================\n",
        "# Ensure 4-hourly input/output columns exist before exclusions\n",
        "reformat4t['input_4hourly'] = reformat4t.groupby('icustay_id')['input_total'].diff().fillna(0)\n",
        "reformat4t['output_4hourly'] = reformat4t.groupby('icustay_id')['uo_total'].diff().fillna(0)\n",
        "\n",
        "# Rename columns for consistency\n",
        "reformat4t.rename(columns={'morta_90': 'mortality_90d', 'vaso_step_max': 'max_dose_vaso', 'timestep_id': 'bloc'}, inplace=True)\n",
        "reformat4t['max_dose_vaso'] = reformat4t['max_dose_vaso'].fillna(0)\n",
        "\n",
        "# ======================================================================\n",
        "# 2. UTILITY FUNCTIONS (VECTORIZED)\n",
        "# ======================================================================\n",
        "def numel_unique(df, col):\n",
        "    \"\"\"Count the number of unique patient IDs.\"\"\"\n",
        "    return len(df[col].unique())\n",
        "\n",
        "def remove_patients(df, condition):\n",
        "    \"\"\"Remove all rows belonging to patients (icustay_id) that meet a given condition.\"\"\"\n",
        "    ids_to_remove = df.loc[condition, 'icustay_id'].unique()\n",
        "    mask = df['icustay_id'].isin(ids_to_remove)\n",
        "    return df[~mask].reset_index(drop=True)\n",
        "\n",
        "# ======================================================================\n",
        "# 3. HIERARCHICAL MORTALITY IMPUTATION (Fill NaN -> 1 if confirmed dead)\n",
        "# ======================================================================\n",
        "print(\"\\n--- MORTALITY IMPUTATION (Hierarchical Filling NaN -> 1) ---\")\n",
        "\n",
        "# 3.1 Ensure 'onset_time_T0', 'dod', 'expire_flag', 'outtime', and 'endtime' are available\n",
        "if 'onset_time_T0' not in reformat4t.columns:\n",
        "    onset_df_full = final_merged_df2[['icustay_id', 'onset_time_T0']].drop_duplicates()\n",
        "    reformat4t = pd.merge(reformat4t, onset_df_full, on='icustay_id', how='left')\n",
        "\n",
        "# ENSURE that 'outtime' and 'endtime' columns are merged from demog or another source\n",
        "if (\n",
        "    'dod' not in reformat4t.columns\n",
        "    or 'expire_flag' not in reformat4t.columns\n",
        "    or 'outtime' not in reformat4t.columns\n",
        "    or 'intime' not in reformat4t.columns\n",
        "):\n",
        "    # Replace demog_for_merge with all required time-related columns\n",
        "    demog_for_merge = demog[['icustay_id', 'dod', 'expire_flag', 'outtime', 'intime']].drop_duplicates().copy()\n",
        "    reformat4t = pd.merge(reformat4t, demog_for_merge, on='icustay_id', how='left')\n",
        "\n",
        "# 3.2 Convert date columns\n",
        "try:\n",
        "    reformat4t['onset_time_T0'] = pd.to_datetime(reformat4t['onset_time_T0'], errors='coerce')\n",
        "    reformat4t['dod'] = pd.to_datetime(reformat4t['dod'], errors='coerce')\n",
        "    # Also convert outtime and endtime\n",
        "    reformat4t['outtime'] = pd.to_datetime(reformat4t['outtime'], errors='coerce')\n",
        "    reformat4t['intime'] = pd.to_datetime(reformat4t['intime'], errors='coerce')\n",
        "except Exception as e:\n",
        "    print(f\"ERROR converting time columns: {e}. Cannot proceed with time calculations.\")\n",
        "\n",
        "\n",
        "# 3.3 Impute 'morta_hosp' using 'expire_flag'\n",
        "reformat4t.loc[reformat4t['morta_hosp'].isnull() & (reformat4t['expire_flag'] == 1), 'morta_hosp'] = 1\n",
        "print(f\"Filled NaN morta_hosp with 1 where expire_flag=1: {reformat4t.loc[reformat4t['morta_hosp'] == 1, 'icustay_id'].nunique()} patients now have morta_hosp=1.\")\n",
        "\n",
        "# 3.4 Impute 'mortality_90d' using DOD within 90 days of onset\n",
        "ninety_days = pd.Timedelta(days=90)\n",
        "condition_morta_90d_calc = (\n",
        "    reformat4t['mortality_90d'].isnull() &\n",
        "    reformat4t['dod'].notna() &\n",
        "    ((reformat4t['dod'] - reformat4t['onset_time_T0']) <= ninety_days) &\n",
        "    ((reformat4t['dod'] - reformat4t['onset_time_T0']) >= pd.Timedelta(seconds=0))\n",
        ")\n",
        "reformat4t.loc[condition_morta_90d_calc, 'mortality_90d'] = 1\n",
        "print(\"Filled NaN mortality_90d with 1 based on DOD check.\")\n",
        "\n",
        "# 3.5 Calculate time variables for early mortality exclusion\n",
        "# A. died_within_48h_of_out_time (boolean)\n",
        "# Criteria: (Patient died) AND (Death time - ICU discharge time ≤ 48 hours)\n",
        "hours_48 = pd.Timedelta(hours=48)\n",
        "reformat4t['died_within_48h_of_out_time'] = (\n",
        "    reformat4t['dod'].notna() &\n",
        "    (reformat4t['dod'] - reformat4t['outtime'] <= hours_48) &\n",
        "    (reformat4t['dod'] > reformat4t['outtime'])  # Ensure DOD occurs after OUTTIME\n",
        ")\n",
        "\n",
        "# B. delay_end_of_record_and_discharge_or_death (in hours)\n",
        "# The time difference between the end of recorded data (endtime) and the time of discharge/death (outtime/dod)\n",
        "# Take the earlier of outtime and dod as 'discharge_or_death_time'\n",
        "hours_4 = pd.Timedelta(hours=4)\n",
        "reformat4t['record_endtime'] = reformat4t['intime'] + (reformat4t['bloc'] * hours_4)\n",
        "\n",
        "reformat4t['discharge_or_death_time'] = np.where(\n",
        "    reformat4t['dod'].notna(),\n",
        "    reformat4t['dod'],\n",
        "    reformat4t['outtime']\n",
        ")\n",
        "\n",
        "# Compute the time difference in hours\n",
        "reformat4t['delay_end_of_record_and_discharge_or_death'] = (\n",
        "    reformat4t['discharge_or_death_time'] - reformat4t['record_endtime']\n",
        ").dt.total_seconds() / 3600\n",
        "\n",
        "# Note: 'endtime' is used as a proxy for the end of the record since 'bloc' only captures 4-hour intervals\n",
        "\n",
        "print(\"Calculation complete.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 4. PATIENT EXCLUSION\n",
        "# ======================================================================\n",
        "print(\"\\n--- PATIENT EXCLUSION ---\")\n",
        "count_before = numel_unique(reformat4t, 'icustay_id')\n",
        "print(f\"Number of patients before exclusion: {count_before}\")\n",
        "\n",
        "# 4.1 Extreme adn unrealistic HR, RR, urine output, Total Bilirubin, and intake (> 10000 ml per 4 hours)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['HR'] < 10)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['RR'] > 60)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['output_4hourly'] > 12000)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['Total_bili'] > 10000)\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['input_4hourly'] > 10000)\n",
        "print(f\"Number of patients after UO/Bili/Intake outlier exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.2 Exclude patients under 18 years old\n",
        "print(\"Excluding patients younger than 18 years old...\")\n",
        "reformat4t = remove_patients(reformat4t, reformat4t['age'] < 18)\n",
        "print(f\"Number of patients after age <18 exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.3 Exclude patients who did not reach T=0 (did not reach Bloc 7)\n",
        "print(\"Excluding patients who did not reach T=0 (Bloc 7)...\")\n",
        "max_bloc_per_patient = reformat4t.groupby('icustay_id')['bloc'].max().reset_index()\n",
        "MIN_BLOC_ONSET = 7\n",
        "ids_to_remove_before_t0 = max_bloc_per_patient.loc[max_bloc_per_patient['bloc'] < MIN_BLOC_ONSET, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_before_t0)].reset_index(drop=True)\n",
        "print(f\"Number of patients after exclusion before T=0: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.4 Exclude patients with ICU stay duration < 24 hours (Min 6 blocs) AFTER ONSET (T=0)\n",
        "print(\"Excluding patients with LOS < 24 hours (Min 6 blocs) after T=0...\")\n",
        "MIN_BLOC_AFTER_ONSET = 6  # 24 hours\n",
        "MIN_BLOC_TOTAL = MIN_BLOC_ONSET + MIN_BLOC_AFTER_ONSET  # Bloc 7 + 6 = Bloc 13\n",
        "ids_to_remove_los = max_bloc_per_patient.loc[max_bloc_per_patient['bloc'] < MIN_BLOC_TOTAL, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_los)].reset_index(drop=True)\n",
        "print(f\"Number of patients after LOS < {MIN_BLOC_TOTAL} blocs exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# Create patient_stats\n",
        "patient_stats = reformat4t.groupby('icustay_id').agg(\n",
        "    max_mortality_90d=('mortality_90d', 'max'),\n",
        "    max_vaso=('max_dose_vaso', 'max'),\n",
        "    max_sofa=('SOFA', 'max'),\n",
        "    max_bloc=('bloc', 'max')\n",
        ").reset_index()\n",
        "\n",
        "# Patients with mortality_90d=1 dan max_bloc less than 13\n",
        "ids_to_remove_early_death_absolute = patient_stats.loc[\n",
        "    (patient_stats['max_mortality_90d'] == 1) &\n",
        "    (patient_stats['max_bloc'] < MIN_BLOC_TOTAL), # MIN_BLOC_TOTAL=13\n",
        "    'icustay_id'\n",
        "].unique()\n",
        "\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_early_death_absolute)].reset_index(drop=True)\n",
        "print(f\"Number of patients after absolute early death exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.5 Exclude patients with high missingness (≥8 out of 46 variables)\n",
        "print(\"Excluding patients with high missing data (≥8 out of 46 features)...\")\n",
        "missing_counts = reformat4t.groupby('icustay_id')[TIME_SERIES_FEATURES].apply(\n",
        "    lambda x: x.isnull().sum(axis=1).max()\n",
        ").reset_index(name='max_missing_features')\n",
        "MAX_MISSING_ALLOWED = 7\n",
        "ids_to_remove_high_missing = missing_counts.loc[missing_counts['max_missing_features'] > MAX_MISSING_ALLOWED, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_high_missing)].reset_index(drop=True)\n",
        "print(f\"Number of patients after high missing data exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.6 Exclude patients whose treatment was withdrawn\n",
        "# Create withdrawal_stats\n",
        "withdrawal_stats = reformat4t.groupby('icustay_id').agg(\n",
        "    max_mortality_90d=('mortality_90d', 'max'),\n",
        "    max_vaso=('max_dose_vaso', 'max'),\n",
        "    max_sofa=('SOFA', 'max'),\n",
        "    max_bloc=('bloc', 'max')\n",
        ").reset_index()\n",
        "\n",
        "last_rows = reformat4t[reformat4t['bloc'] == reformat4t.groupby('icustay_id')['bloc'].transform('max')].copy()\n",
        "\n",
        "# Merge using withdrawal_stats\n",
        "merged_df = last_rows.merge(\n",
        "    withdrawal_stats[['icustay_id', 'max_mortality_90d', 'max_vaso', 'max_sofa', 'max_bloc']],\n",
        "    on='icustay_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "condition_withdrawal = (\n",
        "    (merged_df['max_mortality_90d'] == 1) &\n",
        "    (merged_df['max_dose_vaso'] == 0) &\n",
        "    (merged_df['max_vaso'] > 0.3) &\n",
        "    (merged_df['SOFA'] >= merged_df['max_sofa'] / 2) &\n",
        "    (merged_df['max_bloc'] < 20)\n",
        ")\n",
        "\n",
        "ids_to_remove_withdrawal = merged_df.loc[condition_withdrawal, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_withdrawal)].reset_index(drop=True)\n",
        "print(f\"Number of patients after treatment withdrawal exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 4.7 Exclude patients who died in ICU during the data collection period\n",
        "print(\"Excluding patients who died within 48h of discharge AND had a short delay between end of record and discharge/death...\")\n",
        "\n",
        "# Only one row per patient is needed; take the last record per icustay_id\n",
        "last_rows_to_check = reformat4t[reformat4t['bloc'] == reformat4t.groupby('icustay_id')['bloc'].transform('max')].copy()\n",
        "\n",
        "condition_early_death_after_record = (\n",
        "    (last_rows_to_check['died_within_48h_of_out_time'] == True) &\n",
        "    (last_rows_to_check['delay_end_of_record_and_discharge_or_death'] < 24)\n",
        ")\n",
        "\n",
        "# Get IDs of patients meeting the exclusion criteria\n",
        "ids_to_remove_early_death = last_rows_to_check.loc[condition_early_death_after_record, 'icustay_id'].unique()\n",
        "\n",
        "# Remove those patients from the main DataFrame\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_early_death)].reset_index(drop=True)\n",
        "\n",
        "print(f\"Number of patients after early death exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# Final count after all exclusions\n",
        "count_after = numel_unique(reformat4t, 'icustay_id')\n",
        "print(f\"Number of patients remaining after all exclusions: {count_after}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 5. FINAL MORTALITY IMPUTATION (Conservative Censor: NaN -> 0)\n",
        "# ======================================================================\n",
        "print(\"\\n--- FINAL MORTALITY IMPUTATION (Conservative Censor: NaN -> 0) ---\")\n",
        "reformat4t['morta_hosp'] = reformat4t['morta_hosp'].fillna(0)\n",
        "reformat4t['mortality_90d'] = reformat4t['mortality_90d'].fillna(0)\n",
        "print(\"All remaining NaN mortality labels are set to 0.\")\n",
        "\n",
        "# ======================================================================\n",
        "# 6. TRUNCATE RECORDS TO T=20\n",
        "# ======================================================================\n",
        "print(\"\\n--- TRUNCATING RECORDS TO T=20 (80 HOURS) ---\")\n",
        "MAX_BLOC = 20\n",
        "reformat4t = reformat4t[reformat4t['bloc'] <= MAX_BLOC].reset_index(drop=True)\n",
        "print(f\"Total records after truncation (Max bloc={MAX_BLOC}): {len(reformat4t)}\")\n",
        "\n",
        "# ======================================================================\n",
        "# 7. CREATE SEPSIS COHORT & SAVE TIME-SERIES DATA\n",
        "# ======================================================================\n",
        "print(\"\\n--- SEPSIS COHORT CREATION & DATA SAVING (Filtering T=0 SOFA) ---\")\n",
        "\n",
        "# 7.1 Get SOFA at T=0 (Bloc 7)\n",
        "t0_records = reformat4t[reformat4t['bloc'] == 7].copy()\n",
        "final_patient_t0_summary = t0_records.groupby('icustay_id').agg(\n",
        "    morta_90d=('mortality_90d', 'max'),\n",
        "    sofa_at_t0=('SOFA', 'first'),\n",
        "    max_sofa_overall=('SOFA', 'max')\n",
        ").reset_index()\n",
        "\n",
        "# 7.2 Apply Sepsis-3 filter (SOFA at T=0 ≥ 2)\n",
        "sepsis_cohort_ids = final_patient_t0_summary.loc[final_patient_t0_summary['sofa_at_t0'] >= 2, 'icustay_id'].unique()\n",
        "\n",
        "# 7.3 Ensure there's no critical features missing in T=0\n",
        "print(\"Excluding patients missing critical T=0 features (SOFA, Lactate, PaO2/FiO2)...\")\n",
        "critical_t0_missing = reformat4t[reformat4t['bloc'] == MIN_BLOC_ONSET].copy()\n",
        "condition_critical_missing = (\n",
        "    critical_t0_missing['SOFA'].isnull() |\n",
        "    critical_t0_missing['Lactate'].isnull() |\n",
        "    critical_t0_missing['PaO2_FiO2'].isnull()\n",
        ")\n",
        "ids_to_remove_critical = critical_t0_missing.loc[condition_critical_missing, 'icustay_id'].unique()\n",
        "reformat4t = reformat4t[~reformat4t['icustay_id'].isin(ids_to_remove_critical)].reset_index(drop=True)\n",
        "print(f\"Number of patients after critical T=0 missing exclusion: {numel_unique(reformat4t, 'icustay_id')}\")\n",
        "\n",
        "# 7.4 Save time-series data\n",
        "final_sepsis_df = reformat4t[reformat4t['icustay_id'].isin(sepsis_cohort_ids)].reset_index(drop=True)\n",
        "output_data_filename = 'SEPSIS-COHORT-TIMESERIES.csv'\n",
        "output_data_path = os.path.join(data_dir, output_data_filename)\n",
        "\n",
        "final_sepsis_df.to_csv(output_data_path, index=False)\n",
        "\n",
        "print(f\"Sepsis Cohort Time-Series Data saved to: {output_data_path}\")\n",
        "print(f\"Total records in this file: {len(final_sepsis_df)}\")\n",
        "\n",
        "# 7.4 Save patient summary\n",
        "sepsis_cohort_summary = final_patient_t0_summary.loc[final_patient_t0_summary['icustay_id'].isin(sepsis_cohort_ids)].copy()\n",
        "onset_times = final_merged_df2.drop_duplicates(subset=['icustay_id'])[['icustay_id', 'onset_time_T0']].set_index('icustay_id')\n",
        "sepsis_cohort_summary = sepsis_cohort_summary.merge(onset_times, on='icustay_id', how='left')\n",
        "sepsis_cohort_summary.rename(columns={'onset_time_T0': 'sepsis_time'}, inplace=True)\n",
        "\n",
        "print(f\"Number of final Sepsis patients (SOFA at T=0 ≥ 2): {len(sepsis_cohort_summary)}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVRqSx2wUqeR",
        "outputId": "d352bc7d-8ceb-428c-8fbc-d6a804c593ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting with DataFrame 'reformat4t' (copy of final_merged_df2).\n",
            "\n",
            "--- MORTALITY IMPUTATION (Hierarchical Filling NaN -> 1) ---\n",
            "Filled NaN morta_hosp with 1 where expire_flag=1: 3223 patients now have morta_hosp=1.\n",
            "Filled NaN mortality_90d with 1 based on DOD check.\n",
            "Calculation complete.\n",
            "\n",
            "--- PATIENT EXCLUSION ---\n",
            "Number of patients before exclusion: 24948\n",
            "Number of patients after UO/Bili/Intake outlier exclusion: 24919\n",
            "Excluding patients younger than 18 years old...\n",
            "Number of patients after age <18 exclusion: 21965\n",
            "Excluding patients who did not reach T=0 (Bloc 7)...\n",
            "Number of patients after exclusion before T=0: 21349\n",
            "Excluding patients with LOS < 24 hours (Min 6 blocs) after T=0...\n",
            "Number of patients after LOS < 13 blocs exclusion: 19323\n",
            "Number of patients after absolute early death exclusion: 19323\n",
            "Excluding patients with high missing data (≥8 out of 46 features)...\n",
            "Number of patients after high missing data exclusion: 19323\n",
            "Number of patients after treatment withdrawal exclusion: 19293\n",
            "Excluding patients who died within 48h of discharge AND had a short delay between end of record and discharge/death...\n",
            "Number of patients after early death exclusion: 19192\n",
            "Number of patients remaining after all exclusions: 19192\n",
            "\n",
            "--- FINAL MORTALITY IMPUTATION (Conservative Censor: NaN -> 0) ---\n",
            "All remaining NaN mortality labels are set to 0.\n",
            "\n",
            "--- TRUNCATING RECORDS TO T=20 (80 HOURS) ---\n",
            "Total records after truncation (Max bloc=20): 371574\n",
            "\n",
            "--- SEPSIS COHORT CREATION & DATA SAVING (Filtering T=0 SOFA) ---\n",
            "Excluding patients missing critical T=0 features (SOFA, Lactate, PaO2/FiO2)...\n",
            "Number of patients after critical T=0 missing exclusion: 19192\n",
            "Sepsis Cohort Time-Series Data saved to: /content/drive/MyDrive/mimic3/SEPSIS-COHORT-TIMESERIES.csv\n",
            "Total records in this file: 364563\n",
            "Number of final Sepsis patients (SOFA at T=0 ≥ 2): 18830\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================================\n",
        "# Pivoting relevant columns for final DataFrame\n",
        "# ======================================================================\n",
        "\n",
        "print(\"--- STARTING FINAL COHORT REFINEMENT AND MERGE ---\")\n",
        "\n",
        "# ======================================================================\n",
        "# 1. PREPARE MECHANICAL VENTILATION DATA (MechVent)\n",
        "# ======================================================================\n",
        "mechvent_df = MechVent[['icustay_id', 'MechVent']].dropna(subset=['icustay_id']).copy()\n",
        "mechvent_df['icustay_id'] = mechvent_df['icustay_id'].astype(int)\n",
        "\n",
        "# Aggregate: Determine if the patient was ever intubated (max MechVent = 1)\n",
        "mechvent_status = mechvent_df.groupby('icustay_id')['MechVent'].max().reset_index()\n",
        "mechvent_status.rename(columns={'MechVent': 'on_mechvent'}, inplace=True)\n",
        "\n",
        "# ======================================================================\n",
        "# 2. PREPARE READMISSION DATA (is_readmit)\n",
        "# ======================================================================\n",
        "# Extract 'is_readmit' from the original merged dataset (final_merged_df2)\n",
        "readmit_status = final_merged_df[['icustay_id', 'is_readmit']].drop_duplicates().copy()\n",
        "\n",
        "# ======================================================================\n",
        "# 3. MERGE STATIC FEATURES INTO FINAL SEPSIS DATAFRAME\n",
        "# ======================================================================\n",
        "# Merge ventilation status\n",
        "if 'on_mechvent' in final_sepsis_df.columns:\n",
        "    final_sepsis_df.drop(columns=['on_mechvent'], inplace=True)\n",
        "final_sepsis_df = pd.merge(\n",
        "    final_sepsis_df,\n",
        "    mechvent_status,\n",
        "    on='icustay_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Merge readmission status\n",
        "if 'is_readmit' in final_sepsis_df.columns:\n",
        "    final_sepsis_df.drop(columns=['is_readmit'], inplace=True)\n",
        "\n",
        "final_sepsis_df = pd.merge(\n",
        "    final_sepsis_df,\n",
        "    readmit_status,\n",
        "    on='icustay_id',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Clean missing static feature values (if any)\n",
        "# Assumption: Missing on_mechvent or is_readmit means 0\n",
        "final_sepsis_df['on_mechvent'] = final_sepsis_df['on_mechvent'].fillna(0).astype(int)\n",
        "final_sepsis_df['is_readmit'] = final_sepsis_df['is_readmit'].fillna(0).astype(int)\n",
        "\n",
        "# ======================================================================\n",
        "# 4. FILTER REQUIRED COLUMNS AND SAVE FINAL OUTPUT\n",
        "# ======================================================================\n",
        "# Add back 'bloc' and 'mortality_90d' because they are fundamental for\n",
        "# time-series structure and outcome labeling (even if not in the 47 features)\n",
        "FINAL_COLUMNS = ['icustay_id', 'bloc', 'morta_hosp', 'mortality_90d', 'is_readmit', 'on_mechvent', 'max_dose_vaso'] + TIME_SERIES_FEATURES\n",
        "\n",
        "# Ensure all required columns exist before saving\n",
        "missing_cols = [col for col in FINAL_COLUMNS if col not in final_sepsis_df.columns]\n",
        "if missing_cols:\n",
        "    print(f\"⚠️ Warning: Missing columns in final_sepsis_df before saving: {missing_cols}\")\n",
        "\n",
        "# Select only the required columns\n",
        "final_output_df = final_sepsis_df[FINAL_COLUMNS].copy()\n",
        "\n",
        "# Save final cleaned dataset\n",
        "output_data_filename_final = 'SEPSIS-COHORT-FINAL-DF.csv'\n",
        "output_data_path_final = os.path.join(data_dir, output_data_filename_final)\n",
        "\n",
        "final_output_df.to_csv(output_data_path_final, index=False)\n",
        "\n",
        "print(f\"\\nFinal Sepsis Cohort data successfully saved to:{output_data_path_final}\")\n",
        "print(f\"Total records: {len(final_output_df)}\")\n",
        "print(f\"Total columns: {len(final_output_df.columns)}\")\n",
        "print(f\"Saved columns: {final_output_df.columns.tolist()}\")\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "matga7PkqAkf",
        "outputId": "b3d0e714-d3c0-4c82-a920-a03219750ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STARTING FINAL COHORT REFINEMENT AND MERGE ---\n",
            "\n",
            "Final Sepsis Cohort data successfully saved to:/content/drive/MyDrive/mimic3/SEPSIS-COHORT-FINAL-DF.csv\n",
            "Total records: 364563\n",
            "Total columns: 53\n",
            "Saved columns: ['icustay_id', 'bloc', 'morta_hosp', 'mortality_90d', 'is_readmit', 'on_mechvent', 'max_dose_vaso', 'SOFA', 'SIRS', 'RR', 'Temp_C', 'SpO2', 'HR', 'MeanBP', 'GCS', 'Lactate', 'Arterial_pH', 'paO2', 'paCO2', 'FiO2', 'PaO2_FiO2', 'Sodium', 'Potassium', 'Chloride', 'HCO3', 'BUN', 'Creatinine', 'Total_bili', 'Albumin', 'Platelets_count', 'WBC_Count', 'HCT', 'Glucose', 'input_total', 'uo_total', 'SysBP', 'DiaBP', 'Shock_Index', 'Arterial_BE', 'Ionized Calcium', 'SGOT(AST)', 'SGPT(ALT)', 'PT', 'PTT', 'INR', 'Hb', 'Magnesium', 'Calcium', 'cumulated_balance', 'Weight_kg', 'elixhauser', 'age', 'gender_numeric']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    }
  ]
}
